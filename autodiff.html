<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mahendra Mariadassou">
<meta name="author" content="Hugo Gangloff">
<meta name="author" content="Arthur Leroy">
<meta name="author" content="Lucia Clarotto">

<title>Tutoriel de différentiation automatique – Finist’R 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b8ded578f01cf45f867b749dfb801973.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Pas de résultats",
    "search-matching-documents-text": "documents trouvés",
    "search-copy-link-title": "Copier le lien vers la recherche",
    "search-hide-matches-text": "Cacher les correspondances additionnelles",
    "search-more-match-text": "correspondance de plus dans ce document",
    "search-more-matches-text": "correspondances de plus dans ce document",
    "search-clear-button-title": "Effacer",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Annuler",
    "search-submit-button-title": "Envoyer",
    "search-label": "Recherche"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Finist’R 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Recherche"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Basculer la navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/StateOfTheR/finistR2025"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./instructions.html"> 
<span class="menu-text">Instructions</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-développement" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Développement</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-développement">    
        <li>
    <a class="dropdown-item" href="./how_to_build_your_package.html">
 <span class="dropdown-text">How to build your package - Julia edition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./positron.qmd">
 <span class="dropdown-text">Positron</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link active" href="./autodiff.html" aria-current="page"> 
<span class="menu-text">Différentiation automatique</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./animint.html"> 
<span class="menu-text">animint2</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./hsmc.qmd"> 
<span class="menu-text">HMSC</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./traccar.html"> <i class="bi bi-Bicycle" role="img">
</i> 
<span class="menu-text">GPS – traccar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./sparse_cholesky.html"> 
<span class="menu-text">Sparse Cholesky</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how_to_build_your_package.html"> 
<span class="menu-text">How to build your package - Julia edition</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sur cette page</h2>
   
  <ul>
  <li><a href="#théorie-xx" id="toc-théorie-xx" class="nav-link active" data-scroll-target="#théorie-xx">Théorie [XX]</a></li>
  <li><a href="#exemple-en-torch-mm-lc" id="toc-exemple-en-torch-mm-lc" class="nav-link" data-scroll-target="#exemple-en-torch-mm-lc">Exemple en Torch [MM, LC]</a>
  <ul class="collapse">
  <li><a href="#en-utilisant-les-primitives-de-torch" id="toc-en-utilisant-les-primitives-de-torch" class="nav-link" data-scroll-target="#en-utilisant-les-primitives-de-torch">En utilisant les primitives de torch</a></li>
  <li><a href="#en-utilisant-notre-propre-fonction" id="toc-en-utilisant-notre-propre-fonction" class="nav-link" data-scroll-target="#en-utilisant-notre-propre-fonction">En utilisant notre propre fonction</a>
  <ul class="collapse">
  <li><a href="#définition-de-la-fonction" id="toc-définition-de-la-fonction" class="nav-link" data-scroll-target="#définition-de-la-fonction">Définition de la fonction</a></li>
  <li><a href="#vérification-des-dérivées" id="toc-vérification-des-dérivées" class="nav-link" data-scroll-target="#vérification-des-dérivées">Vérification des dérivées</a></li>
  </ul></li>
  <li><a href="#comparaison-des-temps-de-calculs" id="toc-comparaison-des-temps-de-calculs" class="nav-link" data-scroll-target="#comparaison-des-temps-de-calculs">Comparaison des temps de calculs</a></li>
  <li><a href="#impact-de-jit" id="toc-impact-de-jit" class="nav-link" data-scroll-target="#impact-de-jit">Impact de JIT</a></li>
  </ul></li>
  <li><a href="#exemple-en-jax-hg-al" id="toc-exemple-en-jax-hg-al" class="nav-link" data-scroll-target="#exemple-en-jax-hg-al">Exemple en JAX [HG, AL]</a>
  <ul class="collapse">
  <li><a href="#dérivée-par-rapport-à-x" id="toc-dérivée-par-rapport-à-x" class="nav-link" data-scroll-target="#dérivée-par-rapport-à-x">Dérivée par rapport à <span class="math inline">\(x\)</span></a></li>
  </ul></li>
  <li><a href="#comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" id="toc-comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" class="nav-link" data-scroll-target="#comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients">Comparaison des temps de calcul en JAX entre autograd (backward et forward) et le calcul explicite des gradients</a></li>
  <li><a href="#jit-compilation-avec-jax" id="toc-jit-compilation-avec-jax" class="nav-link" data-scroll-target="#jit-compilation-avec-jax">JIT compilation avec JAX</a>
  <ul class="collapse">
  <li><a href="#avec-vjp-et-jvp" id="toc-avec-vjp-et-jvp" class="nav-link" data-scroll-target="#avec-vjp-et-jvp">Avec VJP et JVP</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tutoriel de différentiation automatique</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Auteur·rice·s</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Mahendra Mariadassou <a href="mailto:mahendra.mariadassou@inrae.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-2986-354X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            INRAE - MaIAGE
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Hugo Gangloff <a href="mailto:hugo.gangloff@inrae.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            INRAE - MIA Paris Saclay
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Arthur Leroy </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Lucia Clarotto </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Date de publication</div>
    <div class="quarto-title-meta-contents">
      <p class="date">18 août 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modifié</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">22 août 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>L’obectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d’une fonction qui n’est disponible dans les primitives fournies par JAX/Torch. Les deux cas d’usage envisagées sont:</p>
<ul>
<li>l’utilisation d’une fonction non différentiable pour lesquels on veut écrire une dérivée “non-standard” afin de pouvoir l’utiliser dans JAX/Torch</li>
<li>l’utilisation d’une fonction donc une approximation analytique de la dérivée est disponible mais qui n’est pas implémentée dans JAX/Torch</li>
</ul>
</div>
</div>
<p>Dans ce tutoriel, on considère une fonction jouet <span class="math inline">\(f\)</span> qui dépend d’une entrée <span class="math inline">\(x\)</span> et de paramètres <span class="math inline">\(a, b\)</span>.</p>
<p><span class="math display">\[
f: (x, a, b) \in \mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R} \mapsto \tanh(a^\top x + b) \in \mathbb{R}
\]</span></p>
<p>On rappelle que <span class="math inline">\(tanh'(x) = 1 - \tanh^2(x)\)</span> et que <span class="math display">\[
\frac{\partial f}{\partial x} = a.(1 - \tanh^2(a^\top x + b)) \qquad \frac{\partial d}{\partial f} = x.(1 - \tanh^2(a^\top x + b)) \qquad \frac{\partial f}{\partial b} = (1 - \tanh^2(a^\top x + b))
\]</span> ou en mode matriciel <span class="math display">\[
\nabla f(x, a, b) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial a}, \frac{\partial f}{\partial b}\right)^\top
\]</span></p>
<p>On rappelle que la différentiation automatique fait appel à la <em>chain-rule</em>. Pour une fonction <span class="math inline">\(g\)</span> à valeurs dans <span class="math inline">\(\mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R}\)</span>, et en notant <span class="math inline">\(h = f \circ g\)</span>, on a <span class="math display">\[
\begin{align}
\nabla h(z) &amp; = \frac{\partial (f \circ g)(z)}{\partial z} = \nabla f(g(z))^\top \nabla g(z) \\
            &amp; = \frac{\partial (f \circ g)(z)}{\partial z} = \frac{\partial h(z)}{\partial g(z)} \frac{\partial g(z)}{\partial z}
\end{align}            
\]</span></p>
<p>On peut calculer <span class="math inline">\(\nabla h(z)\)</span> de deux façons:</p>
<ul>
<li>en mode forward (ou <code>jvp</code>): on commence par calculer <span class="math inline">\(v = \nabla g(z)\)</span> (aussi appelé <code>tangents</code>) et <span class="math inline">\(g(z)\)</span> et le gradient <span class="math inline">\(J = \nabla f(g(z))\)</span> avant de calculer le produit scalaire <span class="math inline">\(J v\)</span>. En pratique on écrit une fonction <code>jvp</code> <span class="math inline">\((x, v) \mapsto \nabla f(x) v (\symeq f(x + v) - f(x))\)</span> qui calcule directement le produit scalaire pour éviter d’avoir à matérialiser <span class="math inline">\(\nabla f\)</span>.</li>
<li>en mode reverse (ou <code>vjp</code>): on rétro-propage le gradient en calculant <span class="math inline">\(J = \frac{\partial h(z)}{\partial g(z)} = \nabla f(g(z))\)</span> (<span class="emoji" data-emoji="warning">⚠️</span> il faut avoir calculé et stocké <span class="math inline">\(g(z)\)</span> au préalable) et <span class="math inline">\(v = \frac{\partial g(z)}{\partial z}\)</span> et on calcule le produit scalaire <span class="math inline">\(v^\top J\)</span>. En pratique, on écrit une fonction <code>vjp</code> (ou <code>backward</code>) <span class="math inline">\((x, v) \mapsto v^\top \nabla f(x)\)</span> qui calcule directement le produit scalaire pour éviter d’avoir à matérialiser <span class="math inline">\(\nabla f\)</span>.</li>
</ul>
<p>En pratique il faut écrire <code>jvp</code> et <code>vjp</code> pour chaque fonction utilisée dans la composition.</p>
<section id="théorie-xx" class="level1">
<h1>Théorie [XX]</h1>
<p>Pour un rapide résumé de ce qu’est l’auto différentiation, et de son intérêt par rapport à d’autres stratégies de calcul numérique ou d’approximation des gradients d’une fonction, voici une vidéo assez complète en 14 min : </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/wG_nF1awSSY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="exemple-en-torch-mm-lc" class="level1">
<h1>Exemple en Torch [MM, LC]</h1>
<section id="en-utilisant-les-primitives-de-torch" class="level2">
<h2 class="anchored" data-anchor-id="en-utilisant-les-primitives-de-torch">En utilisant les primitives de torch</h2>
<div id="4880c12c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On definit notre fonction <span class="math inline">\(f\)</span> en torch.</p>
<div id="f27d8243" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_torch(x, a, b):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tanh(torch.dot(x, a) <span class="op">+</span> b)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On définit des valeurs pour lesquelles on sait calculer facilement le gradient.</p>
<div id="a860d080" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">2.</span>, <span class="fl">3.</span>], requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones(<span class="dv">2</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="op">-</span><span class="fl">2.</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Et on calcule les dérivées partielles (avec la convention <span class="math inline">\(\partial f / \partial x =\)</span> <code>x.grad</code>).</p>
<div id="f67754b3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Définit y par rapport à x</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f_torch(x, a, b)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Calcule et évalue le graphe de différentiation automatique de y par rapport à x </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Renvoie dy/dx</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>x.grad, a.grad, b.grad</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))</code></pre>
</div>
</div>
<p>On peut être plus concis pour calculer notre gradient (ici par rapport à <span class="math inline">\(x\)</span>) en définissant directement la fonction <span class="math inline">\((x, a, b) \mapsto \frac{\partial f}{\partial x}(x, a, b)\)</span> dans <code>df_torch_dx</code></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Le paramètre <code>argnums=0</code> précise qu’on calcule la dérivée par rapport au premier argument de <span class="math inline">\(f\)</span>, en l’occurence <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<div id="fecd4af1" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_torch_dx <span class="op">=</span> torch.func.grad(f_torch, argnums<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On vérifie que les deux façons de faire donnent le même résultat.</p>
<div id="070bcbb1" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(df_torch_dx(x, a, b), x.grad)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="en-utilisant-notre-propre-fonction" class="level2">
<h2 class="anchored" data-anchor-id="en-utilisant-notre-propre-fonction">En utilisant notre propre fonction</h2>
<p>Le code qui suit correspond à l’application des informations disponibles dans la <a href="https://docs.pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">documentation de torch</a> sur notre fonction example. Un autre tutoriel intéressant est <a href="https://apxml.com/courses/advanced-pytorch/chapter-1-pytorch-internals-autograd/custom-autograd-functions">le suivant</a>.</p>
<p>On doit définir 4 méthodes: - <code>forward</code> qui reçoit les entrées et calcule la sortie - <code>setup_context</code> qui stocke dans un objet <code>ctx</code> des tenseurs qui peuvent être réutilisés au moment du calcul de la dérivée (dans notre exemple, on a juste besoin de <span class="math inline">\(x\)</span>, <span class="math inline">\(a\)</span> et <span class="math inline">\(1 - \tanh^2(a^\top x + b)\)</span>. - <code>backward</code> (ou <code>vjp</code>) qui reçoit le gradient calculé en aval et renvoie le gradient, pour faire de la différentiation automatique en mode reverse. - <code>jvp</code> qui reçoit une différentielle calculée en amont et la multiplie en amont avant de la renvoyer, pour faire de la différentiation automatique en mode forward.</p>
<section id="définition-de-la-fonction" class="level3">
<h3 class="anchored" data-anchor-id="définition-de-la-fonction">Définition de la fonction</h3>
<div id="974c9d94" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> f_torch_manual(torch.autograd.Function):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    We can implement our own custom autograd Functions by subclassing</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    torch.autograd.Function and implementing the forward and backward passes</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    which operate on Tensors.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(x, a, b):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">        In the forward pass we receive a Tensor containing the input and return</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">        a Tensor containing the output.</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.tanh(torch.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setup_context(ctx, inputs, output):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">        ctx is a context object that can be used</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co">        to stash information for backward computation. You can cache tensors for</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">        use in the backward pass using the ``ctx.save_for_backward`` method. Other</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co">        objects can be stored directly as attributes on the ctx object, such as</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co">        ``ctx.my_object = my_object``.</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        x, a, b <span class="op">=</span> inputs</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">## save output to cut computation time</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        scaling <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> output.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="co"># tanh' = 1 - tanh^2</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        ctx.save_for_backward(x, a, scaling)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(ctx, grad_output):</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""        </span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the output, and we need to compute the gradient of the loss</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the input.</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co">        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        x, a, scaling <span class="op">=</span> ctx.saved_tensors</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        grad_x <span class="op">=</span> grad_output <span class="op">*</span> a <span class="op">*</span> scaling</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        grad_a <span class="op">=</span> grad_output <span class="op">*</span> x <span class="op">*</span> scaling</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        grad_b <span class="op">=</span> grad_output <span class="op">*</span> scaling</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad_x, grad_a, grad_b <span class="co"># on doit calculer les grad par rapport à tous les arguments rajouter grad par rapport à a et b</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> jvp(x, a, b, tangents):</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""                </span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="co">        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Vector v of small perturbations</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        tx, ta, tb <span class="op">=</span> tangents</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Matrix (in this case vector) of first order gradient</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> torch.tanh(torch.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        scaling <span class="op">=</span> (<span class="fl">1.</span> <span class="op">-</span> result.<span class="bu">pow</span>(<span class="dv">2</span>))</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        Jx <span class="op">=</span> a <span class="op">*</span> scaling</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        Ja <span class="op">=</span> x <span class="op">*</span> scaling</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>        Jb <span class="op">=</span> scaling</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Return J(x, a, b)v</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.dot(Jx, tx) <span class="op">+</span> torch.dot(Ja, ta) <span class="op">+</span> Jb <span class="op">*</span> tb</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vérification-des-dérivées" class="level3">
<h3 class="anchored" data-anchor-id="vérification-des-dérivées">Vérification des dérivées</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">En mode reverse</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">En mode forward</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="10a0cde8" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Définit f</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> f_torch_manual.<span class="bu">apply</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> f(x, a, b)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Calcule et évalue le graphe de différentiation automatique de y par rapport à x </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">## Réinitialise les gradients à zéro avant tout calcul </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>x.grad.zero_(), a.grad.zero_(), b.grad.zero_()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Renvoie dy/dx</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>x.grad, a.grad, b.grad</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))</code></pre>
</div>
</div>
<p>On vérifie qu’on obtient bien le même résultat qu’en laissant <code>torch</code> faire le calcul :party:.</p>
<p>On aurait aussi pu utiliser les opérateurs <em>fonctionnels</em> pour calculer la fonction dérivée (en utilisant le mode reverse)</p>
<div id="00e6f0b9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>f_grad_rev <span class="op">=</span> torch.func.jacrev(func<span class="op">=</span>f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>et vérifier que le résultat coincide avec le calcul fait à la main.</p>
<div id="1b1ae133" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>On calcule la dérivée par rapport à la première coordonnée de <span class="math inline">\(x\)</span></p>
<div id="83c9a5da" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tangents <span class="op">=</span> (torch.tensor([<span class="fl">1.</span>, <span class="fl">0.</span>]), torch.tensor([<span class="fl">0.</span>, <span class="fl">0.</span>]), torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>f_torch_manual.jvp(x <span class="op">=</span> x, a <span class="op">=</span> a, b <span class="op">=</span> b, tangents <span class="op">=</span> tangents)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor(0.0099, grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>puis par rapport à la deuxième coordonnée de <span class="math inline">\(a\)</span></p>
<div id="6d2c70fa" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tangents <span class="op">=</span> (torch.tensor([<span class="fl">0.</span>, <span class="fl">0.</span>]), torch.tensor([<span class="fl">0.</span>, <span class="fl">1.</span>]), torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>f_torch_manual.jvp(x <span class="op">=</span> x, a <span class="op">=</span> a, b <span class="op">=</span> b, tangents <span class="op">=</span> tangents)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor(0.0296, grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Et on valide que les résultats obtenus coïncident avec ceux obtenus en mode reverse et directement en utilisant <code>torch</code> <span class="emoji" data-emoji="grin">😁</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Avertissement
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>En théorie, on pourrait utiliser les opérateurs <em>fonctionnels</em> pour calculer la fonction dérivée (en utilisant le mode forward)</p>
<div id="cda1ecd0" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>f_grad_fwd <span class="op">=</span> torch.func.jacfwd(func<span class="op">=</span>f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>mais il faut définir une méthode statique <code>vmap</code> et je n’ai pas compris comment faire <span class="emoji" data-emoji="cry">😢</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="comparaison-des-temps-de-calculs" class="level2">
<h2 class="anchored" data-anchor-id="comparaison-des-temps-de-calculs">Comparaison des temps de calculs</h2>
<p>On compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.</p>
<div id="1fb3920b" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(np.arange(dim)<span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>), requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="fl">0.5</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor( (np.arange(dim) <span class="op">-</span> <span class="dv">37</span>) <span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>), requires_grad <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="93549a59" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>f_torch_grad_rev <span class="op">=</span> torch.func.jacrev(func<span class="op">=</span>f_torch, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_rev(x, a, b)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>f_torch_grad_fwd <span class="op">=</span> torch.func.jacfwd(func<span class="op">=</span>f_torch, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_fwd(x, a, b)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_grad_rev(x, a, b)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>424 μs ± 1.88 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
497 μs ± 2.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
763 μs ± 3.61 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</code></pre>
</div>
</div>
</section>
<section id="impact-de-jit" class="level2">
<h2 class="anchored" data-anchor-id="impact-de-jit">Impact de JIT</h2>
<p>On essaie de jitter nos fonctions pour vérifier si cela accélère le calcul des gradients.</p>
<p>Plus d’info sur le JIT dans <code>pytorch</code> sont disponibles dans <a href="https://docs.pytorch.org/docs/stable/jit.html">cette documentation</a>.</p>
<div id="5f725e9a" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>f_torch_grad_rev_jit <span class="op">=</span> torch.jit.trace(f_torch_grad_rev, (x, a, b))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>f_torch_grad_fwd_jit <span class="op">=</span> torch.jit.trace(f_torch_grad_fwd, (x, a, b))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cb6d6488" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_rev_jit(x, a, b)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_fwd_jit(x, a, b)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit f_grad_rev_jit(x, a, b)</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>56.1 μs ± 103 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
125 μs ± 649 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)</code></pre>
</div>
</div>
</section>
</section>
<section id="exemple-en-jax-hg-al" class="level1">
<h1>Exemple en JAX [HG, AL]</h1>
<p>Pour illustrer la différentiation automatique, nous allons utiliser JAX, sur une function simple dont nous connaissons les gradients analytiques.</p>
<section id="dérivée-par-rapport-à-x" class="level2">
<h2 class="anchored" data-anchor-id="dérivée-par-rapport-à-x">Dérivée par rapport à <span class="math inline">\(x\)</span></h2>
<div id="325f0fd5" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad, jit</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On définit une dimension arbitraire pour nos inputs</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">## On initialise les paramètres et le vecteur d'input de la fonction</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> jnp.arange(dim)<span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> (jnp.arange(dim) <span class="op">-</span> <span class="dv">37</span>) <span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co">## On définit une fonction simple dont on connaît les gradients analytiques</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="co">## On affiche la valeur de la fonction pour vérifier que tout est ok</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>f(x, a, b) </span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Array(0.56842977, dtype=float32)</code></pre>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Dérivée par rapport à <span class="math inline">\(x\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Dérivée par rapport à <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Dans un premier temps, on peut définir les gradients exacts de cette fonction à partir d’une formule analytique.</p>
<div id="09ff6c1c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dx(x, a, b):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Puis on définit les gradients via autograd et on vérifie que les résultats sont identiques.</p>
<div id="538b0d3f" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par défaut</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>grad_df_dx <span class="op">=</span> jax.grad(<span class="kw">lambda</span> x: f(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via jax.jacfwd</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>fwdgrad_df_dx <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> x: f(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On vérifie que les gradients retournent des valeurs identiques</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), df_dx(x, a, b))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Si les assertions ne retournent pas d'erreur, les gradients sont corrects</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'All good, we are ready to go!'</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All good, we are ready to go!</code></pre>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>On définit également les gradients exacts par rapport aux paramètres <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> pour vérifier que l’on pourrait les optimiser dans un algorithme d’apprentissage.</p>
<div id="9b43ec6c" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dab(x, a, b):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>), <span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Puis on définit ces gradients via autograd et on vérifie que les résultats sont identiques.</p>
<div id="618f4671" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par défaut</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>grad_df_dab <span class="op">=</span> jax.grad(<span class="kw">lambda</span> a_b: f(x, <span class="op">*</span>a_b), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via jax.jacfwd</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>fwdgrad_df_dab <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> a_b: f(x, <span class="op">*</span>a_b), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On vérifie que les gradients retournent des valeurs identiques</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Si les assertions ne retournent pas d'erreur, les gradients sont corrects</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'All good, we are ready to go!'</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All good, we are ready to go!</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Nous avons donc bien vérifié que les gradients calculés avec JAX sont identiques aux gradients analytiques.</p>
</section>
</section>
<section id="comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" class="level1">
<h1>Comparaison des temps de calcul en JAX entre autograd (backward et forward) et le calcul explicite des gradients</h1>
<p>Pour mettre en lumière les différences entre l’autograd backward et forward, nous allons définir une nouvelle fonction <span class="math inline">\(g\)</span> dont les sorties sont de plus grandes dimensions que les entrées.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true"><span class="math inline">\(x \in \mathbb{R}^d\)</span>, f(x) <span class="math inline">\(\in \mathbb{R}\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false"><span class="math inline">\(x \in \mathbb{R}\)</span>, g(x) <span class="math inline">\(\in \mathbb{R}^d\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<p>Comparaison des temps de calcul :</p>
<div id="84c6ddf4" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On lance une quantification du temps de calcul pour les trois méthodes (la commande .block_until_ready() est spécifique à JAX pour forcer l'évaluation des opérations asynchrones)</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit df_dx(x, a, b).block_until_ready()</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_df_dx(x).block_until_ready()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_df_dx(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>96 μs ± 281 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
1.4 ms ± 6.51 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
1.78 ms ± 6.96 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<p>On définit une nouvelle fonction <span class="math inline">\(g\)</span> qui retourne un vecteur de dimension <span class="math inline">\(d\)</span>:</p>
<div id="b9e3bda9" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On définit la dimension du vecteur de sortie</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>dim_g <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On initialise un point d'évaluation pour la fonction $g$</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>x_g <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On définit la nouvelle fonction $g$ qui retourne un vecteur de dimension $dim_g$</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(x):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">## On définit le gradient analytique de la fonction $g$</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dg_dx(x):</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([<span class="dv">1</span><span class="op">-</span> jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par défaut</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>grad_dg_dx <span class="op">=</span> jax.jacrev(<span class="kw">lambda</span> x: g(x), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via $jax.jacfwd$</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>fwdgrad_dg_dx <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> x: g(x), argnums<span class="op">=</span><span class="dv">0</span>)    </span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comparaison des temps de calcul :</p>
<div id="195ce8e5" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit dg_dx(x_g).block_until_ready()</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_dg_dx(x_g).block_until_ready()</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_dg_dx(x_g).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>13.6 ms ± 136 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
252 ms ± 809 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)
106 ms ± 565 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>On onbserve que le calcul explicite des gradients est plus rapide que les deux autres méthodes. Il y a donc un prix à payer pour la différentiation automatique, bien qu’elle soit très utile pour des fonctions complexes. Cependant, nous allons voir qu’en combinaison avec une autre stratégie clef en JAX, le jitting, les choses changent.</p>
</section>
<section id="jit-compilation-avec-jax" class="level1">
<h1>JIT compilation avec JAX</h1>
<p>La compilation JIT (Just-In-Time) permet d’optimiser les performances des fonctions en les compilant à la volée. JAX fournit la fonction <code>jit</code> pour cela.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Gradients jittés pour la fonction <span class="math inline">\(f\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Gradients jittés pour la fonction <span class="math inline">\(g\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div id="0c2d2ac4" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On redéfinit la fonction $f$ et ses gradients avec JIT</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jit(x, a, b):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dx_jit(x, a, b):</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co">## On compile les gradients de la fonction $f$ avec JIT</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>grad_f_jit <span class="op">=</span> jax.jit(jax.grad(<span class="kw">lambda</span> x: f_jit(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>fwdgrad_f_jit <span class="op">=</span> jax.jit(jax.jacfwd(<span class="kw">lambda</span> x: f_jit(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut maintenant mesurer le temps de calcul des gradients avec JIT</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit df_dx_jit(x, a, b).block_until_ready()</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_f_jit(x).block_until_ready()</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_f_jit(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>8.34 μs ± 14.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
5.65 μs ± 21.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
10.1 μs ± 172 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div id="6c379591" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On redéfinit la dimension du vecteur de sortie pour la fonction $g$ (les gradients sont sensibles à la dimension du vecteur de sortie, même avec JIT, notamment lorsque l'on compare l'autograd backward et forward)</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>dim_g <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co">## On redéfinit la fonction $f$ et ses gradients avec JIT</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_jit(x):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dg_dx_jit(x):</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([<span class="dv">1</span><span class="op">-</span> jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co">## Pour que le calcul soit rapide aussi, il faut ajouter un 'vmap' avant le 'jit' pour que la fonction soit vectorisée et puisse tirer parti de la compilation JIT.</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>grad_g_jit <span class="op">=</span> jax.jit(jax.vmap(jax.jacrev(<span class="kw">lambda</span> x: g_jit(x), argnums<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>fwdgrad_g_jit <span class="op">=</span> jax.jit(jax.vmap(jax.jacfwd(<span class="kw">lambda</span> x: g_jit(x), argnums<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut maintenant mesurer le temps de calcul des gradients avec JIT</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit dg_dx_jit(x).block_until_ready()</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_g_jit(x).block_until_ready()</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_g_jit(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>18 μs ± 476 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
25.7 μs ± 719 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
21.1 μs ± 1.48 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>On peut constater dans les deux cas que la compilation JIT permet d’accélérer considérablement le calcul des gradients, et rendre l’autograd négligeable d’un point de vue computationel. Dans le cas de la fonction <span class="math inline">\(g\)</span> qui retourne un vecteur, la vectorisation permet aussi d’accélérer le calcul. Ainsi la combinaison de l’autograd, du jitting et de la vectorisation permet d’obtenir des performances optimales et rendre la différentiation automatique aussi efficace que l’analytique, même pour des fonctions complexes.</p>
<section id="avec-vjp-et-jvp" class="level2">
<h2 class="anchored" data-anchor-id="avec-vjp-et-jvp">Avec VJP et JVP</h2>
<p>Dans les sections précédentes, <code>jax.jacrev</code> et <code>jax.jacfwd</code> utilisent, respectivement, les VJP et les JVP des opérations élémentaires de la fonction <code>f</code>. Dans certains cas, nous pouvons avoir besoin de définir nous-mêmes les VJP et JVP comme vu en introduction.</p>
<p>Nous allons alors définir les VJP et JVP pour <code>f</code> à l’aide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions élémentaires sous-jacentes ne seront alors plus utilisés.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">JVP</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">VJP</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<p><a href="https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#custom-jvps-with-jax-custom-jvp">Lien vers la documentation de JAX</a></p>
<p>Un JVP est capable de dévoiler une colonne de la jacobienne à la fois. Ce n’est pas adapté pour cette fonction dont la jacobienne est large. Une passe JVP ne peut dévoiler qu’une seule dérivée partielle : si l’on veut la dérivée par rapport à chaque dimension de <span class="math inline">\(x\)</span>, chaque dimension de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span>, il nous faut faire <span class="math inline">\(dim + dim + 1\)</span> fois des JVPs ce qui n’est pas du tout efficace. Nous l’avons vu dans la section précédente où en fait, <code>jax.jacfwd</code> doit en fait appeler tous ces JVPs (ce qui est fait de manière cachée à l’utilisateur).</p>
<p>Pour définir un <code>custom_jvp</code> en JAX, il faut attacher à <code>f</code>, une fonction <code>f_jvp</code>, qui prend deux entrées <code>primals</code> le point où l’on calcule le gradient et <code>tangents</code> le vecteur tangent (à voir aussi comme les gradients en amont du graphe que l’on parcourt en descendant). <code>f_jvp</code> retourne un tuple de deux vecteurs, <code>f(primals)</code> et le JVP <code>df_dx @ tangents</code>, où, bien sûr, <code>df_dx</code> contient l’expression analytique de la dérivée (c’est une matrice jacobienne mais elle n’est jamais stockée en mémoire car tout de suite <em>réduite</em> par le produit matriciel).</p>
<p>Nous avons vu que si <code>tangents</code> est un vecteur <em>one-hot encoded</em> nous dévoilons <strong>une colonne</strong> de la matrice jacobienne (celle où se situe le <span class="math inline">\(1\)</span>). Dans l’exemple ci-dessous nous calculons de manière <em>forward</em> <span class="math inline">\(\frac{\partial f}{\partial x_0}\)</span>.</p>
<div id="e1a84d36" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.custom_jvp</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="at">@f.defjvp</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jvp(primals, tangents):</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    x, a, b <span class="op">=</span> primals</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    x_dot, a_dot, b_dot <span class="op">=</span> tangents</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    primal_out <span class="op">=</span> f(x, a, b)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> primal_out, (jnp.dot(df_dx(x, a, b), x_dot) <span class="op">+</span>  jnp.dot((x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span> <span class="dv">2</span>)), a_dot) <span class="op">+</span> jnp.dot((<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span> <span class="dv">2</span>), b_dot))</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>x0_tangents <span class="op">=</span> jnp.zeros(dim)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>x0_tangents <span class="op">=</span> x0_tangents.at[<span class="dv">0</span>].<span class="bu">set</span>(<span class="dv">1</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>_, x_dot <span class="op">=</span> jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), <span class="fl">0.</span>))</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x)[<span class="dv">0</span>], x_dot)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
<p>On note que, s’il est défini, le <code>custom_jvp</code> sera utilisé par JAX, en mode forward <strong>et</strong> en mode backward. Notons aussi la syntaxe particulière émanant du fait que <code>f</code> prend trois arguments en entrée.</p>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<p><a href="https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules">Lien vers la documentation de JAX</a></p>
<p>Un VJP est capable de dévoiler une ligne de la jacobienne à la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de <span class="math inline">\(f\)</span> en un seul appel à JVP car c’est une matrice à une seule ligne. Nous l’avons vu dans la section précédente où en fait, <code>jax.jacrec</code> doit en fait appeler tous ces VJPs (ce qui est fait de manière cachée à l’utilisateur).</p>
<p>Si nous souhaitons explicitement définir le VJP, nous devons d’abord écrire une fonction qui décrit la passe forward. C’est ici <code>f_fwd</code> qui retourne <code>f(primal)</code> et des valeurs stockées pour le moment de la passe backward (à la manière de <code>save_for_backward</code> vu dans la section <code>pytorch</code> !). Il faut ici bien réfléchir à ce qui est nécessaire de stocker et ce qui est superflu, afin d’optimiser au mieux le code. Ici nous stockons <code>f(x,a,b)</code>, <code>x</code> et <code>a</code> car ces valeurs sont réutilisées dans la passe backward où nous calculons <span class="math inline">\(g. \frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial x}\)</span>, <span class="math inline">\(g.\frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial a}\)</span> et <span class="math inline">\(g.\frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial b}\)</span>. Avec <span class="math inline">\(g\)</span> le gradient provenant de l’aval du graphe pour calculer les VJPs (rappelons que nous les calculons de manière backward en remontant le graphe).</p>
<p>Nous comprenons à nouveau que si <code>g</code> est un vecteur <em>one-hot encoded</em> nous dévoilons <strong>une ligne</strong> de la matrice jacobienne (celle où se situe le <span class="math inline">\(1\)</span>).</p>
<p>Nous devons également écrire une fonction <code>f_bwd</code> qui prend en argument les valeurs stockées dans la passe forward ainsi que <code>g</code> défini dans le paragraphe précédent. Ici <code>g</code> est scalaire <code>f</code> a valeurs dans <span class="math inline">\(\mathbb{R}\)</span>. <code>f_bwd</code> retourne autant de sorties que <code>f</code> compte d’entrées.</p>
<div id="8284927a" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.custom_vjp</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_fwd(x, a, b):</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    primal_out <span class="op">=</span> f(x, a, b)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> primal_out, (x, a, primal_out)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_bwd(res, g):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    x, a, primal_out <span class="op">=</span> res</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g, x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g, (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>f.defvjp(f_fwd, f_bwd)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>_, f_vjp <span class="op">=</span> jax.vjp(f, x, a, b) <span class="co"># renvoie f(primal) et f_vjp qui est une fonction qui doit être évaluée en `g`</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), f_vjp(<span class="fl">1.</span>)[<span class="dv">0</span>])</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(<span class="fl">1.</span>)[i <span class="op">+</span> <span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
<p>Notons que la définition d’un <code>custom_vjp</code> redéfinit la fonction <code>grad</code> qui utilise donc aussi <code>f_fwd</code>. Ainsi, nous avons l’équivalence :</p>
<div id="c96788fe" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(f_vjp(<span class="fl">1.</span>)[<span class="dv">0</span>], jax.grad(f)(x, a, b))</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copié");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copié");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>