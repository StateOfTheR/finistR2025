<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mahendra Mariadassou">
<meta name="author" content="Hugo Gangloff">
<meta name="author" content="Arthur Leroy">
<meta name="author" content="Lucia Clarotto">

<title>Tutoriel de diff√©rentiation automatique ‚Äì Finist‚ÄôR 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b8ded578f01cf45f867b749dfb801973.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Pas de r√©sultats",
    "search-matching-documents-text": "documents trouv√©s",
    "search-copy-link-title": "Copier le lien vers la recherche",
    "search-hide-matches-text": "Cacher les correspondances additionnelles",
    "search-more-match-text": "correspondance de plus dans ce document",
    "search-more-matches-text": "correspondances de plus dans ce document",
    "search-clear-button-title": "Effacer",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Annuler",
    "search-submit-button-title": "Envoyer",
    "search-label": "Recherche"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Finist‚ÄôR 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Recherche"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Basculer la navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/StateOfTheR/finistR2025"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./instructions.html"> 
<span class="menu-text">Instructions</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-d√©veloppement" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">D√©veloppement</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-d√©veloppement">    
        <li>
    <a class="dropdown-item" href="./how_to_build_your_package.html">
 <span class="dropdown-text">How to build your package - Julia edition</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./positron.qmd">
 <span class="dropdown-text">Positron</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link active" href="./autodiff.html" aria-current="page"> 
<span class="menu-text">Diff√©rentiation automatique</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./animint.html"> 
<span class="menu-text">animint2</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./hsmc.qmd"> 
<span class="menu-text">HMSC</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./traccar.html"> <i class="bi bi-Bicycle" role="img">
</i> 
<span class="menu-text">GPS ‚Äì traccar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./sparse_cholesky.html"> 
<span class="menu-text">Sparse Cholesky</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./how_to_build_your_package.html"> 
<span class="menu-text">How to build your package - Julia edition</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sur cette page</h2>
   
  <ul>
  <li><a href="#th√©orie-xx" id="toc-th√©orie-xx" class="nav-link active" data-scroll-target="#th√©orie-xx">Th√©orie [XX]</a></li>
  <li><a href="#exemple-en-torch-mm-lc" id="toc-exemple-en-torch-mm-lc" class="nav-link" data-scroll-target="#exemple-en-torch-mm-lc">Exemple en Torch [MM, LC]</a>
  <ul class="collapse">
  <li><a href="#en-utilisant-les-primitives-de-torch" id="toc-en-utilisant-les-primitives-de-torch" class="nav-link" data-scroll-target="#en-utilisant-les-primitives-de-torch">En utilisant les primitives de torch</a></li>
  <li><a href="#en-utilisant-notre-propre-fonction" id="toc-en-utilisant-notre-propre-fonction" class="nav-link" data-scroll-target="#en-utilisant-notre-propre-fonction">En utilisant notre propre fonction</a>
  <ul class="collapse">
  <li><a href="#d√©finition-de-la-fonction" id="toc-d√©finition-de-la-fonction" class="nav-link" data-scroll-target="#d√©finition-de-la-fonction">D√©finition de la fonction</a></li>
  <li><a href="#v√©rification-des-d√©riv√©es" id="toc-v√©rification-des-d√©riv√©es" class="nav-link" data-scroll-target="#v√©rification-des-d√©riv√©es">V√©rification des d√©riv√©es</a></li>
  </ul></li>
  <li><a href="#comparaison-des-temps-de-calculs" id="toc-comparaison-des-temps-de-calculs" class="nav-link" data-scroll-target="#comparaison-des-temps-de-calculs">Comparaison des temps de calculs</a></li>
  <li><a href="#impact-de-jit" id="toc-impact-de-jit" class="nav-link" data-scroll-target="#impact-de-jit">Impact de JIT</a></li>
  </ul></li>
  <li><a href="#exemple-en-jax-hg-al" id="toc-exemple-en-jax-hg-al" class="nav-link" data-scroll-target="#exemple-en-jax-hg-al">Exemple en JAX [HG, AL]</a>
  <ul class="collapse">
  <li><a href="#d√©riv√©e-par-rapport-√†-x" id="toc-d√©riv√©e-par-rapport-√†-x" class="nav-link" data-scroll-target="#d√©riv√©e-par-rapport-√†-x">D√©riv√©e par rapport √† <span class="math inline">\(x\)</span></a></li>
  </ul></li>
  <li><a href="#comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" id="toc-comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" class="nav-link" data-scroll-target="#comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients">Comparaison des temps de calcul en JAX entre autograd (backward et forward) et le calcul explicite des gradients</a></li>
  <li><a href="#jit-compilation-avec-jax" id="toc-jit-compilation-avec-jax" class="nav-link" data-scroll-target="#jit-compilation-avec-jax">JIT compilation avec JAX</a>
  <ul class="collapse">
  <li><a href="#avec-vjp-et-jvp" id="toc-avec-vjp-et-jvp" class="nav-link" data-scroll-target="#avec-vjp-et-jvp">Avec VJP et JVP</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Tutoriel de diff√©rentiation automatique</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Auteur¬∑rice¬∑s</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Mahendra Mariadassou <a href="mailto:mahendra.mariadassou@inrae.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-2986-354X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            INRAE - MaIAGE
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Hugo Gangloff <a href="mailto:hugo.gangloff@inrae.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            INRAE - MIA Paris Saclay
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Arthur Leroy </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Lucia Clarotto </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Date de publication</div>
    <div class="quarto-title-meta-contents">
      <p class="date">18 ao√ªt 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modifi√©</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">22 ao√ªt 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>L‚Äôobectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d‚Äôune fonction qui n‚Äôest disponible dans les primitives fournies par JAX/Torch. Les deux cas d‚Äôusage envisag√©es sont:</p>
<ul>
<li>l‚Äôutilisation d‚Äôune fonction non diff√©rentiable pour lesquels on veut √©crire une d√©riv√©e ‚Äúnon-standard‚Äù afin de pouvoir l‚Äôutiliser dans JAX/Torch</li>
<li>l‚Äôutilisation d‚Äôune fonction donc une approximation analytique de la d√©riv√©e est disponible mais qui n‚Äôest pas impl√©ment√©e dans JAX/Torch</li>
</ul>
</div>
</div>
<p>Dans ce tutoriel, on consid√®re une fonction jouet <span class="math inline">\(f\)</span> qui d√©pend d‚Äôune entr√©e <span class="math inline">\(x\)</span> et de param√®tres <span class="math inline">\(a, b\)</span>.</p>
<p><span class="math display">\[
f: (x, a, b) \in \mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R} \mapsto \tanh(a^\top x + b) \in \mathbb{R}
\]</span></p>
<p>On rappelle que <span class="math inline">\(tanh'(x) = 1 - \tanh^2(x)\)</span> et que <span class="math display">\[
\frac{\partial f}{\partial x} = a.(1 - \tanh^2(a^\top x + b)) \qquad \frac{\partial d}{\partial f} = x.(1 - \tanh^2(a^\top x + b)) \qquad \frac{\partial f}{\partial b} = (1 - \tanh^2(a^\top x + b))
\]</span> ou en mode matriciel <span class="math display">\[
\nabla f(x, a, b) = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial a}, \frac{\partial f}{\partial b}\right)^\top
\]</span></p>
<p>On rappelle que la diff√©rentiation automatique fait appel √† la <em>chain-rule</em>. Pour une fonction <span class="math inline">\(g\)</span> √† valeurs dans <span class="math inline">\(\mathbb{R}^p \times \mathbb{R}^p \times \mathbb{R}\)</span>, et en notant <span class="math inline">\(h = f \circ g\)</span>, on a <span class="math display">\[
\begin{align}
\nabla h(z) &amp; = \frac{\partial (f \circ g)(z)}{\partial z} = \nabla f(g(z))^\top \nabla g(z) \\
            &amp; = \frac{\partial (f \circ g)(z)}{\partial z} = \frac{\partial h(z)}{\partial g(z)} \frac{\partial g(z)}{\partial z}
\end{align}            
\]</span></p>
<p>On peut calculer <span class="math inline">\(\nabla h(z)\)</span> de deux fa√ßons:</p>
<ul>
<li>en mode forward (ou <code>jvp</code>): on commence par calculer <span class="math inline">\(v = \nabla g(z)\)</span> (aussi appel√© <code>tangents</code>) et <span class="math inline">\(g(z)\)</span> et le gradient <span class="math inline">\(J = \nabla f(g(z))\)</span> avant de calculer le produit scalaire <span class="math inline">\(J v\)</span>. En pratique on √©crit une fonction <code>jvp</code> <span class="math inline">\((x, v) \mapsto \nabla f(x) v (\symeq f(x + v) - f(x))\)</span> qui calcule directement le produit scalaire pour √©viter d‚Äôavoir √† mat√©rialiser <span class="math inline">\(\nabla f\)</span>.</li>
<li>en mode reverse (ou <code>vjp</code>): on r√©tro-propage le gradient en calculant <span class="math inline">\(J = \frac{\partial h(z)}{\partial g(z)} = \nabla f(g(z))\)</span> (<span class="emoji" data-emoji="warning">‚ö†Ô∏è</span> il faut avoir calcul√© et stock√© <span class="math inline">\(g(z)\)</span> au pr√©alable) et <span class="math inline">\(v = \frac{\partial g(z)}{\partial z}\)</span> et on calcule le produit scalaire <span class="math inline">\(v^\top J\)</span>. En pratique, on √©crit une fonction <code>vjp</code> (ou <code>backward</code>) <span class="math inline">\((x, v) \mapsto v^\top \nabla f(x)\)</span> qui calcule directement le produit scalaire pour √©viter d‚Äôavoir √† mat√©rialiser <span class="math inline">\(\nabla f\)</span>.</li>
</ul>
<p>En pratique il faut √©crire <code>jvp</code> et <code>vjp</code> pour chaque fonction utilis√©e dans la composition.</p>
<section id="th√©orie-xx" class="level1">
<h1>Th√©orie [XX]</h1>
<p>Pour un rapide r√©sum√© de ce qu‚Äôest l‚Äôauto diff√©rentiation, et de son int√©r√™t par rapport √† d‚Äôautres strat√©gies de calcul num√©rique ou d‚Äôapproximation des gradients d‚Äôune fonction, voici une vid√©o assez compl√®te en 14 min : </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/wG_nF1awSSY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="exemple-en-torch-mm-lc" class="level1">
<h1>Exemple en Torch [MM, LC]</h1>
<section id="en-utilisant-les-primitives-de-torch" class="level2">
<h2 class="anchored" data-anchor-id="en-utilisant-les-primitives-de-torch">En utilisant les primitives de torch</h2>
<div id="4880c12c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On definit notre fonction <span class="math inline">\(f\)</span> en torch.</p>
<div id="f27d8243" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_torch(x, a, b):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tanh(torch.dot(x, a) <span class="op">+</span> b)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On d√©finit des valeurs pour lesquelles on sait calculer facilement le gradient.</p>
<div id="a860d080" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">2.</span>, <span class="fl">3.</span>], requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones(<span class="dv">2</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="op">-</span><span class="fl">2.</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Et on calcule les d√©riv√©es partielles (avec la convention <span class="math inline">\(\partial f / \partial x =\)</span> <code>x.grad</code>).</p>
<div id="f67754b3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">## D√©finit y par rapport √† x</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f_torch(x, a, b)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Renvoie dy/dx</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>x.grad, a.grad, b.grad</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))</code></pre>
</div>
</div>
<p>On peut √™tre plus concis pour calculer notre gradient (ici par rapport √† <span class="math inline">\(x\)</span>) en d√©finissant directement la fonction <span class="math inline">\((x, a, b) \mapsto \frac{\partial f}{\partial x}(x, a, b)\)</span> dans <code>df_torch_dx</code></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Le param√®tre <code>argnums=0</code> pr√©cise qu‚Äôon calcule la d√©riv√©e par rapport au premier argument de <span class="math inline">\(f\)</span>, en l‚Äôoccurence <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<div id="fecd4af1" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>df_torch_dx <span class="op">=</span> torch.func.grad(f_torch, argnums<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On v√©rifie que les deux fa√ßons de faire donnent le m√™me r√©sultat.</p>
<div id="070bcbb1" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(df_torch_dx(x, a, b), x.grad)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="en-utilisant-notre-propre-fonction" class="level2">
<h2 class="anchored" data-anchor-id="en-utilisant-notre-propre-fonction">En utilisant notre propre fonction</h2>
<p>Le code qui suit correspond √† l‚Äôapplication des informations disponibles dans la <a href="https://docs.pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">documentation de torch</a> sur notre fonction example. Un autre tutoriel int√©ressant est <a href="https://apxml.com/courses/advanced-pytorch/chapter-1-pytorch-internals-autograd/custom-autograd-functions">le suivant</a>.</p>
<p>On doit d√©finir 4 m√©thodes: - <code>forward</code> qui re√ßoit les entr√©es et calcule la sortie - <code>setup_context</code> qui stocke dans un objet <code>ctx</code> des tenseurs qui peuvent √™tre r√©utilis√©s au moment du calcul de la d√©riv√©e (dans notre exemple, on a juste besoin de <span class="math inline">\(x\)</span>, <span class="math inline">\(a\)</span> et <span class="math inline">\(1 - \tanh^2(a^\top x + b)\)</span>. - <code>backward</code> (ou <code>vjp</code>) qui re√ßoit le gradient calcul√© en aval et renvoie le gradient, pour faire de la diff√©rentiation automatique en mode reverse. - <code>jvp</code> qui re√ßoit une diff√©rentielle calcul√©e en amont et la multiplie en amont avant de la renvoyer, pour faire de la diff√©rentiation automatique en mode forward.</p>
<section id="d√©finition-de-la-fonction" class="level3">
<h3 class="anchored" data-anchor-id="d√©finition-de-la-fonction">D√©finition de la fonction</h3>
<div id="974c9d94" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> f_torch_manual(torch.autograd.Function):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    We can implement our own custom autograd Functions by subclassing</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    torch.autograd.Function and implementing the forward and backward passes</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    which operate on Tensors.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(x, a, b):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">        In the forward pass we receive a Tensor containing the input and return</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">        a Tensor containing the output.</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.tanh(torch.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setup_context(ctx, inputs, output):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">        ctx is a context object that can be used</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co">        to stash information for backward computation. You can cache tensors for</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">        use in the backward pass using the ``ctx.save_for_backward`` method. Other</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co">        objects can be stored directly as attributes on the ctx object, such as</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co">        ``ctx.my_object = my_object``.</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        x, a, b <span class="op">=</span> inputs</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">## save output to cut computation time</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        scaling <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> output.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="co"># tanh' = 1 - tanh^2</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        ctx.save_for_backward(x, a, scaling)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(ctx, grad_output):</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""        </span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the output, and we need to compute the gradient of the loss</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the input.</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co">        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        x, a, scaling <span class="op">=</span> ctx.saved_tensors</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        grad_x <span class="op">=</span> grad_output <span class="op">*</span> a <span class="op">*</span> scaling</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        grad_a <span class="op">=</span> grad_output <span class="op">*</span> x <span class="op">*</span> scaling</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        grad_b <span class="op">=</span> grad_output <span class="op">*</span> scaling</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grad_x, grad_a, grad_b <span class="co"># on doit calculer les grad par rapport √† tous les arguments rajouter grad par rapport √† a et b</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> jvp(x, a, b, tangents):</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""                </span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="co">        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Vector v of small perturbations</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        tx, ta, tb <span class="op">=</span> tangents</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Matrix (in this case vector) of first order gradient</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> torch.tanh(torch.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        scaling <span class="op">=</span> (<span class="fl">1.</span> <span class="op">-</span> result.<span class="bu">pow</span>(<span class="dv">2</span>))</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        Jx <span class="op">=</span> a <span class="op">*</span> scaling</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        Ja <span class="op">=</span> x <span class="op">*</span> scaling</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>        Jb <span class="op">=</span> scaling</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Return J(x, a, b)v</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.dot(Jx, tx) <span class="op">+</span> torch.dot(Ja, ta) <span class="op">+</span> Jb <span class="op">*</span> tb</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="v√©rification-des-d√©riv√©es" class="level3">
<h3 class="anchored" data-anchor-id="v√©rification-des-d√©riv√©es">V√©rification des d√©riv√©es</h3>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">En mode reverse</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">En mode forward</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="10a0cde8" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">## D√©finit f</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> f_torch_manual.<span class="bu">apply</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> f(x, a, b)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x </span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">## R√©initialise les gradients √† z√©ro avant tout calcul </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>x.grad.zero_(), a.grad.zero_(), b.grad.zero_()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">## Renvoie dy/dx</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>x.grad, a.grad, b.grad</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))</code></pre>
</div>
</div>
<p>On v√©rifie qu‚Äôon obtient bien le m√™me r√©sultat qu‚Äôen laissant <code>torch</code> faire le calcul :party:.</p>
<p>On aurait aussi pu utiliser les op√©rateurs <em>fonctionnels</em> pour calculer la fonction d√©riv√©e (en utilisant le mode reverse)</p>
<div id="00e6f0b9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>f_grad_rev <span class="op">=</span> torch.func.jacrev(func<span class="op">=</span>f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>et v√©rifier que le r√©sultat coincide avec le calcul fait √† la main.</p>
<div id="1b1ae133" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>On calcule la d√©riv√©e par rapport √† la premi√®re coordonn√©e de <span class="math inline">\(x\)</span></p>
<div id="83c9a5da" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tangents <span class="op">=</span> (torch.tensor([<span class="fl">1.</span>, <span class="fl">0.</span>]), torch.tensor([<span class="fl">0.</span>, <span class="fl">0.</span>]), torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>f_torch_manual.jvp(x <span class="op">=</span> x, a <span class="op">=</span> a, b <span class="op">=</span> b, tangents <span class="op">=</span> tangents)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor(0.0099, grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>puis par rapport √† la deuxi√®me coordonn√©e de <span class="math inline">\(a\)</span></p>
<div id="6d2c70fa" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tangents <span class="op">=</span> (torch.tensor([<span class="fl">0.</span>, <span class="fl">0.</span>]), torch.tensor([<span class="fl">0.</span>, <span class="fl">1.</span>]), torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>f_torch_manual.jvp(x <span class="op">=</span> x, a <span class="op">=</span> a, b <span class="op">=</span> b, tangents <span class="op">=</span> tangents)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor(0.0296, grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Et on valide que les r√©sultats obtenus co√Øncident avec ceux obtenus en mode reverse et directement en utilisant <code>torch</code> <span class="emoji" data-emoji="grin">üòÅ</span></p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Avertissement
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>En th√©orie, on pourrait utiliser les op√©rateurs <em>fonctionnels</em> pour calculer la fonction d√©riv√©e (en utilisant le mode forward)</p>
<div id="cda1ecd0" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>f_grad_fwd <span class="op">=</span> torch.func.jacfwd(func<span class="op">=</span>f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>mais il faut d√©finir une m√©thode statique <code>vmap</code> et je n‚Äôai pas compris comment faire <span class="emoji" data-emoji="cry">üò¢</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="comparaison-des-temps-de-calculs" class="level2">
<h2 class="anchored" data-anchor-id="comparaison-des-temps-de-calculs">Comparaison des temps de calculs</h2>
<p>On compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.</p>
<div id="1fb3920b" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(np.arange(dim)<span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>), requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="fl">0.5</span>, requires_grad <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor( (np.arange(dim) <span class="op">-</span> <span class="dv">37</span>) <span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>), requires_grad <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="93549a59" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>f_torch_grad_rev <span class="op">=</span> torch.func.jacrev(func<span class="op">=</span>f_torch, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_rev(x, a, b)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>f_torch_grad_fwd <span class="op">=</span> torch.func.jacfwd(func<span class="op">=</span>f_torch, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_fwd(x, a, b)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_grad_rev(x, a, b)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>424 Œºs ¬± 1.88 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)
497 Œºs ¬± 2.4 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)
763 Œºs ¬± 3.61 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)</code></pre>
</div>
</div>
</section>
<section id="impact-de-jit" class="level2">
<h2 class="anchored" data-anchor-id="impact-de-jit">Impact de JIT</h2>
<p>On essaie de jitter nos fonctions pour v√©rifier si cela acc√©l√®re le calcul des gradients.</p>
<p>Plus d‚Äôinfo sur le JIT dans <code>pytorch</code> sont disponibles dans <a href="https://docs.pytorch.org/docs/stable/jit.html">cette documentation</a>.</p>
<div id="5f725e9a" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>f_torch_grad_rev_jit <span class="op">=</span> torch.jit.trace(f_torch_grad_rev, (x, a, b))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>f_torch_grad_fwd_jit <span class="op">=</span> torch.jit.trace(f_torch_grad_fwd, (x, a, b))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cb6d6488" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_rev_jit(x, a, b)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit f_torch_grad_fwd_jit(x, a, b)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># %timeit f_grad_rev_jit(x, a, b)</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>56.1 Œºs ¬± 103 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)
125 Œºs ¬± 649 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)</code></pre>
</div>
</div>
</section>
</section>
<section id="exemple-en-jax-hg-al" class="level1">
<h1>Exemple en JAX [HG, AL]</h1>
<p>Pour illustrer la diff√©rentiation automatique, nous allons utiliser JAX, sur une function simple dont nous connaissons les gradients analytiques.</p>
<section id="d√©riv√©e-par-rapport-√†-x" class="level2">
<h2 class="anchored" data-anchor-id="d√©riv√©e-par-rapport-√†-x">D√©riv√©e par rapport √† <span class="math inline">\(x\)</span></h2>
<div id="325f0fd5" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad, jit</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On d√©finit une dimension arbitraire pour nos inputs</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">## On initialise les param√®tres et le vecteur d'input de la fonction</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> jnp.arange(dim)<span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> (jnp.arange(dim) <span class="op">-</span> <span class="dv">37</span>) <span class="op">/</span> (dim<span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="co">## On d√©finit une fonction simple dont on conna√Æt les gradients analytiques</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="co">## On affiche la valeur de la fonction pour v√©rifier que tout est ok</span></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>f(x, a, b) </span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Array(0.56842977, dtype=float32)</code></pre>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">D√©riv√©e par rapport √† <span class="math inline">\(x\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">D√©riv√©e par rapport √† <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>Dans un premier temps, on peut d√©finir les gradients exacts de cette fonction √† partir d‚Äôune formule analytique.</p>
<div id="09ff6c1c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dx(x, a, b):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Puis on d√©finit les gradients via autograd et on v√©rifie que les r√©sultats sont identiques.</p>
<div id="538b0d3f" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par d√©faut</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>grad_df_dx <span class="op">=</span> jax.grad(<span class="kw">lambda</span> x: f(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via jax.jacfwd</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>fwdgrad_df_dx <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> x: f(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On v√©rifie que les gradients retournent des valeurs identiques</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), df_dx(x, a, b))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Si les assertions ne retournent pas d'erreur, les gradients sont corrects</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'All good, we are ready to go!'</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All good, we are ready to go!</code></pre>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>On d√©finit √©galement les gradients exacts par rapport aux param√®tres <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> pour v√©rifier que l‚Äôon pourrait les optimiser dans un algorithme d‚Äôapprentissage.</p>
<div id="9b43ec6c" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dab(x, a, b):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>), <span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span></span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Puis on d√©finit ces gradients via autograd et on v√©rifie que les r√©sultats sont identiques.</p>
<div id="618f4671" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par d√©faut</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>grad_df_dab <span class="op">=</span> jax.grad(<span class="kw">lambda</span> a_b: f(x, <span class="op">*</span>a_b), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via jax.jacfwd</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>fwdgrad_df_dab <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> a_b: f(x, <span class="op">*</span>a_b), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On v√©rifie que les gradients retournent des valeurs identiques</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">## Si les assertions ne retournent pas d'erreur, les gradients sont corrects</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'All good, we are ready to go!'</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All good, we are ready to go!</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Nous avons donc bien v√©rifi√© que les gradients calcul√©s avec JAX sont identiques aux gradients analytiques.</p>
</section>
</section>
<section id="comparaison-des-temps-de-calcul-en-jax-entre-autograd-backward-et-forward-et-le-calcul-explicite-des-gradients" class="level1">
<h1>Comparaison des temps de calcul en JAX entre autograd (backward et forward) et le calcul explicite des gradients</h1>
<p>Pour mettre en lumi√®re les diff√©rences entre l‚Äôautograd backward et forward, nous allons d√©finir une nouvelle fonction <span class="math inline">\(g\)</span> dont les sorties sont de plus grandes dimensions que les entr√©es.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true"><span class="math inline">\(x \in \mathbb{R}^d\)</span>, f(x) <span class="math inline">\(\in \mathbb{R}\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false"><span class="math inline">\(x \in \mathbb{R}\)</span>, g(x) <span class="math inline">\(\in \mathbb{R}^d\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<p>Comparaison des temps de calcul :</p>
<div id="84c6ddf4" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On lance une quantification du temps de calcul pour les trois m√©thodes (la commande .block_until_ready() est sp√©cifique √† JAX pour forcer l'√©valuation des op√©rations asynchrones)</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit df_dx(x, a, b).block_until_ready()</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_df_dx(x).block_until_ready()</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_df_dx(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>96 Œºs ¬± 281 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)
1.4 ms ¬± 6.51 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)
1.78 ms ¬± 6.96 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)</code></pre>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<p>On d√©finit une nouvelle fonction <span class="math inline">\(g\)</span> qui retourne un vecteur de dimension <span class="math inline">\(d\)</span>:</p>
<div id="b9e3bda9" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On d√©finit la dimension du vecteur de sortie</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>dim_g <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">## On initialise un point d'√©valuation pour la fonction $g$</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>x_g <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">## On d√©finit la nouvelle fonction $g$ qui retourne un vecteur de dimension $dim_g$</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(x):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">## On d√©finit le gradient analytique de la fonction $g$</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dg_dx(x):</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([<span class="dv">1</span><span class="op">-</span> jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">## jax.grad calcule la formule backward par d√©faut</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>grad_dg_dx <span class="op">=</span> jax.jacrev(<span class="kw">lambda</span> x: g(x), argnums<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut aussi calculer la formule forward via $jax.jacfwd$</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>fwdgrad_dg_dx <span class="op">=</span> jax.jacfwd(<span class="kw">lambda</span> x: g(x), argnums<span class="op">=</span><span class="dv">0</span>)    </span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comparaison des temps de calcul :</p>
<div id="195ce8e5" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit dg_dx(x_g).block_until_ready()</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_dg_dx(x_g).block_until_ready()</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_dg_dx(x_g).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>13.6 ms ¬± 136 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)
252 ms ¬± 809 Œºs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)
106 ms ¬± 565 Œºs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>On onbserve que le calcul explicite des gradients est plus rapide que les deux autres m√©thodes. Il y a donc un prix √† payer pour la diff√©rentiation automatique, bien qu‚Äôelle soit tr√®s utile pour des fonctions complexes. Cependant, nous allons voir qu‚Äôen combinaison avec une autre strat√©gie clef en JAX, le jitting, les choses changent.</p>
</section>
<section id="jit-compilation-avec-jax" class="level1">
<h1>JIT compilation avec JAX</h1>
<p>La compilation JIT (Just-In-Time) permet d‚Äôoptimiser les performances des fonctions en les compilant √† la vol√©e. JAX fournit la fonction <code>jit</code> pour cela.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true">Gradients jitt√©s pour la fonction <span class="math inline">\(f\)</span></a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false">Gradients jitt√©s pour la fonction <span class="math inline">\(g\)</span></a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div id="0c2d2ac4" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On red√©finit la fonction $f$ et ses gradients avec JIT</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jit(x, a, b):</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_dx_jit(x, a, b):</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b) <span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co">## On compile les gradients de la fonction $f$ avec JIT</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>grad_f_jit <span class="op">=</span> jax.jit(jax.grad(<span class="kw">lambda</span> x: f_jit(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>fwdgrad_f_jit <span class="op">=</span> jax.jit(jax.jacfwd(<span class="kw">lambda</span> x: f_jit(x, a, b), argnums<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut maintenant mesurer le temps de calcul des gradients avec JIT</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit df_dx_jit(x, a, b).block_until_ready()</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_f_jit(x).block_until_ready()</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_f_jit(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>8.34 Œºs ¬± 14.3 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)
5.65 Œºs ¬± 21.6 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)
10.1 Œºs ¬± 172 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)</code></pre>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div id="6c379591" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">## On red√©finit la dimension du vecteur de sortie pour la fonction $g$ (les gradients sont sensibles √† la dimension du vecteur de sortie, m√™me avec JIT, notamment lorsque l'on compare l'autograd backward et forward)</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>dim_g <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co">## On red√©finit la fonction $f$ et ses gradients avec JIT</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g_jit(x):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="at">@jit</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dg_dx_jit(x):</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array([<span class="dv">1</span><span class="op">-</span> jnp.tanh(x <span class="op">+</span> i<span class="op">/</span>dim_g)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dim_g)])</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co">## Pour que le calcul soit rapide aussi, il faut ajouter un 'vmap' avant le 'jit' pour que la fonction soit vectoris√©e et puisse tirer parti de la compilation JIT.</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>grad_g_jit <span class="op">=</span> jax.jit(jax.vmap(jax.jacrev(<span class="kw">lambda</span> x: g_jit(x), argnums<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>fwdgrad_g_jit <span class="op">=</span> jax.jit(jax.vmap(jax.jacfwd(<span class="kw">lambda</span> x: g_jit(x), argnums<span class="op">=</span><span class="dv">0</span>)))</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co">## On peut maintenant mesurer le temps de calcul des gradients avec JIT</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit dg_dx_jit(x).block_until_ready()</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit grad_g_jit(x).block_until_ready()</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit fwdgrad_g_jit(x).block_until_ready()</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>18 Œºs ¬± 476 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)
25.7 Œºs ¬± 719 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)
21.1 Œºs ¬± 1.48 Œºs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>On peut constater dans les deux cas que la compilation JIT permet d‚Äôacc√©l√©rer consid√©rablement le calcul des gradients, et rendre l‚Äôautograd n√©gligeable d‚Äôun point de vue computationel. Dans le cas de la fonction <span class="math inline">\(g\)</span> qui retourne un vecteur, la vectorisation permet aussi d‚Äôacc√©l√©rer le calcul. Ainsi la combinaison de l‚Äôautograd, du jitting et de la vectorisation permet d‚Äôobtenir des performances optimales et rendre la diff√©rentiation automatique aussi efficace que l‚Äôanalytique, m√™me pour des fonctions complexes.</p>
<section id="avec-vjp-et-jvp" class="level2">
<h2 class="anchored" data-anchor-id="avec-vjp-et-jvp">Avec VJP et JVP</h2>
<p>Dans les sections pr√©c√©dentes, <code>jax.jacrev</code> et <code>jax.jacfwd</code> utilisent, respectivement, les VJP et les JVP des op√©rations √©l√©mentaires de la fonction <code>f</code>. Dans certains cas, nous pouvons avoir besoin de d√©finir nous-m√™mes les VJP et JVP comme vu en introduction.</p>
<p>Nous allons alors d√©finir les VJP et JVP pour <code>f</code> √† l‚Äôaide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions √©l√©mentaires sous-jacentes ne seront alors plus utilis√©s.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true">JVP</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false">VJP</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<p><a href="https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#custom-jvps-with-jax-custom-jvp">Lien vers la documentation de JAX</a></p>
<p>Un JVP est capable de d√©voiler une colonne de la jacobienne √† la fois. Ce n‚Äôest pas adapt√© pour cette fonction dont la jacobienne est large. Une passe JVP ne peut d√©voiler qu‚Äôune seule d√©riv√©e partielle : si l‚Äôon veut la d√©riv√©e par rapport √† chaque dimension de <span class="math inline">\(x\)</span>, chaque dimension de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span>, il nous faut faire <span class="math inline">\(dim + dim + 1\)</span> fois des JVPs ce qui n‚Äôest pas du tout efficace. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, <code>jax.jacfwd</code> doit en fait appeler tous ces JVPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).</p>
<p>Pour d√©finir un <code>custom_jvp</code> en JAX, il faut attacher √† <code>f</code>, une fonction <code>f_jvp</code>, qui prend deux entr√©es <code>primals</code> le point o√π l‚Äôon calcule le gradient et <code>tangents</code> le vecteur tangent (√† voir aussi comme les gradients en amont du graphe que l‚Äôon parcourt en descendant). <code>f_jvp</code> retourne un tuple de deux vecteurs, <code>f(primals)</code> et le JVP <code>df_dx @ tangents</code>, o√π, bien s√ªr, <code>df_dx</code> contient l‚Äôexpression analytique de la d√©riv√©e (c‚Äôest une matrice jacobienne mais elle n‚Äôest jamais stock√©e en m√©moire car tout de suite <em>r√©duite</em> par le produit matriciel).</p>
<p>Nous avons vu que si <code>tangents</code> est un vecteur <em>one-hot encoded</em> nous d√©voilons <strong>une colonne</strong> de la matrice jacobienne (celle o√π se situe le <span class="math inline">\(1\)</span>). Dans l‚Äôexemple ci-dessous nous calculons de mani√®re <em>forward</em> <span class="math inline">\(\frac{\partial f}{\partial x_0}\)</span>.</p>
<div id="e1a84d36" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.custom_jvp</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="at">@f.defjvp</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_jvp(primals, tangents):</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    x, a, b <span class="op">=</span> primals</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    x_dot, a_dot, b_dot <span class="op">=</span> tangents</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    primal_out <span class="op">=</span> f(x, a, b)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> primal_out, (jnp.dot(df_dx(x, a, b), x_dot) <span class="op">+</span>  jnp.dot((x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span> <span class="dv">2</span>)), a_dot) <span class="op">+</span> jnp.dot((<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span> <span class="dv">2</span>), b_dot))</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>x0_tangents <span class="op">=</span> jnp.zeros(dim)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>x0_tangents <span class="op">=</span> x0_tangents.at[<span class="dv">0</span>].<span class="bu">set</span>(<span class="dv">1</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>_, x_dot <span class="op">=</span> jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), <span class="fl">0.</span>))</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x)[<span class="dv">0</span>], x_dot)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
<p>On note que, s‚Äôil est d√©fini, le <code>custom_jvp</code> sera utilis√© par JAX, en mode forward <strong>et</strong> en mode backward. Notons aussi la syntaxe particuli√®re √©manant du fait que <code>f</code> prend trois arguments en entr√©e.</p>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<p><a href="https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules">Lien vers la documentation de JAX</a></p>
<p>Un VJP est capable de d√©voiler une ligne de la jacobienne √† la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de <span class="math inline">\(f\)</span> en un seul appel √† JVP car c‚Äôest une matrice √† une seule ligne. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, <code>jax.jacrec</code> doit en fait appeler tous ces VJPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).</p>
<p>Si nous souhaitons explicitement d√©finir le VJP, nous devons d‚Äôabord √©crire une fonction qui d√©crit la passe forward. C‚Äôest ici <code>f_fwd</code> qui retourne <code>f(primal)</code> et des valeurs stock√©es pour le moment de la passe backward (√† la mani√®re de <code>save_for_backward</code> vu dans la section <code>pytorch</code> !). Il faut ici bien r√©fl√©chir √† ce qui est n√©cessaire de stocker et ce qui est superflu, afin d‚Äôoptimiser au mieux le code. Ici nous stockons <code>f(x,a,b)</code>, <code>x</code> et <code>a</code> car ces valeurs sont r√©utilis√©es dans la passe backward o√π nous calculons <span class="math inline">\(g. \frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial x}\)</span>, <span class="math inline">\(g.\frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial a}\)</span> et <span class="math inline">\(g.\frac{\partial \mathrm{tanh}(f(x,a,b))}{\partial b}\)</span>. Avec <span class="math inline">\(g\)</span> le gradient provenant de l‚Äôaval du graphe pour calculer les VJPs (rappelons que nous les calculons de mani√®re backward en remontant le graphe).</p>
<p>Nous comprenons √† nouveau que si <code>g</code> est un vecteur <em>one-hot encoded</em> nous d√©voilons <strong>une ligne</strong> de la matrice jacobienne (celle o√π se situe le <span class="math inline">\(1\)</span>).</p>
<p>Nous devons √©galement √©crire une fonction <code>f_bwd</code> qui prend en argument les valeurs stock√©es dans la passe forward ainsi que <code>g</code> d√©fini dans le paragraphe pr√©c√©dent. Ici <code>g</code> est scalaire <code>f</code> a valeurs dans <span class="math inline">\(\mathbb{R}\)</span>. <code>f_bwd</code> retourne autant de sorties que <code>f</code> compte d‚Äôentr√©es.</p>
<div id="8284927a" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.custom_vjp</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a, b):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.tanh(jnp.dot(a, x) <span class="op">+</span> b)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_fwd(x, a, b):</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    primal_out <span class="op">=</span> f(x, a, b)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> primal_out, (x, a, primal_out)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_bwd(res, g):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    x, a, primal_out <span class="op">=</span> res</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (a <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g, x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g, (<span class="dv">1</span> <span class="op">-</span> primal_out <span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> g)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>f.defvjp(f_fwd, f_bwd)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>_, f_vjp <span class="op">=</span> jax.vjp(f, x, a, b) <span class="co"># renvoie f(primal) et f_vjp qui est une fonction qui doit √™tre √©valu√©e en `g`</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(grad_df_dx(x), f_vjp(<span class="fl">1.</span>)[<span class="dv">0</span>])</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(<span class="fl">1.</span>)[i <span class="op">+</span> <span class="dv">1</span>]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>))</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
<p>Notons que la d√©finition d‚Äôun <code>custom_vjp</code> red√©finit la fonction <code>grad</code> qui utilise donc aussi <code>f_fwd</code>. Ainsi, nous avons l‚Äô√©quivalence :</p>
<div id="c96788fe" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(f_vjp(<span class="fl">1.</span>)[<span class="dv">0</span>], jax.grad(f)(x, a, b))</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All right!"</span>)</span></code><button title="Copier vers le presse-papier" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>All right!</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copi√©");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copi√©");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>