{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Tutoriel de différentiation automatique\"\n",
        "from: markdown+emoji\n",
        "lang: fr\n",
        "format: html\n",
        "execute:\n",
        "    cache: true\n",
        "toc: true\n",
        "author:\n",
        "  - name: Mahendra Mariadassou\n",
        "    orcid: 0000-0003-2986-354X\n",
        "    email: mahendra.mariadassou@inrae.fr\n",
        "    affiliations:\n",
        "      - name: INRAE - MaIAGE\n",
        "        adress: Domaine de Vilvert\n",
        "        city: Jouy-en-Josas\n",
        "        state: France\n",
        "  - name: Hugo Gangloff\n",
        "  - name: Arthur Leroy\n",
        "  - name: Lucia Clarotto\n",
        "date: \"2025-08-18\"\n",
        "date-modified: today\n",
        "date-format: \"[Last Updated on] MMMM, YYYY\"\n",
        "---\n",
        "\n",
        "\n",
        "::: callout-note\n",
        "L'obectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP et le VJP d'une fonction simple:\n",
        "$$\n",
        "f: (x, a, b) \\in \\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R} \\mapsto \\tanh(a^\\top x + b) \\in \\mathbb{R}\n",
        "$$\n",
        "On considère les dérivées par rapport à l'entrée $x$ et aux paramètres $a, b$. On rappelle que $tanh'(x) = 1 - \\tanh^2(x)$ et que \n",
        "\n",
        "$$\n",
        "\\frac{\\partial d}{\\partial x} = a.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial a} = x.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial b} = (1 - \\tanh^2(a^\\top x + b))\n",
        "$$\n",
        ":::\n",
        "\n",
        "# Théorie [XX]\n",
        "\n",
        "# Exemple en Torch [MM, LC]\n"
      ],
      "id": "18fa9293"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "id": "b1a4cb9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On definit notre fonction $f$ en torch. \n"
      ],
      "id": "7f127468"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f_torch(x, a, b):\n",
        "    return torch.tanh(torch.dot(x, a) + b)"
      ],
      "id": "24c77c6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On définit des valeurs pour lesquelles on sait calculer facilement le gradient. \n"
      ],
      "id": "56a36529"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = torch.tensor([2., 3.], requires_grad = True)\n",
        "a = torch.ones(2, requires_grad = True)\n",
        "b = torch.tensor(-2., requires_grad = True)"
      ],
      "id": "74b3d3ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et on calcule les dérivées partielles ($\\partial f / \\partial x = `x.grad`).\n"
      ],
      "id": "01c83624"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Définit y par rapport à x\n",
        "y = f_torch(x, a, b)\n",
        "y\n",
        "## Calcule et évalue le graphe de différentiation automatique de y par rapport à x \n",
        "y.backward()\n",
        "## Renvoie dy/dx\n",
        "x.grad, a.grad, b.grad"
      ],
      "id": "b45ac6c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: panel-tabset\n",
        "\n",
        "Le code qui suit correspond à l'application des informations disponibles dans la [documentation de torch](https://docs.pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd) sur notre fonction example. \n",
        "\n",
        "On doit définir 4 méthodes: \n",
        "- `forward` qui reçoit les entrées et calcule la sortie\n",
        "- `setup_context` qui stocke dans un objet `ctx` des tenseurs qui peuvent être réutilisés au moment du calcul de la dérivée\n",
        "- `backward` (ou `vjp`) qui reçoit le gradient calculé en aval et renvoie le gradient, pour faire de la différentiation automatique en mode reverse. \n",
        "- `jvp` qui reçoit le gradient calculé en amont et renvoie le gradient, pour faire de la différentiation automatique en mode forward. \n",
        "\n",
        "## Définition de la fonction \n"
      ],
      "id": "ba6ddb3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class f_torch_manual(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(x, a, b):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache tensors for\n",
        "        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n",
        "        objects can be stored directly as attributes on the ctx object, such as\n",
        "        ``ctx.my_object = my_object``. Check out `Extending torch.autograd <https://docs.pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd>`_\n",
        "        for further details.\n",
        "        \"\"\"\n",
        "        output = torch.tanh(torch.dot(a, x) + b)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def setup_context(ctx, inputs, output):\n",
        "        x, a, b = inputs\n",
        "        ctx.result = output\n",
        "        ## save output to cut computation time\n",
        "        ctx.save_for_backward(x, a, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"        \n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation\n",
        "        \"\"\"\n",
        "        x, a, b = ctx.saved_tensors\n",
        "        result  = ctx.result\n",
        "        mult_fact = (1 - result.pow(2))\n",
        "        grad_x = grad_output * a * mult_fact\n",
        "        grad_a = grad_output * x * mult_fact\n",
        "        grad_b = grad_output * mult_fact\n",
        "        return grad_x, grad_a, grad_b # on doit calculer les grad par rapport à tous les arguments rajouter grad par rapport à a et b\n",
        "\n",
        "    @staticmethod\n",
        "    def jvp(x, a, b, tangents):\n",
        "        \"\"\"                \n",
        "        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation\n",
        "        \"\"\"\n",
        "        ## Vector v of small perturbations\n",
        "        tx, ta, tb = tangents\n",
        "        ## Matrix (in this case vector) of first order gradient\n",
        "        result = torch.tanh(torch.dot(a, x) + b)\n",
        "        mult_fact = (1 - result.pow(2))\n",
        "        Jx = a * mult_fact\n",
        "        Ja = x * mult_fact\n",
        "        Jb = mult_fact\n",
        "        ## Return J(x, a, b)v\n",
        "        return torch.dot(Jx, tx) + torch.dot(Ja, ta) + Jb * tb"
      ],
      "id": "f5c3da4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dérivée par rapport à $x$, $a$ et $b$ \n",
        "\n",
        "### En mode reverse\n"
      ],
      "id": "0ba116e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Définit f\n",
        "f = f_torch_manual.apply\n",
        "z = f(x, a, b)\n",
        "## Calcule et évalue le graphe de différentiation automatique de y par rapport à x \n",
        "## Réinitialise les gradients à zéro avant tout calcul \n",
        "x.grad.zero_(), a.grad.zero_(), b.grad.zero_()\n",
        "z.backward()\n",
        "## Renvoie dy/dx\n",
        "x.grad, a.grad, b.grad"
      ],
      "id": "966096d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### En mode forward\n",
        "\n",
        "On calcule la dérivée par rapport à la première coordonnée de $x$\n"
      ],
      "id": "aabd3b95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tangents = (torch.tensor([1, 0]).float(), torch.tensor([0, 0]).float(), torch.tensor(0).float())\n",
        "f_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)"
      ],
      "id": "ff1ee370",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "puis par rapport à la deuxième coordonnée de $a$\n"
      ],
      "id": "7dd401a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tangents = (torch.tensor([0, 0]).float(), torch.tensor([0, 1]).float(), torch.tensor(0).float())\n",
        "f_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)"
      ],
      "id": "31a5c19e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et on valide que les résultats obtenus coïncident avec ceux obtenus en mode reverse et directement en utilisant `torch` :grin:\n",
        "\n",
        ":::\n",
        "\n",
        "# Exemple en JAX [HG, AL]\n",
        "\n",
        "Pour illustrer la différentiation automatique, nous allons utiliser JAX, sur une function simple dont nous connaissons les gradients analytiques.\n",
        "\n",
        "## Dérivée par rapport à $x$\n"
      ],
      "id": "bb2a4773"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit\n",
        "from jax import random\n",
        "\n",
        "## On définit une dimension arbitraire pour nos inputs\n",
        "dim = 100\n",
        "\n",
        "## On initialise les paramètres et le vecteur d'input de la fonction\n",
        "a = jnp.arange(dim)/ (dim*10)\n",
        "b = 0.5\n",
        "x = (jnp.arange(dim) - 37) / (dim*10)\n",
        "\n",
        "## On définit une fonction simple dont on connaît les gradients analytiques\n",
        "def f(x, a, b):\n",
        "    return jnp.tanh(jnp.dot(a, x) + b)\n",
        "\n",
        "## On affiche la valeur de la fonction pour vérifier que tout est ok\n",
        "f(x, a, b) "
      ],
      "id": "291d46f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: panel-tabset\n",
        "\n",
        "## Dérivée par rapport à $x$\n",
        "\n",
        "Dans un premier temps, on peut définir les gradients exacts de cette fonction à partir d'une formule analytique. \n"
      ],
      "id": "e189f8b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def df_dx(x, a, b):\n",
        "    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)"
      ],
      "id": "beddd4f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Puis on définit les gradients via autograd et on vérifie que les résultats sont identiques.\n"
      ],
      "id": "2ad1d48a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## jax.grad calcule la formule backward par défaut\n",
        "grad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n",
        "## On peut aussi calculer la formule forward via jax.jacfwd\n",
        "fwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) \n",
        "\n",
        "## On vérifie que les gradients retournent des valeurs identiques\n",
        "assert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\n",
        "assert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))\n",
        "\n",
        "## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\n",
        "print('All good, we are ready to go!')"
      ],
      "id": "fe85af47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dérivée par rapport à $a$ et $b$\n",
        "\n",
        "On définit également les gradients exacts par rapport aux paramètres $a$ et $b$ pour vérifier que l'on pourrait les optimiser dans un algorithme d'apprentissage.\n"
      ],
      "id": "d8f95fb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def df_dab(x, a, b):\n",
        "    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2"
      ],
      "id": "8ea12cdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Puis on définit ces gradients via autograd et on vérifie que les résultats sont identiques.\n"
      ],
      "id": "01806f32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## jax.grad calcule la formule backward par défaut\n",
        "grad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n",
        "## On peut aussi calculer la formule forward via jax.jacfwd\n",
        "fwdgrad_df_dab = jax.jacfwd(lambda a_b: f(x, *a_b), argnums=0)\n",
        "\n",
        "## On vérifie que les gradients retournent des valeurs identiques\n",
        "assert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\n",
        "assert all(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) for i in range(2))\n",
        "\n",
        "## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\n",
        "print('All good, we are ready to go!')"
      ],
      "id": "4ece721a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: \n",
        "\n",
        "Nous avons donc bien vérifié que les gradients calculés avec JAX sont identiques aux gradients analytiques.\n",
        "\n",
        "# Comparaison des temps de calcul en JAX entre autograd (backward et forward) et le calcul explicite des gradients\n",
        "\n",
        "Pour mettre en lumière les différences entre l'autograd backward et forward, nous allons définir une nouvelle fonction $g$ dont les sorties sont de plus grandes dimensions que les entrées. \n",
        "\n",
        "::: panel-tabset\n",
        "\n",
        "## $x \\in \\mathbb{R}^d$, f(x) $\\in \\mathbb{R}$\n",
        "\n",
        "Comparaison des temps de calcul :"
      ],
      "id": "283eacc5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## On lance une quantification du temps de calcul pour les trois méthodes (la commande .block_until_ready() est spécifique à JAX pour forcer l'évaluation des opérations asynchrones)\n",
        "%timeit df_dx(x, a, b).block_until_ready()\n",
        "%timeit grad_df_dx(x).block_until_ready()\n",
        "%timeit fwdgrad_df_dx(x).block_until_ready()"
      ],
      "id": "d974d6fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $x \\in \\mathbb{R}$, g(x) $\\in \\mathbb{R}^d$\n",
        "\n",
        "On définit une nouvelle fonction $g$ qui retourne un vecteur de dimension $d$:\n"
      ],
      "id": "a50c69eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## On définit la dimension du vecteur de sortie\n",
        "dim_g = 100\n",
        "## On initialise un point d'évaluation pour la fonction $g$\n",
        "x_g = 0.5\n",
        "\n",
        "## On définit la nouvelle fonction $g$ qui retourne un vecteur de dimension $dim_g$\n",
        "def g(x):\n",
        "    return jnp.array([jnp.tanh(x + i/dim_g) for i in range(dim_g)])\n",
        "\n",
        "## On définit le gradient analytique de la fonction $g$\n",
        "def dg_dx(x):\n",
        "    return jnp.array([1- jnp.tanh(x + i/dim_g)**2 for i in range(dim_g)])\n",
        "    \n",
        "## jax.grad calcule la formule backward par défaut\n",
        "grad_dg_dx = jax.jacrev(lambda x: g(x), argnums=0)\n",
        "## On peut aussi calculer la formule forward via $jax.jacfwd$\n",
        "fwdgrad_dg_dx = jax.jacfwd(lambda x: g(x), argnums=0)    "
      ],
      "id": "e8167346",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparaison des temps de calcul :"
      ],
      "id": "b4fee033"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%timeit dg_dx(x_g).block_until_ready()\n",
        "%timeit grad_dg_dx(x_g).block_until_ready()\n",
        "%timeit fwdgrad_dg_dx(x_g).block_until_ready()"
      ],
      "id": "14a9f6f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "On onbserve que le calcul explicite des gradients est plus rapide que les deux autres méthodes. Il y a donc un prix à payer pour la différentiation automatique, bien qu'elle soit très utile pour des fonctions complexes. Cependant, nous allons voir qu'en combinaison avec une autre stratégie clef en JAX, le jitting, les choses changent.\n",
        "\n",
        "# JIT compilation avec JAX \n",
        "\n",
        "La compilation JIT (Just-In-Time) permet d'optimiser les performances des fonctions en les compilant à la volée. JAX fournit la fonction `jit` pour cela.\n",
        "\n",
        "::: panel-tabset\n",
        "\n",
        "La syntaxe JAX est très confortable, il suffit d'ajouter le décorateur `@jit` avant la définition de la fonction. De manière analogue, on peut utiliser `jax.jit` à la suite de la fonction `jax.grad` ou `jax.jacfwd` pour compiler les gradients.\n",
        "\n",
        "## Gradients jittés pour la fonction $f$"
      ],
      "id": "6de9e334"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## On redéfinit la fonction $f$ et ses gradients avec JIT\n",
        "@jit\n",
        "def f_jit(x, a, b):\n",
        "    return jnp.tanh(jnp.dot(a, x) + b)\n",
        "\n",
        "@jit\n",
        "def df_dx_jit(x, a, b):\n",
        "    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n",
        "\n",
        "## On compile les gradients de la fonction $f$ avec JIT\n",
        "grad_f_jit = jax.jit(jax.grad(lambda x: f_jit(x, a, b), argnums=0))\n",
        "fwdgrad_f_jit = jax.jit(jax.jacfwd(lambda x: f_jit(x, a, b), argnums=0))\n",
        "\n",
        "## On peut maintenant mesurer le temps de calcul des gradients avec JIT\n",
        "%timeit df_dx_jit(x, a, b).block_until_ready()\n",
        "%timeit grad_f_jit(x).block_until_ready()\n",
        "%timeit fwdgrad_f_jit(x).block_until_ready()"
      ],
      "id": "225a52fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients jittés pour la fonction $g$\n"
      ],
      "id": "7e8e29c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## On redéfinit la dimension du vecteur de sortie pour la fonction $g$ (les gradients sont sensibles à la dimension du vecteur de sortie, même avec JIT, notamment lorsque l'on compare l'autograd backward et forward)\n",
        "dim_g = 10\n",
        "\n",
        "## On redéfinit la fonction $f$ et ses gradients avec JIT\n",
        "@jit\n",
        "def g_jit(x):\n",
        "    return jnp.array([jnp.tanh(x + i/dim_g) for i in range(dim_g)])\n",
        "    \n",
        "@jit\n",
        "def dg_dx_jit(x):\n",
        "    return jnp.array([1- jnp.tanh(x + i/dim_g)**2 for i in range(dim_g)])\n",
        "\n",
        "## Pour que le calcul soit rapide aussi, il faut ajouter un 'vmap' avant le 'jit' pour que la fonction soit vectorisée et puisse tirer parti de la compilation JIT.\n",
        "grad_g_jit = jax.jit(jax.vmap(jax.jacrev(lambda x: g_jit(x), argnums=0)))\n",
        "fwdgrad_g_jit = jax.jit(jax.vmap(jax.jacfwd(lambda x: g_jit(x), argnums=0)))\n",
        "\n",
        "## On peut maintenant mesurer le temps de calcul des gradients avec JIT\n",
        "%timeit dg_dx_jit(x).block_until_ready()\n",
        "%timeit grad_g_jit(x).block_until_ready()\n",
        "%timeit fwdgrad_g_jit(x).block_until_ready()"
      ],
      "id": "fe100794",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "On peut constater dans les deux cas que la compilation JIT permet d'accélérer considérablement le calcul des gradients, et rendre l'autograd négligeable d'un point de vue computationel. Dans le cas de la fonction $g$ qui retourne un vecteur, la vectorisation permet aussi d'accélérer le calcul. Ainsi la combinaison de l'autograd, du jitting et de la vectorisation permet d'obtenir des performances optimales et rendre la différentiation automatique aussi efficace que l'analytique, même pour des fonctions complexes.\n",
        "\n",
        "<!--\n",
        "### Dérivée par rapport à $a$ et $b$\n",
        "#\n",
        "#::: \n",
        "#\n",
        "#\n",
        "### JVP et VJP\n",
        "#\n",
        "#Save below\n",
        "#```{python}\n",
        "#import jax\n",
        "#import jax.numpy as jnp\n",
        "#from jax import grad, jit, vmap\n",
        "#from jax import random\n",
        "#\n",
        "#dim = 100\n",
        "#\n",
        "#a = jnp.arange(dim)/ (dim*10)\n",
        "#b = 0.5\n",
        "#\n",
        "#def f(x, a, b):\n",
        "#    return jnp.tanh(jnp.dot(a, x) + b)\n",
        "#\n",
        "#x = (jnp.arange(dim) - 37) / (dim*10)\n",
        "#f(x, a, b) \n",
        "#\n",
        "### jax.grad calcule la formule backward par défaut\n",
        "#grad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n",
        "#grad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n",
        "#\n",
        "#def df_dx(x, a, b):\n",
        "#    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n",
        "#def df_dab(x, a, b):\n",
        "#    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2\n",
        "#\n",
        "#assert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\n",
        "#\n",
        "#\n",
        "## assert jnp.allclose(jnp.array(grad_df_dab((a,b))), jnp.array(df_dab(x, a, b)))\n",
        "#assert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\n",
        "-->\n",
        "::: panel-tabset\n",
        "\n",
        "### JVP \n",
        "\n",
        "Retrouvons les mêmes résultats en redéfinissant les JVP [JAX doc](https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#custom-jvps-with-jax-custom-jvp)\n",
        "Si un custom JVP est défini pour une fonction, il bypass tout les sous JVP qui pourraient y apparaître. Cette définion\n",
        "\n",
        "JVP est capable de dévoiler une colonne de la jacobienne à la fois. Ce n'est pas adapté pour cette fonction. Une passe JVP ne peut dévoiler qu'une seule dérivée. Si l'on veut la dérivée par rapport à tout $x$ il nous faut faire $dim$ fois des JVP.\n"
      ],
      "id": "7a46b0d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@jax.custom_jvp\n",
        "def f(x, a, b):\n",
        "    return jnp.tanh(jnp.dot(a, x) + b)\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "    x, a, b = primals\n",
        "    x_dot, a_dot, b_dot = tangents\n",
        "    primal_out = f(x, a, b)\n",
        "    return primal_out, (jnp.dot(df_dx(x, a, b), x_dot) +  jnp.dot((x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)), a_dot) + jnp.dot((1 - jnp.tanh(jnp.dot(a, x) + b) **2), b_dot))\n",
        "\n",
        "x0_tangents = jnp.zeros(dim)\n",
        "x0_tangents = x0_tangents.at[0].set(1)\n",
        "_, x_dot = jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), 0.))\n",
        "\n",
        "assert jnp.allclose(grad_df_dx(x)[0], x_dot)\n",
        "\n",
        "print(\"All right!\")"
      ],
      "id": "bc77affd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VJP\n",
        "\n",
        "[jax doc](https://docs.jax.dev/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#use-jax-custom-vjp-to-define-custom-reverse-mode-only-rules)\n"
      ],
      "id": "d5ffa3d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@jax.custom_vjp\n",
        "def f(x, a, b):\n",
        "    return jnp.tanh(jnp.dot(a, x) + b)\n",
        "\n",
        "def f_fwd(x, a, b):\n",
        "    return f(x, a, b), (a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n",
        "\n",
        "def f_bwd(res, g):\n",
        "    x_, a_, b_ = res\n",
        "    return (x_ * g,  a_ * g, b_ * g)\n",
        "    #return (jnp.dot(x_, g),  jnp.dot(a_, g), jnp.dot(b_, g))\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)\n",
        "_, f_vjp = jax.vjp(f, x, a, b)\n",
        "\n",
        "assert jnp.allclose(grad_df_dx(x), f_vjp(1.)[0])\n",
        "assert all(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(1.)[i + 1]) for i in range(2))\n",
        "print(\"All right!\")"
      ],
      "id": "208562ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that redefining the VJP redefines the grad function (which passes by f_fwd), and note the equivalency:\n"
      ],
      "id": "81992d50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert jnp.allclose(f_vjp(1.)[0], jax.grad(f)(x, a, b))\n",
        "print(\"All right!\")"
      ],
      "id": "ccc905a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "We conclude that it is much more natural to use the forward mode for this function because..."
      ],
      "id": "c8c11660"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}