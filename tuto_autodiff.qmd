---
title: "Tutoriel de différentiation automatique - STOR 9/01/2026"
from: markdown+emoji
lang: fr
format: html
execute:
    cache: true
toc: true
author:
  - name: Mahendra Mariadassou
    email: mahendra.mariadassou@inrae.fr
    affiliations:
      - name: INRAE - MaIAGE
  - name: Hugo Gangloff
    email: hugo.gangloff@inrae.fr
    affiliations:
      - name: INRAE - MIA Paris Saclay
  - name: Arthur Leroy
    email: arthur.leroy@inrae.fr
    affiliations:
      - name: INRAE - GABI
  - name: Lucia Clarotto
    email: lucia.clarotto@agroparistech.fr
    affiliations:
      - name: AgroParisTech - MIA Paris Saclay
date: "2026-01-16"
date-modified: today
date-format: "DD MMMM YYYY"
---

::: callout-note
L'obectif de ce tutoriel est de montrer comment utiliser JAX pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d'une fonction qui n'est disponible dans les primitives fournies par JAX/Torch. Les deux cas d'usage envisagées sont:

-   l'utilisation d'une fonction non différentiable pour lesquels on veut écrire une dérivée "non-standard" afin de pouvoir l'utiliser dans JAX
-   l'utilisation d'une fonction donc une approximation analytique de la dérivée est disponible mais qui n'est pas implémentée dans JAX
:::

::: callout-note
## Installation de l'environnement \[en amont\]

-   Envoyer mail à STOR et MIA PS
-   Dire que ni la connaissance de JAX ni de l'autodiff n'est nécessaire en amont
-   nouvel environment python
-   pip install jax

## Théorie avec illustration sur un exemple simple \[45 minutes\]

Mahendra

Une couche intermédiaire d'un réseau de neurones de $\mathbb{R}^4$ dans $\mathbb{R}^2$.

$$
f: x = (x_0, x_1, x_2, x_3) \mapsto \tanh(w_0^\top x + b_0, w_1^\top x + b_1) = \tanh(Wx + b)
$$

où $\tanh$ est la fonction tangente hyperbolique appliqué terme à terme.

-   Montrer les étapes d'une dérivation forward et d'une dérivation backward mathématiquement. Le faire en mode TD après avoir annoncé les principes du mode forward et du mode backward et posé les notations (jacobiennes etc). Introduire VJP et JVP.

## Application sur l'exemple simple \[75 minutes\]

Lucia 1. Sans passer par les fonctions forward et backward de JAX, implémenter la dérivée 2. En utilisant `jax.jacfwd` et `jax.jacbwd` (le principe de ces fonctions est vu en Section 1) 3. En utilisant `jax.grad` + `jax.vmap`

#### COFFEE BREAK + SOCIAL TIME \[15 minutes\]

Demander à Julie si on peut acheter une galette des rois :wink:

Arthur

4.  En utilisant `jax.jvp` et `jax.vjp`
5.  En redéfinissant *manuellement* le `jvp` et `vjp` associé à notre exemple (`jax.custom_jvp` et `jax.custom_vjp`)
6.  Vérifier que les différentes approches donnent le même résultat
7.  Comparer les temps de calcul avec `% timeit`

## Application au calcul de la Hessienne \[30 minutes\]

Hugo

Cette 3ème partie a pour but d'introduire et de faire utiliser les fonctions de dérivée forward et de dérivée backward codées avant (composition des deux), le tout après une explication mathématique de ce qu'il se passe.

https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode

## Discussion de l'utilité (exemples complexes et tordus...) \[15 min\]

Tous

1.  Illuster la différence sur des fonctions très *rectangulaires* pour montrer quand utiliser les modes forward et reverse.
2.  Plutôt mode reverse / backward quand $d_\text{output} \ll d_\text{input}$
3.  Plutôt mode forward quand $d_\text{output} \gg d_\text{input}$
4.  Parler de l'application à des fonctions complexes sans dérivée déjà définie (fonction sans dérivée analytique, mais avec bonne approximation analytique, voire Bastien dans ZIPLN ou plus simplement des fonctions non analytiques comme ReLU et associés)
5.  Evoquer l'intérêt du *jitting* (notamment en JAX) qui rend négligeable le surcoût computationnel de l'autodiff par rapport aux gradients analytiques
:::



## Implémentation

### Imports et définition de la fonction

```{python}
import jax
import jax.numpy as jnp
```

```{python}
key = jax.random.PRNGKey(0)

W = jax.random.normal(key, (2, 4))
b = jax.random.normal(key, (2,))

def f(x):
    return jnp.tanh(W @ x + b)
```

On testera les dérivées en un point donné :

```{python}
x = jnp.array([0.2, -0.1, 0.5, 1.0])
```


::: callout-note
Ici on calcule la dérivée par rapport à $\mathbf{x}$ en gardant $W$ et $\mathbf{b}$ comme des paramètres. Dans l'apprentissage d'un réseau de neurones, on calcule plutôt les dérivées par rapport à $W$ et $\mathbf{b}$, mais le principe sera exactement le même!
:::

---

### 1. Dérivée implémentée analytiquement

On note
$$
\mathbf{z} = W\mathbf{x} + \mathbf{b} \in \mathbb{R}^2\\
f_i(\mathbf{x}) = \tanh(z_i)
$$

avec $\mathbf{x} \in \mathbb{R}^4$, $W \in \mathbb{R}^{2\times 4}$,  $\mathbf{b} \in \mathbb{R}^2$.

La dérivée de $\tanh$ est :
$$
\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z).
$$

La jacobienne de $f$ est donc
$$
J_f(\mathbf{x}) = \begin{bmatrix}
1 - \tanh^2(z)) & 0 \\
0 & 1 - \tanh^2(z))
\end{bmatrix} W
$$


```{python}
def jacobian_manual(x):
    z = W @ x + b              # (2,)
    D = 1.0 - jnp.tanh(z)**2   # (2,)
    return jnp.diag(D) @ W     # (2, 4)
```


```{python}
J_manual = jacobian_manual(x)
J_manual
```

### 2. Jacobienne avec Jax autodiff

JAX permet de calculer directement la jacobienne complète.


Modifier avec focntion à plusieurs paramètres avec argnums 


#### Différentiation reverse-mode (`jax.jacrev`)

On peut calculer la formule backward via `jax.jacrev`

```{python}
J_bwd = jax.jacrev(f)(x)

J_bwd
```

#### Différentiation forward-mode (`jax.jacfwd`)

On peut aussi calculer la formule forward via `jax.jacfwd`

```{python}
J_fwd = jax.jacfwd(f)(x)

J_fwd
```


### 3. Jacobienne avec gradient et vmap (`jax.grad` + `jax.vmap`)

`jax.grad` calcule la formule backward par défaut, mais elle ne s'applique qu’aux fonctions scalaire → scalaire. On calcule donc le gradient de chaque composante de `f` séparément.

```{python}
def f_i(i, x):
    return f(x)[i]
```

```{python}
grad_f_i = lambda i: jax.grad(lambda x: f_i(i, x))

J_grad_vmap = jax.vmap(lambda i: grad_f_i(i)(x))(jnp.arange(2))
J_grad_vmap
```

La sortie est bien une matrice $(2 \times 4)$.

---

### 4. Vérification de l’égalité des résultats

```{python}
print(jnp.allclose(J_manual, J_bwd))
print(jnp.allclose(J_manual, J_fwd))
print(jnp.allclose(J_manual, J_grad_vmap))
```

```{python}
print(jnp.max(jnp.abs(J_manual - J_fwd)))
```

---

### 5. Comparaison des temps de calcul

> Les benchmarks doivent être faits après **compilation JIT**.

```{python}
jac_manual_jit = jax.jit(jacobian_manual)
jac_bwd_jit = jax.jit(jax.jacrev(f))
jac_fwd_jit = jax.jit(jax.jacfwd(f))
jac_grad_vmap_jit = jax.jit(
    lambda x: jax.vmap(lambda i: jax.grad(lambda x: f(x)[i])(x))(jnp.arange(2))
)
```

```{python}
# Warm-up
jac_manual_jit(x)
jac_bwd_jit(x)
jac_fwd_jit(x)
jac_grad_vmap_jit(x)
```

```{python}
%timeit jac_manual_jit(x)
%timeit jac_bwd_jit(x)
%timeit jac_fwd_jit(x)
%timeit jac_grad_vmap_jit(x)
```

---

## Conclusion

- La dérivation **manuelle** est la plus rapide et la plus contrôlable, mais peu scalable. Ici elle est aussi longue que `jacrev`.
- `jacrev`  est souvent préférable quand $\dim(f(x))$ est petite (notre cas ici). Ce cadre correspond exactement à ce qui se passe dans une **couche de réseau de neurones**, et se généralise naturellement à des architectures plus profondes.
- `jacfwd` est souvent préférable quand $\dim(x)$ est petite.


