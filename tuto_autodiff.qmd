---
title: "Tutoriel de différentiation automatique - STOR 9/01/2026"
from: markdown+emoji
lang: fr
format: html
execute:
    cache: true
toc: true
author:
  - name: Mahendra Mariadassou
    email: mahendra.mariadassou@inrae.fr
    affiliations:
      - name: INRAE - MaIAGE
  - name: Hugo Gangloff
    email: hugo.gangloff@inrae.fr
    affiliations:
      - name: INRAE - MIA Paris Saclay
  - name: Arthur Leroy
    email: arthur.leroy@inrae.fr
    affiliations:
      - name: INRAE - GABI
  - name: Lucia Clarotto
    email: lucia.clarotto@agroparistech.fr
    affiliations:
      - name: AgroParisTech - MIA Paris Saclay
date: "2026-01-16"
date-modified: today
date-format: "DD MMMM YYYY"
---

::: callout-note
L'obectif de ce tutoriel est de montrer comment utiliser JAX pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d'une fonction qui n'est disponible dans les primitives fournies par JAX/Torch. Les deux cas d'usage envisagées sont:

-   l'utilisation d'une fonction non différentiable pour lesquels on veut écrire une dérivée "non-standard" afin de pouvoir l'utiliser dans JAX
-   l'utilisation d'une fonction donc une approximation analytique de la dérivée est disponible mais qui n'est pas implémentée dans JAX
:::

::: callout-note
## Installation de l'environnement \[en amont\]

-   Envoyer mail à STOR et MIA PS
-   Dire que ni la connaissance de JAX ni de l'autodiff n'est nécessaire en amont
-   nouvel environment python
-   pip install jax
:::

## Théorie avec illustration sur un exemple simple \[45 minutes\]

### Introduction à la Dérivation Automatique avec JAX

Ce tutoriel introduit les concepts de **Jacobian-Vector Product (JVP)** et **Vector-Jacobian Product (VJP)** à travers un exemple concret : une couche intermédiaire d’un réseau de neurones. L’objectif est de poser les notations mathématiques, puis de montrer comment JAX implémente ces opérations.

**Contexte et Notations**

On considère une couche intermédiaire d’un réseau de neurones :

$$
f: \mathbb{R}^4 \to \mathbb{R}^2, \quad \mathbf{x} \mapsto \tanh(W\mathbf{x} + \mathbf{b})
$$

où :

- $\mathbf{x} = (x_0, x_1, x_2, x_3) \in \mathbb{R}^4$ (entrée),
- $W \in \mathbb{R}^{2 \times 4} = \begin{pmatrix} \mathbf{w}_0 \\ \mathbf{w}_1 \end{pmatrix}$ (matrice des poids),
- $\mathbf{b} = (b_0, b_1)\in \mathbb{R}^2$ (biais),
- $\tanh$ est appliquée **terme à terme**.

**Jacobienne de $f$**

La jacobienne de $f$ au point $\mathbf{x}$ notée $J_f(\mathbf{x})$ (ou parfois $\partial_\mathbf{x} f$ est une application de linéaire de $\mathbb{R}^4$ de $\mathbb{R}^2 de matrice: 
$$
J_f(\mathbf{x}) = 
\frac{\partial f}{\partial \mathbf{x}} =
\begin{pmatrix}
\frac{\partial f_i(\mathbf{x})}{\partial x_j}
\end{pmatrix}_{i=1\dots2, j = 1 \dots4} =
\begin{bmatrix}
1 - \tanh^2(\mathbf{w}_0^\top \mathbf{x} + b_0) & . \\
. & 1 - \tanh^2(\mathbf{w}_1^\top \mathbf{x} + b_1) 
\end{bmatrix} \cdot W
$$

### Dérivation Forward (JVP)

**Principe**

Le **JVP** (pour Jacobian Vector Product) calcule la _dérivée directionnelle_ de $f$ dans la direction d’un vecteur $\mathbf{v} \in \mathbb{R}^4$ :
$$
J_f(\mathbf{x}) \cdot \mathbf{v} \in \mathbb{R}^{2}
$$
$J_f(\mathbf{x}) \cdot \mathbf{v}$ est le vecteur tangent de $f$ au point $\mathbf{x}$ dans la direction $\mathbf{v}$. 


Si on ne considère pas un point $\mathbf{x}$ en particulier, on peut voir $J_f$ comme une application $J_f: \mathbb{R}^4 \to (\mathbb{R}^4 \to \mathbb{R}^2)$. En particulier, si on se donne un point d'intérêt $\mathbf{x}$ et une direction d'intérêt $\mathbf{v}$, on peut définir une application de $\mathbb{R}^4 \times \mathbb{R}^4 \to \mathbb{R}^2$ donnée par 

$$
\texttt{jvp}_f(\mathbf{x}, \mathbf{v}) = J_f(\mathbf{x}). \mathbf{v} \in \mathbb{R}^{2}
$$

Dans le langage de l'autodiff:

- $\mathbf{x}$ est le _vecteur primal_ (point d'évaluation)
- $\mathbf{v}$ est le _vecteur tangent_ (direction d'évaluation de la jacobienne)

Cette écriture et ce vocabulaire sont très utiles pour calculer une dérivée directionnelle de façon séquentielle. Reprenons notre example simple et décomposons le en écrivant $\mathbf{z} = (z_0, z_1) = z(\mathbf{x}) = W\mathbf{x} + \mathbf{b} = (\mathbf{w}_0^\top \mathbf{x} + b_0, \mathbf{w}_1^\top \mathbf{x} + b_1)$. On peut calculer:

$$
\begin{align}
f(\mathbf{x}) & = \tanh(\mathbf{z}) = \tanh(z(\mathbf{x})) \\
J_z(\mathbf{x}) & = W \\
J_{\tanh}(\mathbf{z}) & = \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \\
\texttt{jvp}_z(\mathbf{x}, \mathbf{v}) & = W \cdot \mathbf{v} \\
\texttt{jvp}_{\tanh}(\mathbf{z}, \mathbf{u}) & = \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \mathbf{u}  \\
\end{align}
$$
On voit bien que, avec $\mathbf{z}$ précédemment défini, on a:

$$
J_f(\mathbf{x}) \cdot \mathbf{v} = J_{\tanh}(\mathbf{z}) \cdot J_z(\mathbf{x})
$$

Et en particulier: 
$$
\texttt{jvp}_f(\mathbf{x}, \mathbf{v}) = \texttt{jvp}_{\tanh}(z(\mathbf{x}), \texttt{jvp}_{z}(\mathbf{x}))  
$$

::: callout-important

Si $h = g \circ f$, pour calculer $\texttt{jvp}_h(\cdot, \cdot)$, il **suffit** de calculer $(\mathbf{x}, \mathbf{v}) \to (f(\mathbf{x}), \texttt{jvp}_f(\mathbf{x}, \mathbf{v}))$ et $(\mathbf{x}, \mathbf{v}) \to (g(\mathbf{x}), \texttt{jvp}_g(\mathbf{x}, \mathbf{v}))$ en des points arbitraires. En d'autres termes, il suffit de propager les vecteurs primaux et tangents le long de la chaîne (approche forward). 

En pratique, la fonction `jax.jvp` est définie par $\texttt{jax.jvp}: (f, \mathbf{x}, \mathbf{v}) \to (f(\mathbf{x}), J_f(\mathbf{x})\cdot \mathbf{v})$. 

:::

::: callout-note

L'approche JVP est très frugale en mémoire et conceptuellement simple, les vecteurs primaux et tangent sont calculés et propagés à la volée (approche forward) et il n'est pas nécessaire de stocker quoi que ce soit. Le coût d'une évaluation de $(f(x), \texttt{vjp}_f(x, v))$ est à peu près 3 fois celui d'une évaluation de $f(x)$. 

L'approche JVP est utile pour évaluer les **colonnes** de $J_f(\mathbf{x})$: il suffit de prendre $\mathbf{v}$ de la forme $\mathbf{v} = (0, \dots, 0, 1, 0, \dots, 0)$. Elle fonctionne donc très bien quand $J_f(\mathbf{x})$ est long ($f: \mathbb{R}^n \to \mathbb{R}^m$ avec $n \leq m$). 

:::

### Dérivation Backward (VJP)

**Principe**

Le **VJP** (pour Vector Jacobian Product) calcule le produit d’un vecteur $\mathbf{u} \in \mathbb{R}^2$ avec la transposée de la jacobienne :
$$
\mathbf{u}^\top \cdot J_f(\mathbf{x})
$$

Si on ne considère pas un point $\mathbf{x}$ en particulier, on peut voir $J_f$ comme une application $J_f: \mathbb{R}^4 \to (\mathbb{R}^4 \to \mathbb{R}^2)$. En particulier, si on se donne un point d'intérêt $\mathbf{x}$ et une co-direction d'intérêt $\mathbf{u}$, on peut définir une application de $\mathbb{R}^2 \times \mathbb{R}^4 \to \mathbb{R}^4$ donnée par 

$$
\texttt{vjp}_f(\mathbf{x}, \mathbf{u}) = \mathbf{u}^\top J_f(\mathbf{x}) \in \mathbb{R}^{4}
$$

Son intérêt est de calculer facilement les lignes de $J_f(\mathbf{x})$ (en prenant $\mathbf{u}$ de la forme $\mathbf{u} = (0, \dots, 0, 1, 0, \dots, 0)$), ce qui est plus efficace pour les matrices larges. 

::: {.callout-note collapse = "true"}
## Détails de l'utilisation de VJP pour les fonctions composées

Soit une fonction composée $f \circ g$, où :
- $g: \mathbb{R}^n \to \mathbb{R}^m$ (fonction interne),
- $f: \mathbb{R}^m \to \mathbb{R}^p$ (fonction externe).

On cherche à calculer la dérivée de $f \circ g$ par rapport à $\mathbf{x} \in \mathbb{R}^n$, c'est-à-dire la **jacobienne** de la composition :
$$
J_{f \circ g}(\mathbf{x}) = J_f(g(\mathbf{x})) \cdot J_g(\mathbf{x}).
$$

La règle de la chaîne pour les fonctions vectorielles s'écrit (en terme de jacobiennes) :
$$
J_{f \circ g}(\mathbf{x}) = J_f(\mathbf{y}) \cdot J_g(\mathbf{x}), \quad \text{où } \mathbf{y} = g(\mathbf{x}).
$$

Le **VJP** (Vector-Jacobian Product) permet de calculer efficacement le produit d'un vecteur cotangent $\mathbf{u} \in \mathbb{R}^p$ avec la jacobienne de la composition :
$$
\mathbf{u}^\top \cdot J_{f \circ g}(\mathbf{x}) = \mathbf{u}^\top \cdot \left( J_f(\mathbf{y}) \cdot J_g(\mathbf{x}) \right).
$$

1. **Évaluer la fonction interne** :
   Calculer $\mathbf{y} = g(\mathbf{x})$.

2. **Calculer le VJP de $f$** :
   Calculer $\mathbf{v}^\top = \mathbf{u}^\top \cdot J_f(\mathbf{y})$.
   Ce vecteur $\mathbf{v} \in \mathbb{R}^m$ est le **gradient adjoint** de $f$ pondéré par $\mathbf{u}$.

3. **Calculer le VJP de $g$** :
   Calculer $\mathbf{v}^\top \cdot J_g(\mathbf{x})$.
   Ce produit donne le gradient final $\mathbf{u}^\top \cdot J_{f \circ g}(\mathbf{x})$.
::: 

#### Application sur notre exemple. 

On rappelle les notations 
- $z(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$ (couche linéaire),
- $f(\mathbf{x}) = \tanh(\mathbf{z})$ (activation).

Pour un vecteur cotangent $\mathbf{u} \in \mathbb{R}^2$:

- Calcul de la couche linéaire $z(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$
- Première étape de la rétropropagation: 
$$
\mathbf{v}^\top = \texttt{vjp}_{\tanh}(\mathbf{z}, \mathbf{u}) = \mathbf{u}^\top J_{\tanh}(\mathbf{z}) = \mathbf{u}^\top \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \in \mathbb{R}^2
$$
- Deuxième étape de la rétropropagation: 
$$
\mathbf{u}^T J_f(\mathbf{x}) = \texttt{vjp}_{z}(\mathbf{x}, \mathbf{v}) = \mathbf{v}^T J_z(\mathbf{x}) = \mathbf{v}^\top W \in \mathbb{R}^4
$$

::: callout-important
## Différence avec le mode forward

- Le VJP permet de **propager les gradients** depuis la sortie de $f$ jusqu'à son entrée.
- C'est la base du **mode reverse** de la différentiation automatique où l'on calcule les gradients depuis la sortie vers l'entrée.
- Contrairement au mode forward, il nécessite de stocker les valeurs des vecteurs primaux lors d'une première passe forward avant de retropropager le gradient lors d'une passe reverse. 

En pratique, la fonction `jax.vjp` est définie par $\texttt{jax.jvp}: (f, \mathbf{x}, \mathbf{u}) \to (f(\mathbf{x}), \mathbf{u}^\top J_f(\mathbf{x}))$. 

:::

## Application sur l'exemple simple \[75 minutes\]

Lucia 1. Sans passer par les fonctions forward et backward de JAX, implémenter la dérivée 2. En utilisant `jax.jacfwd` et `jax.jacbwd` (le principe de ces fonctions est vu en Section 1) 3. En utilisant `jax.grad` + `jax.vmap`

#### COFFEE BREAK + SOCIAL TIME \[15 minutes\]

Demander à Julie si on peut acheter une galette des rois :wink:

Arthur

4.  En utilisant `jax.jvp` et `jax.vjp`
5.  En redéfinissant *manuellement* le `jvp` et `vjp` associé à notre exemple (`jax.custom_jvp` et `jax.custom_vjp`)
6.  Vérifier que les différentes approches donnent le même résultat
7.  Comparer les temps de calcul avec `% timeit`

## Application au calcul de la Hessienne \[30 minutes\]

Hugo

Cette 3ème partie a pour but d'introduire et de faire utiliser les fonctions de dérivée forward et de dérivée backward codées avant (composition des deux), le tout après une explication mathématique de ce qu'il se passe.

https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode

## Discussion de l'utilité (exemples complexes et tordus...) \[15 min\]

Tous

1.  Illuster la différence sur des fonctions très *rectangulaires* pour montrer quand utiliser les modes forward et reverse.
2.  Plutôt mode reverse / backward quand $d_\text{output} \ll d_\text{input}$
3.  Plutôt mode forward quand $d_\text{output} \gg d_\text{input}$
4.  Parler de l'application à des fonctions complexes sans dérivée déjà définie (fonction sans dérivée analytique, mais avec bonne approximation analytique, voire Bastien dans ZIPLN ou plus simplement des fonctions non analytiques comme ReLU et associés)
5.  Evoquer l'intérêt du *jitting* (notamment en JAX) qui rend négligeable le surcoût computationnel de l'autodiff par rapport aux gradients analytiques




## Implémentation

### Imports et définition de la fonction

```{python}
import jax
import jax.numpy as jnp
```

```{python}
key = jax.random.PRNGKey(0)

W = jax.random.normal(key, (2, 4))
b = jax.random.normal(key, (2,))

def f(x):
    return jnp.tanh(W @ x + b)
```

On testera les dérivées en un point donné :

```{python}
x = jnp.array([0.2, -0.1, 0.5, 1.0])
```


::: callout-note
Ici on calcule la dérivée par rapport à $\mathbf{x}$ en gardant $W$ et $\mathbf{b}$ comme des paramètres. Dans l'apprentissage d'un réseau de neurones, on calcule plutôt les dérivées par rapport à $W$ et $\mathbf{b}$, mais le principe sera exactement le même!
:::

---

### 1. Dérivée implémentée analytiquement

On note
$$
\mathbf{z} = W\mathbf{x} + \mathbf{b} \in \mathbb{R}^2\\
f_i(\mathbf{x}) = \tanh(z_i)
$$

avec $\mathbf{x} \in \mathbb{R}^4$, $W \in \mathbb{R}^{2\times 4}$,  $\mathbf{b} \in \mathbb{R}^2$.

La dérivée de $\tanh$ est :
$$
\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z).
$$

La jacobienne de $f$ est donc
$$
J_f(\mathbf{x}) = \begin{bmatrix}
1 - \tanh^2(z)) & 0 \\
0 & 1 - \tanh^2(z))
\end{bmatrix} W
$$


```{python}
def jacobian_manual(x):
    z = W @ x + b              # (2,)
    D = 1.0 - jnp.tanh(z)**2   # (2,)
    return jnp.diag(D) @ W     # (2, 4)
```


```{python}
J_manual = jacobian_manual(x)
J_manual
```

### 2. Jacobienne avec Jax autodiff

JAX permet de calculer directement la jacobienne complète.


Modifier avec focntion à plusieurs paramètres avec argnums 


#### Différentiation reverse-mode (`jax.jacrev`)

On peut calculer la formule backward via `jax.jacrev`

```{python}
J_bwd = jax.jacrev(f)(x)

J_bwd
```

#### Différentiation forward-mode (`jax.jacfwd`)

On peut aussi calculer la formule forward via `jax.jacfwd`

```{python}
J_fwd = jax.jacfwd(f)(x)

J_fwd
```


### 3. Jacobienne avec gradient et vmap (`jax.grad` + `jax.vmap`)

`jax.grad` calcule la formule backward par défaut, mais elle ne s'applique qu’aux fonctions scalaire → scalaire. On calcule donc le gradient de chaque composante de `f` séparément.

```{python}
def f_i(i, x):
    return f(x)[i]
```

```{python}
grad_f_i = lambda i: jax.grad(lambda x: f_i(i, x))

J_grad_vmap = jax.vmap(lambda i: grad_f_i(i)(x))(jnp.arange(2))
J_grad_vmap
```

La sortie est bien une matrice $(2 \times 4)$.

---

### 4. Vérification de l’égalité des résultats

```{python}
print(jnp.allclose(J_manual, J_bwd))
print(jnp.allclose(J_manual, J_fwd))
print(jnp.allclose(J_manual, J_grad_vmap))
```

```{python}
print(jnp.max(jnp.abs(J_manual - J_fwd)))
```

---

### 5. Comparaison des temps de calcul

> Les benchmarks doivent être faits après **compilation JIT**.

```{python}
jac_manual_jit = jax.jit(jacobian_manual)
jac_bwd_jit = jax.jit(jax.jacrev(f))
jac_fwd_jit = jax.jit(jax.jacfwd(f))
jac_grad_vmap_jit = jax.jit(
    lambda x: jax.vmap(lambda i: jax.grad(lambda x: f(x)[i])(x))(jnp.arange(2))
)
```

```{python}
# Warm-up
jac_manual_jit(x)
jac_bwd_jit(x)
jac_fwd_jit(x)
jac_grad_vmap_jit(x)
```

```{python}
%timeit jac_manual_jit(x)
%timeit jac_bwd_jit(x)
%timeit jac_fwd_jit(x)
%timeit jac_grad_vmap_jit(x)
```

---

## Conclusion

- La dérivation **manuelle** est la plus rapide et la plus contrôlable, mais peu scalable. Ici elle est aussi longue que `jacrev`.
- `jacrev`  est souvent préférable quand $\dim(f(x))$ est petite (notre cas ici). Ce cadre correspond exactement à ce qui se passe dans une **couche de réseau de neurones**, et se généralise naturellement à des architectures plus profondes.
- `jacfwd` est souvent préférable quand $\dim(x)$ est petite.


