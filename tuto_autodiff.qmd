---
title: "Tutoriel de différentiation automatique - STOR 9/01/2026"
from: markdown+emoji
lang: fr
format: html
execute:
    cache: true
toc: true
author:
  - name: Mahendra Mariadassou
    email: mahendra.mariadassou@inrae.fr
    affiliations:
      - name: INRAE - MaIAGE
  - name: Hugo Gangloff
    email: hugo.gangloff@inrae.fr
    affiliations:
      - name: INRAE - MIA Paris Saclay
  - name: Arthur Leroy
    email: arthur.leroy@inrae.fr
    affiliations:
      - name: INRAE - GABI
  - name: Lucia Clarotto
    email: lucia.clarotto@agroparistech.fr
    affiliations:
      - name: AgroParisTech - MIA Paris Saclay
date: "2026-01-16"
date-modified: today
date-format: "DD MMMM YYYY"
---

::: callout-note
L'obectif de ce tutoriel est de montrer comment utiliser JAX pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d'une fonction qui n'est disponible dans les primitives fournies par JAX/Torch. Les deux cas d'usage envisagées sont:

-   l'utilisation d'une fonction non différentiable pour lesquels on veut écrire une dérivée "non-standard" afin de pouvoir l'utiliser dans JAX
-   l'utilisation d'une fonction donc une approximation analytique de la dérivée est disponible mais qui n'est pas implémentée dans JAX
:::

::: callout-note
## Installation de l'environnement \[en amont\]

-   Envoyer mail à STOR et MIA PS
-   Dire que ni la connaissance de JAX ni de l'autodiff n'est nécessaire en amont
-   nouvel environment python
-   pip install jax
:::

## Théorie avec illustration sur un exemple simple \[45 minutes\]

### Introduction à la Dérivation Automatique avec JAX

Ce tutoriel introduit les concepts de **Jacobian-Vector Product (JVP)** et **Vector-Jacobian Product (VJP)** à travers un exemple concret : une couche intermédiaire d’un réseau de neurones. L’objectif est de poser les notations mathématiques, puis de montrer comment JAX implémente ces opérations.

**Contexte et Notations**

On considère une couche intermédiaire d’un réseau de neurones :

$$
f: \mathbb{R}^4 \to \mathbb{R}^2, \quad \mathbf{x} \mapsto \tanh(W\mathbf{x} + \mathbf{b})
$$

où :

- $\mathbf{x} = (x_0, x_1, x_2, x_3) \in \mathbb{R}^4$ (entrée),
- $W \in \mathbb{R}^{2 \times 4} = \begin{pmatrix} \mathbf{w}_0 \\ \mathbf{w}_1 \end{pmatrix}$ (matrice des poids),
- $\mathbf{b} = (b_0, b_1)\in \mathbb{R}^2$ (biais),
- $\tanh$ est appliquée **terme à terme**.

**Jacobienne de $f$**

La jacobienne de $f$ au point $\mathbf{x}$ notée $J_f(\mathbf{x})$ (ou parfois $\partial_\mathbf{x} f$ est une application de linéaire de $\mathbb{R}^4$ de $\mathbb{R}^2$ de matrice: 
$$
J_f(\mathbf{x}) = 
\frac{\partial f}{\partial \mathbf{x}} =
\begin{pmatrix}
\frac{\partial f_i(\mathbf{x})}{\partial x_j}
\end{pmatrix}_{i=1\dots2, j = 1 \dots4} =
\begin{bmatrix}
1 - \tanh^2(\mathbf{w}_0^\top \mathbf{x} + b_0) & . \\
. & 1 - \tanh^2(\mathbf{w}_1^\top \mathbf{x} + b_1) 
\end{bmatrix} \cdot W
$$

### Dérivation Forward (JVP)

**Principe**

Le **JVP** (pour Jacobian Vector Product) calcule la _dérivée directionnelle_ de $f$ dans la direction d’un vecteur $\mathbf{v} \in \mathbb{R}^4$ :
$$
J_f(\mathbf{x}) \cdot \mathbf{v} \in \mathbb{R}^{2}
$$
$J_f(\mathbf{x}) \cdot \mathbf{v}$ est le vecteur tangent de $f$ au point $\mathbf{x}$ dans la direction $\mathbf{v}$. 


Si on ne considère pas un point $\mathbf{x}$ en particulier, on peut voir $J_f$ comme une application $J_f: \mathbb{R}^4 \to (\mathbb{R}^4 \to \mathbb{R}^2)$. En particulier, si on se donne un point d'intérêt $\mathbf{x}$ et une direction d'intérêt $\mathbf{v}$, on peut définir une application de $\mathbb{R}^4 \times \mathbb{R}^4 \to \mathbb{R}^2$ donnée par 

$$
\texttt{jvp}_f(\mathbf{x}, \mathbf{v}) = J_f(\mathbf{x}). \mathbf{v} \in \mathbb{R}^{2}
$$

Dans le langage de l'autodiff:

- $\mathbf{x}$ est le _vecteur primal_ (point d'évaluation)
- $\mathbf{v}$ est le _vecteur tangent_ (direction d'évaluation de la jacobienne)

Cette écriture et ce vocabulaire sont très utiles pour calculer une dérivée directionnelle de façon séquentielle. 

::: callout-important

Si $h = f \circ g$, pour calculer $\texttt{jvp}_h(\cdot, \cdot)$, il **suffit** de calculer $(\mathbf{x}, \mathbf{v}) \to (f(\mathbf{x}), \texttt{jvp}_f(\mathbf{x}, \mathbf{v}))$ et $(\mathbf{x}, \mathbf{v}) \to (g(\mathbf{x}), \texttt{jvp}_g(\mathbf{x}, \mathbf{v}))$ en des points arbitraires. En d'autres termes, il suffit de propager les vecteurs primaux et tangents le long de la chaîne (approche forward). On a en effet: 

$$
\texttt{jvp}_h(\mathbf{x}, \mathbf{v}) = \texttt{jvp}_{f}(g(\mathbf{x}), \texttt{jvp}_{g}(\mathbf{x}))
$$
:::

#### Application sur notre exemple. 

Reprenons notre example simple et décomposons le en écrivant $\mathbf{z} = (z_0, z_1) = z(\mathbf{x}) = W\mathbf{x} + \mathbf{b} = (\mathbf{w}_0^\top \mathbf{x} + b_0, \mathbf{w}_1^\top \mathbf{x} + b_1)$. On peut calculer:

$$
\begin{align}
f(\mathbf{x}) & = \tanh(\mathbf{z}) = \tanh(z(\mathbf{x})) \\
J_z(\mathbf{x}) & = W \\
J_{\tanh}(\mathbf{z}) & = \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \\
\texttt{jvp}_z(\mathbf{x}, \mathbf{v}) & = W \cdot \mathbf{v} \\
\texttt{jvp}_{\tanh}(\mathbf{z}, \mathbf{u}) & = \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \mathbf{u}  \\
\end{align}
$$
On voit bien que, avec $\mathbf{z}$ précédemment défini, on a:

$$
J_f(\mathbf{x}) \cdot \mathbf{v} = J_{\tanh}(\mathbf{z}) \cdot J_z(\mathbf{x})\cdot \mathbf{v}
$$

qu'on peut réécrire en terme de `jvp` comme suit
$$
\texttt{jvp}_f(\mathbf{x}, \mathbf{v}) = \texttt{jvp}_{\tanh}(z(\mathbf{x}), \texttt{jvp}_{z}(\mathbf{x}))  
$$

::: callout-note

L'approche JVP est très frugale en mémoire et conceptuellement simple, les vecteurs primaux et tangent sont calculés et propagés à la volée (approche forward) et il n'est pas nécessaire de stocker quoi que ce soit. Le coût d'une évaluation de $(f(x), \texttt{vjp}_f(x, v))$ est à peu près 3 fois celui d'une évaluation de $f(x)$. 

L'approche JVP est utile pour évaluer les **colonnes** de $J_f(\mathbf{x})$: il suffit de prendre $\mathbf{v}$ de la forme $\mathbf{v} = (0, \dots, 0, 1, 0, \dots, 0)$. Elle fonctionne donc très bien quand $J_f(\mathbf{x})$ est long ($f: \mathbb{R}^n \to \mathbb{R}^m$ avec $n \leq m$). 

En pratique, la fonction `jax.jvp` est définie par $\texttt{jax.jvp}: (f, \mathbf{x}, \mathbf{v}) \to (f(\mathbf{x}), J_f(\mathbf{x})\cdot \mathbf{v})$ (elle renvoie à la fois le nouveau vecteur primal et le nouveau vecteur tangent). 

:::

### Dérivation Backward (VJP)

**Principe**

Le **VJP** (pour Vector Jacobian Product) calcule le produit d’un vecteur $\mathbf{u} \in \mathbb{R}^2$ avec la transposée de la jacobienne :
$$
\mathbf{u}^\top \cdot J_f(\mathbf{x})
$$

Si on ne considère pas un point $\mathbf{x}$ en particulier, on peut voir $J_f$ comme une application $J_f: \mathbb{R}^4 \to (\mathbb{R}^4 \to \mathbb{R}^2)$. En particulier, si on se donne un point d'intérêt $\mathbf{x}$ et une co-direction d'intérêt $\mathbf{u}$, on peut définir une application de $\mathbb{R}^2 \times \mathbb{R}^4 \to \mathbb{R}^4$ donnée par 

$$
\texttt{vjp}_f(\mathbf{x}, \mathbf{u}) = \mathbf{u}^\top J_f(\mathbf{x}) \in \mathbb{R}^{4}
$$

Son intérêt est de calculer facilement les lignes de $J_f(\mathbf{x})$, en prenant $\mathbf{u}$ de la forme $\mathbf{u} = (0, \dots, 0, 1, 0, \dots, 0)$, ce qui est plus efficace pour les matrices larges. 

::: {.callout-note collapse = "true"}
## Détails de l'utilisation de VJP pour les fonctions composées

Soit une fonction composée $f \circ g$, où :

- $g: \mathbb{R}^n \to \mathbb{R}^m$ (fonction interne),
- $f: \mathbb{R}^m \to \mathbb{R}^p$ (fonction externe).

On cherche à calculer la dérivée de $f \circ g$ par rapport à $\mathbf{x} \in \mathbb{R}^n$, c'est-à-dire la **jacobienne** de la composition :
$$
J_{f \circ g}(\mathbf{x}) = J_f(g(\mathbf{x})) \cdot J_g(\mathbf{x}) = J_f(\mathbf{y}) \cdot J_g(\mathbf{x}), \quad \text{où } \mathbf{y} = g(\mathbf{x}).
$$

Le **VJP** (Vector-Jacobian Product) permet de calculer le produit d'un vecteur cotangent $\mathbf{u} \in \mathbb{R}^p$ avec $J_{f \circ g}(\mathbf{x})$ de façon séquentielle :
$$
\mathbf{u}^\top \cdot J_{f \circ g}(\mathbf{x}) = \mathbf{u}^\top \cdot \left( J_f(\mathbf{y}) \cdot J_g(\mathbf{x}) \right).
$$

1. **Évaluer la fonction interne** :
   Calculer $\mathbf{y} = g(\mathbf{x})$.
2. **Calculer le VJP de $f$** :
   Calculer $\mathbf{v}^\top = \mathbf{u}^\top \cdot J_f(\mathbf{y})$.
   Ce vecteur $\mathbf{v} \in \mathbb{R}^m$ est le **gradient adjoint** de $f$ au point $\mathbf{y}$ pondéré par $\mathbf{u}$.
3. **Calculer le VJP de $g$** :
   Calculer $\mathbf{v}^\top \cdot J_g(\mathbf{x})$. 
   Ce produit donne le le **gradient adjoint** de $g$ au point $\mathbf{x}$ pondéré par $\mathbf{u}$ qui correspond au gradient final $\mathbf{u}^\top \cdot J_{f \circ g}(\mathbf{x})$.
::: 

#### Application sur notre exemple. 

On rappelle que 

- $z(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$
- $f(\mathbf{x}) = \tanh(\mathbf{z})$

Pour un vecteur cotangent $\mathbf{u} \in \mathbb{R}^2$:

- Calcul de la couche linéaire $z(\mathbf{x}) = W\mathbf{x} + \mathbf{b}$
- Première étape de la rétropropagation: 
$$
\mathbf{v}^\top = \texttt{vjp}_{\tanh}(\mathbf{z}, \mathbf{u}) = \mathbf{u}^\top J_{\tanh}(\mathbf{z}) = \mathbf{u}^\top \begin{bmatrix} 1 - \tanh^2(z_0) & . \\ . & 1 - \tanh^2(z_1) \end{bmatrix} \in \mathbb{R}^2
$$
- Deuxième (et dernière) étape de la rétropropagation: 
$$
\mathbf{u}^T J_f(\mathbf{x}) = \texttt{vjp}_{z}(\mathbf{x}, \mathbf{v}) = \mathbf{v}^T J_z(\mathbf{x}) = \mathbf{v}^\top W \in \mathbb{R}^4
$$

::: callout-note
## Différence avec le mode forward

- Le VJP permet de **propager les gradients** depuis la sortie de $f$ jusqu'à son entrée.
- C'est la base du **mode reverse** de la différentiation automatique où l'on calcule les gradients depuis la sortie vers l'entrée.
- Contrairement au mode forward, il nécessite de **stocker les valeurs** des vecteurs primaux lors d'une première passe forward avant de rétropropager le gradient lors d'une passe reverse. 

En pratique, la fonction `jax.vjp` est définie par $\texttt{jax.jvp}: (f, \mathbf{x}, \mathbf{u}) \to (f(\mathbf{x}), \mathbf{u}^\top J_f(\mathbf{x}))$. 

:::

## Application sur l'exemple simple \[75 minutes\]

Lucia 1. Sans passer par les fonctions forward et backward de JAX, implémenter la dérivée 2. En utilisant `jax.jacfwd` et `jax.jacbwd` (le principe de ces fonctions est vu en Section 1) 3. En utilisant `jax.grad` + `jax.vmap`

#### COFFEE BREAK + SOCIAL TIME \[15 minutes\]

Demander à Julie si on peut acheter une galette des rois :wink:

Arthur

4.  En utilisant `jax.jvp` et `jax.vjp`
5.  En redéfinissant *manuellement* le `jvp` et `vjp` associé à notre exemple (`jax.custom_jvp` et `jax.custom_vjp`)
6.  Vérifier que les différentes approches donnent le même résultat
7.  Comparer les temps de calcul avec `% timeit`

## Application au calcul de la Hessienne \[30 minutes\]

Hugo

Cette 3ème partie a pour but d'introduire et de faire utiliser les fonctions de dérivée forward et de dérivée backward codées avant (composition des deux), le tout après une explication mathématique de ce qu'il se passe.

https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode

## Discussion de l'utilité (exemples complexes et tordus...) \[15 min\]

Tous

1.  Illuster la différence sur des fonctions très *rectangulaires* pour montrer quand utiliser les modes forward et reverse.
2.  Plutôt mode reverse / backward quand $d_\text{output} \ll d_\text{input}$
3.  Plutôt mode forward quand $d_\text{output} \gg d_\text{input}$
4.  Parler de l'application à des fonctions complexes sans dérivée déjà définie (fonction sans dérivée analytique, mais avec bonne approximation analytique, voire Bastien dans ZIPLN ou plus simplement des fonctions non analytiques comme ReLU et associés)
5.  Evoquer l'intérêt du *jitting* (notamment en JAX) qui rend négligeable le surcoût computationnel de l'autodiff par rapport aux gradients analytiques




## Implémentation

### Imports et définition de la fonction

```{python}
import jax
import jax.numpy as jnp
```

```{python}
key = jax.random.PRNGKey(0)

dim_entree = 4
dim_sortie = 2
W = jax.random.normal(key, (dim_sortie, dim_entree))
b = jax.random.normal(key, (dim_sortie,))

def f(x, W, b):
  return jnp.tanh(W @ x + b)
```

On testera les dérivées en un point donné :

```{python}
x = jnp.array([0.2, -0.1, 0.5, 1.0])
```


::: callout-note
Ici on calcule la dérivée par rapport à $\mathbf{x}$ en gardant $W$ et $\mathbf{b}$ comme des paramètres. Dans l'apprentissage d'un réseau de neurones, on calcule plutôt les dérivées par rapport à $W$ et $\mathbf{b}$, mais le principe sera exactement le même!
:::

---

### 1. Dérivée implémentée analytiquement

On note
$$
\mathbf{z} = W\mathbf{x} + \mathbf{b} \in \mathbb{R}^2\\
f_i(\mathbf{x}) = \tanh(z_i)
$$

avec $\mathbf{x} \in \mathbb{R}^4$, $W \in \mathbb{R}^{2\times 4}$,  $\mathbf{b} \in \mathbb{R}^2$.

La dérivée de $\tanh$ est :
$$
\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z).
$$

La jacobienne de $f$ est donc
$$
J_f(\mathbf{x}) = \begin{bmatrix}
1 - \tanh^2(z)) & 0 \\
0 & 1 - \tanh^2(z))
\end{bmatrix} W
$$


```{python}
def jacobian_manual(x, W, b):
    z = W @ x + b              # (2,)
    D = 1.0 - jnp.tanh(z)**2   # (2,)
    return jnp.diag(D) @ W     # (2, 4)
```


```{python}
J_manual = jacobian_manual(x, W, b)
J_manual
```

### 2. Jacobienne avec Jax autodiff

JAX permet de calculer directement la jacobienne complète.

#### Différentiation reverse-mode (`jax.jacrev`)

On peut calculer la formule backward via `jax.jacrev` (ou son alias `jax.jacobian`). 

Dès le départ on va fixer l'index de l'argument de la fonction `f` par rapport auquel on veut dériver. Dans notre example, on dérive par rapport à `x` qui correspond à l'index 0 des arguments. Par la suite, on utilisera cet index pour pouvoir dire par rapport à quelle variable on veut dériver, avec l'argument `argnum`.

```{python}
idx_jac = 0
```

```{python}
J_bwd = jax.jacrev(lambda x: f(x, W, b), argnums=idx_jac) 


J_bwd(x)
```

- La fonction `jax.jacrev` prend en entrée au moins une fonction et retourne un *callable*, donc une autre fonction.

- `lambda x: f(x, W, b)` est une fonction anonyme qui :
  - prend un seul argument x
  - appelle `f` en gardant `W` et `b` constants (fermés dans la closure)

- l'argument `argnums` dit par rapport à quelle variable on veut dériver, ici `x`

L'appel du *callable* `J_bwd` sur `x` nous donne la Jacobienne calculée par différentiation automatique.


#### Différentiation forward-mode (`jax.jacfwd`)

On peut aussi calculer la formule forward via `jax.jacfwd` avec le même principe que la précédente.

```{python}
J_fwd = jax.jacfwd(lambda x: f(x, W, b), argnums=idx_jac)

J_fwd(x)
```


### 3. Jacobienne avec gradient et vmap (`jax.grad` + `jax.vmap`)

`jax.grad` est une autre fonction qui calcule la dérivée d'une fonction par formule backward, mais elle ne s'applique qu’aux fonctions scalaire → scalaire. Pour pouvoir l'utiliser, on doit donc calculer le gradient de chaque composante de `f` séparément et ensuite utiliser `vmap` pour appliquer la même fonction à toutes les valeurs du vecteur de sortie.

L’idée de `vmap` est la suivante : *vectoriser une fonction qui agit sur un seul élément pour qu’elle agisse sur tout un lot d’éléments*.

Si on a une fonction $g$ telle que 
$$g(i) = \nabla_x f_i(x,W,b),$$

la fonction `jax.vmap(g)([0, 1])` construit automatiquement le vecteur

$$\begin{pmatrix} 
\nabla_x f_0(x,W,b)\\ 
\nabla_x f_1(x,W,b)
\end{pmatrix},$$

qui est la Jacobienne de $g$.

```{python}
grad_f_i = jax.grad(lambda i, x, W, b: f(x, W, b)[i], argnums=1+idx_jac)

J_grad_vmap = jax.vmap(grad_f_i, in_axes=(0,None,None,None))(jnp.arange(dim_sortie),x,W,b)
J_grad_vmap
```

- `jax.grad` calcule une ligne de la Jacobienne (gradient d’une sortie scalaire), c'est un *callable*
- `jax.vmap` empile ces lignes en parallèle, sans boucle, de façon efficace

Dans la fonction `jax.grad`, l'argnum devient `1 + idx_jac` car il faut considérer le nouveau premier argument `i`.

Dans la fonction `vmap`, on observe un argument en plus de la fonction à mapper :

- `in_axes=(0, None, None, None)` est le *mapping pattern* de `vmap` :
  - `0` : on vectorise sur l’argument `i`
  - `None` : `x, W, b` sont partagés (pas vectorisés)

`jnp.arange(dim_sortie)` fournit les indices $i = 0, 1, ..., dim_sortie−1$ pour lesquels on veut calculer le gradient.

Au final, la sortie est bien la matrice Jacobienne $(2 \times 4)$.

---

### 4. Vérification de l’égalité des résultats

```{python}
print(jnp.allclose(J_manual, J_bwd(x)))
print(jnp.allclose(J_manual, J_fwd(x)))
print(jnp.allclose(J_manual, J_grad_vmap))
```

```{python}
print(jnp.max(jnp.abs(J_manual - J_fwd(x))))
```

---

### 5. Comparaison des temps de calcul

> Les benchmarks doivent être faits après **compilation JIT**.

```{python}

# ## On redéfinit la fonction $f$ et ses gradients avec JIT

# f_jit = jax.jit(lambda x: f(x, W, b))
# jac_manual_jit = jax.jit(lambda x: jacobian_manual(x, W, b))
# jac_fwd_jit = jax.jit(jax.jacfwd(lambda x: f_jit(x, W, b), argnums=0))
# jac_bwd_jit = jax.jit(jax.jacrev(lambda x: f_jit(x, W, b), argnums=0))

# jac_grad_vmap_jit = jax.jit(
# lambda x: jax.vmap(lambda i: grad_f_i(i)(x))(jnp.arange(2)))
```

```{python}
# ## Warm-up
# jac_manual_jit(x)
# jac_bwd_jit(x)
# jac_fwd_jit(x)
# jac_grad_vmap_jit(x)
```

```{python}
# %timeit jac_manual_jit(x)
# %timeit jac_bwd_jit(x)
# %timeit jac_fwd_jit(x)
# %timeit jac_grad_vmap_jit(x)
```

---

## Conclusion

- La dérivation **manuelle** est la plus rapide et la plus contrôlable, mais peu scalable. Ici elle est aussi longue que `jacrev`.
- `jacrev`  est souvent préférable quand `dim_sortie`$=\dim(f(x))$ est petite (notre cas ici). Ce cadre correspond exactement à ce qui se passe dans une **couche de réseau de neurones**, et se généralise naturellement à des architectures plus profondes.
- `jacfwd` est souvent préférable quand `dim_entree`$=$\dim(x)$ est petite.


