---
title: "Tutoriel de différentiation automatique"
from: markdown+emoji
lang: fr
format: html
toc: true
author:
  - name: Mahendra Mariadassou
    orcid: 0000-0003-2986-354X
    email: mahendra.mariadassou@inrae.fr
    affiliations:
      - name: INRAE - MaIAGE
        adress: Domaine de Vilvert
        city: Jouy-en-Josas
        state: France
  - name: Hugo Gangloff
  - name: Arthur Leroy
  - name: Lucia Clarotto
date: "2025-08-18"
date-modified: today
date-format: "[Last Updated on] MMMM, YYYY"
---

::: callout-note
L'obectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP et le VJP d'une fonction simple:
$$
f: x \in \mathbb{R}^p \mapsto tanh(a^\top x + b) \in \mathbb{R}
$$
avec $a \in \mathbb{R}^p$ et $b \in \mathbb{R}$. 
On considère les dérivées par rapport à l'entrée $x$ puis par rapport aux paramètres $a, b$. 
:::

# Théorie [XX]

# Exemple en Torch [MM, LC]

::: panel-tabset

## Dérivée par rapport à $x$

## Dérivée par rapport à $a$ et $b$

:::

# Exemple en JAX [HG, AL]

::: panel-tabset

```{python}
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random

dim = 100

a = jnp.arange(dim)/ (dim*10)
b = 0.5
x = (jnp.arange(dim) - 37) / (dim*10)

def f(x, a, b):
    return jnp.tanh(jnp.dot(a, x) + b)

f(x, a, b) 
```

On définit la fonction les gradients exacts à partir des formules analytiques. 

```{python}
def df_dx(x, a, b):
    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)
# def df_dab(x, a, b):
#     return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2

```

Puis on définit les gradients via autograd et on vérifie que les résultats sont identiques.

```{python}
## jax.grad calcule la formule backward par défaut
grad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) 
fwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) 

assert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))
assert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))

print('All good, we are ready to go!')
```

::: 