---
title: "Tutoriel de différentiation automatique"
from: markdown+emoji
lang: fr
format: html
toc: true
author:
  - name: Mahendra Mariadassou
    orcid: 0000-0003-2986-354X
    email: mahendra.mariadassou@inrae.fr
    affiliations:
      - name: INRAE - MaIAGE
        adress: Domaine de Vilvert
        city: Jouy-en-Josas
        state: France
  - name: Hugo Gangloff
  - name: Arthur Leroy
  - name: Lucia Clarotto
date: "2025-08-18"
date-modified: today
date-format: "[Last Updated on] MMMM, YYYY"
---

::: callout-note
L'obectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP et le VJP d'une fonction simple:
$$
f: x \in \mathbb{R}^p \mapsto tanh(a^\top x + b) \in \mathbb{R}
$$
avec $a \in \mathbb{R}^p$ et $b \in \mathbb{R}$. 
On considère les dérivées par rapport à l'entrée $x$ puis par rapport aux paramètres $a, b$. 
:::

# Théorie [XX]

# Exemple en Torch [MM, LC]

::: panel-tabset

## Dérivée par rapport à $x$

## Dérivée par rapport à $a$ et $b$

:::

# Exemple en JAX [HG, AL]

::: panel-tabset

```{python}
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random

dim = 100

a = jnp.arange(dim)/ (dim*10)
b = 0.5

def f(x, a, b):
    return jnp.tanh(jnp.dot(a, x) + b)

x = (jnp.arange(dim) - 37) / (dim*10)
f(x, a, b) 

## jax.grad calcule la formule backward par défaut
grad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) 
grad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)

def df_dx(x, a, b):
    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)
def df_dab(x, a, b):
    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2

assert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))


# assert jnp.allclose(jnp.array(grad_df_dab((a,b))), jnp.array(df_dab(x, a, b)))
assert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))

print('All good, we are ready to go!')
```



## Dérivée par rapport à $x$

## Dérivée par rapport à $a$ et $b$

::: 