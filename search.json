[
  {
    "objectID": "traccar.html",
    "href": "traccar.html",
    "title": "Traccar",
    "section": "",
    "text": "Traccar is an open-source GPS tracking platform. It allows you to track vehicles, people, or any GPS-enabled device in real time. It comes with a web User Interface to view devices on a map and can integrate a large number of GPS tracking devices.\nIt works with:\n\na GPS device (can be a smartphone) that sends location data to the Traccar server.\na server that stores data in a database (e.g., an OVH server).\na web app and APIs that allow you to visualize, analyze, or forward that data.\n\nWe first provide a tutorial to test the Traccar system (a demo provided by the developers). Then, we describe the main steps to build your own tracking system (setting up the server, connecting devices, and extracting the data from the database).\n\n\nFirst, download the Traccar client application and follow the installation steps.\n\n\n\n\n\nBy default, it should be connected to a demo server (http://demo.traccar.org) and have a specific device identifier.\nYou can connect to the server via this same link and add a system device (click the + symbol on the top of the left bar).\n\n\n\n\n\nTo connect your system device, you must enter your identifier in the Traccar client app in the identifier box.\n\n\n\n\n\nFrom the client app, send your location. This should now be visible on the Traccar Manager.\n‚ö†Ô∏è Note: There are several demo servers. If one server is not working properly (e.g., you cannot connect your device to it), try another one. Here is the list of demo servers.\nYou can follow your own track through time by clicking the replay button."
  },
  {
    "objectID": "traccar.html#demo-of-the-traccar-system",
    "href": "traccar.html#demo-of-the-traccar-system",
    "title": "Traccar",
    "section": "",
    "text": "First, download the Traccar client application and follow the installation steps.\n\n\n\n\n\nBy default, it should be connected to a demo server (http://demo.traccar.org) and have a specific device identifier.\nYou can connect to the server via this same link and add a system device (click the + symbol on the top of the left bar).\n\n\n\n\n\nTo connect your system device, you must enter your identifier in the Traccar client app in the identifier box.\n\n\n\n\n\nFrom the client app, send your location. This should now be visible on the Traccar Manager.\n‚ö†Ô∏è Note: There are several demo servers. If one server is not working properly (e.g., you cannot connect your device to it), try another one. Here is the list of demo servers.\nYou can follow your own track through time by clicking the replay button."
  },
  {
    "objectID": "traccar.html#setting-up-the-ovh-server",
    "href": "traccar.html#setting-up-the-ovh-server",
    "title": "Traccar",
    "section": "Setting up the OVH server",
    "text": "Setting up the OVH server\nFirst, subscribe to an OVH account for a Virtual Private Server (VPS). Choose an Ubuntu 24.04 distribution.\nIt is required to create an ssh authentification key. For the first login, you will need a temporary password, created for the first connection an send by email by OVH.\nAs Traccar relies on MySQL and we want to be able to administrate the MySQL Database, we first need to install the so-called LAMP stack:\n\nLinux\nApache\nMySQL\nphpMyAdmin\n\n\nStep 1: Install and configure Apache and MySQL\nsudo su -\napt update && apt upgrade -y && apt install apache2 -y\nsystemctl start apache2\nsystemctl enable apache2\n\napt install mysql-server -y\nsystemctl start mysql\nsystemctl enable mysql\n\nmysql_secure_installation\nChoose security level 1 (medium) and configure the root password.\nTypical configuration:\nPlease enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 1\nRemove anonymous users? (y|Y for Yes, any other key for No): y\nDisallow root login remotely? (y|Y for Yes, any other key for No): No\nRemove test database and access to it? (y|Y for Yes, any other key for No): n\nReload privilege tables now? (y|Y for Yes, any other key for No): y\nBy default, there is no root password. Let‚Äôs create one manually:\nmysql -u root\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_your_root_password';\n\n\nStep 2: Install PHP and phpMyAdmin\nThis not mandatory for traccar but is helpful to explore the database.\napt install php libapache2-mod-php php-mysql php-mbstring php-zip php-gd php-json php-curl -y\napt install phpmyadmin -y\nDuring installation, choose: - Apache - Do not configure the phpmyadmin database automatically\n\n\nStep 3: Create the phpMyAdmin database and user\nmysql -u root -p\nCREATE DATABASE phpmyadmin;\nCREATE USER 'phpmyadmin'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_phpmyadmin_password'; \nGRANT ALL PRIVILEGES ON phpmyadmin.* TO 'phpmyadmin'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n\n\nImport phpMyAdmin tables\nmysql -u root -p phpmyadmin &lt; /usr/share/phpmyadmin/sql/create_tables.sql\n\n\nEdit the configuration file\nnano /etc/phpmyadmin/config-db.php\n&lt;?php\n$dbuser='phpmyadmin';\n$dbpass='enter_phpmyadmin_password';\n$basepath='';\n$dbname='phpmyadmin';\n$dbserver='localhost';\n$dbport='3306';\n$dbtype='mysql';\n?&gt;\nYou can now access phpMyAdmin at:\nüëâ http://your-server-ip/phpmyadmin with your root or phpmyadmin credentials."
  },
  {
    "objectID": "traccar.html#installing-traccar",
    "href": "traccar.html#installing-traccar",
    "title": "Traccar",
    "section": "Installing Traccar",
    "text": "Installing Traccar\n\nStep 1: Prepare MySQL for Traccar\nThe Traccar installer temporarily uses root/root. We need to relax password restrictions:\nSET GLOBAL validate_password.LENGTH = 4;\nSET GLOBAL validate_password.policy = 0;\nSET GLOBAL validate_password.mixed_case_count = 0;\nSET GLOBAL validate_password.number_count = 0;\nSET GLOBAL validate_password.special_char_count = 0;\nSET GLOBAL validate_password.check_user_name = 0;\n\nALTER USER 'root'@'localhost' IDENTIFIED BY 'root';\nGRANT ALL ON *.* TO 'root'@'localhost' WITH GRANT OPTION;\n\nCREATE USER 'traccar'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_traccar_password';\nCREATE DATABASE traccar;\nGRANT ALL PRIVILEGES ON traccar.* TO 'traccar'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n\n\nStep 2: Install Traccar\nwget https://www.traccar.org/download/traccar-linux-64-latest.zip\napt install unzip\nunzip traccar-linux-*.zip\n./traccar.run\n\n\nStep 3: Configure Traccar\nnano /opt/traccar/conf/traccar.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE properties SYSTEM 'http://java.sun.com/dtd/properties.dtd'&gt;\n&lt;properties&gt;\n    &lt;entry key='database.driver'&gt;com.mysql.cj.jdbc.Driver&lt;/entry&gt;\n    &lt;entry key='database.url'&gt;jdbc:mysql://localhost/traccar?zeroDateTimeBehavior=round&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true&amp;useSSL=false&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useUnicode=yes&amp;characterEncoding=UTF-8&amp;sessionVariables=sql_mode=''&lt;/entry&gt;\n    &lt;entry key='database.user'&gt;traccar&lt;/entry&gt;\n    &lt;entry key='database.password'&gt;enter_traccar_password&lt;/entry&gt;\n&lt;/properties&gt;\n\n\nStep 4: Secure MySQL and start Traccar\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_your_root_password';\nGRANT ALL PRIVILEGES ON traccar.* TO 'traccar'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\nVerify connection:\nmysql -u root -p\nThen start Traccar:\nservice traccar start"
  },
  {
    "objectID": "traccar.html#this-last-step-will-fill-the-database-and-creates-the-diferent-tables-for-the-first-call",
    "href": "traccar.html#this-last-step-will-fill-the-database-and-creates-the-diferent-tables-for-the-first-call",
    "title": "Traccar",
    "section": "This last step will fill the database (and creates the diferent tables for the first call)",
    "text": "This last step will fill the database (and creates the diferent tables for the first call)"
  },
  {
    "objectID": "traccar.html#securing-with-apache-and-ssl",
    "href": "traccar.html#securing-with-apache-and-ssl",
    "title": "Traccar",
    "section": "Securing with Apache and SSL",
    "text": "Securing with Apache and SSL\nCreate the configuration file:\nnano /etc/apache2/sites-available/traccar.conf\n&lt;VirtualHost *:80&gt;\n  ServerName your-domain-or-ip\n  Redirect / https://your-domain-or-ip\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost _default_:443&gt;\n        ServerName your-domain-or-ip\n        ServerAdmin your_email@example.com\n\n        DocumentRoot /var/www/html\n\n        ProxyPass /api/socket ws://localhost:8082/api/socket\n        ProxyPassReverse /api/socket ws://localhost:8082/api/socket\n\n        ProxyPass / http://localhost:8082/\n        ProxyPassReverse / http://localhost:8082/\n\n        SSLEngine on\n        SSLCertificateFile /etc/ssl/certs/ssl-cert-snakeoil.pem\n        SSLCertificateKeyFile /etc/ssl/private/ssl-cert-snakeoil.key\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nInstall SSL and required modules:\nsudo apt-get install ssl-cert\nsudo a2enmod ssl proxy_http proxy_wstunnel rewrite\nsudo service apache2 restart\nEnable the site and generate a Let‚Äôs Encrypt certificate:\nsudo a2dissite 000-default\nsudo a2ensite traccar\nsudo service apache2 restart\nsudo apt install certbot python3-certbot-apache\nsudo certbot --apache\nThe server mysql is now ready and the database which will be used to record the position data is named traccar."
  },
  {
    "objectID": "traccar.html#setting-up-the-individual-device",
    "href": "traccar.html#setting-up-the-individual-device",
    "title": "Traccar",
    "section": "Setting up the individual device",
    "text": "Setting up the individual device\nThe traccar client is available on the Apple store and the Google store. Once downladed, there is a few step to set up the client\nEnter the server address http://51.91.58.42:5055,\nChoose Pr√©cision de la localisation : la plus √©llev√©e\nIntervalle (secondes) 30\nThe user has to send the device id to the traccar administrator\nOn the traccar server, click on the + to add a device and enter the device id.\nThe traccar app records the position when the traacar app is on. The user click on Envoyer la position to send the recoded positions to the traccar database."
  },
  {
    "objectID": "traccar.html#acc√®s-√†-la-base-de-donn√©es-tracca",
    "href": "traccar.html#acc√®s-√†-la-base-de-donn√©es-tracca",
    "title": "Traccar",
    "section": "Acc√®s √† la base de donn√©es tracca",
    "text": "Acc√®s √† la base de donn√©es tracca\nTO access the traccar database, one ption which works is to first create a ssh tunnel which links local port 3307 to remote port 3306 on the server via\nssh -L 3307:localhost:3306 root@51.91.58.42\nThen you can access to the database via mysql or through the following R script\n\nlibrary(DBI)\nlibrary(RMySQL)\n\n\n# Connexion\ncon &lt;- dbConnect(RMySQL::MySQL(),\n                 host = \"127.0.0.1\",\n                 port = 3307,\n                 dbname = \"traccar\",\n                 username = \"user_db_name\",\n                 password = \"user_db_passwd\")\n\n# Exemples de requ√™tes\ndevices &lt;- dbGetQuery(con, \"SELECT * FROM tc_devices\")\npositions &lt;- dbGetQuery(con, \"SELECT * FROM tc_positions\")\n\n# Fermer connexion\ndbDisconnect(con)\n\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.1     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\npositions |&gt; select(id, deviceid, devicetime, latitude,longitude) |&gt; group_by(deviceid) |&gt; count()\n\n# A tibble: 3 √ó 2\n# Groups:   deviceid [3]\n  deviceid     n\n     &lt;int&gt; &lt;int&gt;\n1        1    62\n2        2  1216\n3        3     9\n\npositions |&gt; select(id, deviceid, devicetime, latitude,longitude) |&gt;  ggplot() +aes(x=longitude, y = latitude, col = as.factor(deviceid)) + geom_path() \n\n\n\n\n\n\n\npositions |&gt; \n  mutate(temps = ymd_hms(devicetime)) |&gt;  \n  group_by(deviceid) |&gt;\n  mutate(elapse_time = temps - lag(temps)) |&gt; \n  ggplot() + aes(x=as.factor(deviceid), y = elapse_time) + geom_point() \n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "tutoGPT.html",
    "href": "tutoGPT.html",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Ceci est une version reprise de ce tutoriel pour faire une r√©gression par processus gaussien avec GPyTorch.\nimport math\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nNous commen√ßons par cr√©er des donn√©es simul√©es sur lesquelles on va apprendre la fonction de regression :\n\\[\\begin{split}\\begin{align}\ny &= \\sin(2\\pi x) + \\epsilon \\\\\n  \\epsilon &\\sim \\mathcal{N}(0, 0.04)\n\\end{align}\\end{split}\\]\n# training data is 50 points uniformly sampled in [0,1]\ntrain_x = torch.rand(10)\n# True function is sin(2*pi*x) with Gaussian noise\ntrain_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)\nEnsuite, nous d√©finissons un peu comme un r√©seau de neurones la classe permettant d‚Äôavoir un processus gaussien et nous initialisons. Les choix faits ici sont de prendre une esp√©rance constante (gpytorch.means.ConstantMean()) et un noyau de covariance gaussien gpytorch.kernels.RBFKernel(), la fonction gpytorch.kernels.ScaleKernel() sert √† d√©clarer qu‚Äôil y a un param√®tre de variance devant le noyau.\n# We will use the simplest form of GP model, exact inference\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n# initialize likelihood and model\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = ExactGPModel(train_x, train_y, likelihood)\nLes param√®tres du mod√®le de processus gaussien sont inf√©r√©s en cherchant √† optimiser num√©riquement la vraisemblance. Ces param√®tres estim√©s dans ce mod√®le sont :\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\ntraining_iter = 50\n\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    output = model(train_x)\n    # Calc loss and backprop gradients\n    loss = -mll(output, train_y)\n    loss.backward()\n    if i % 10 ==0:\n      print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n        i + 1, training_iter, loss.item(),\n        model.covar_module.base_kernel.lengthscale.item(),\n        model.likelihood.noise.item()\n      ))\n    optimizer.step()\n\nIter 1/50 - Loss: 1.072   lengthscale: 0.693   noise: 0.693\nIter 11/50 - Loss: 0.817   lengthscale: 0.312   noise: 0.314\nIter 21/50 - Loss: 0.666   lengthscale: 0.136   noise: 0.134\nIter 31/50 - Loss: 0.596   lengthscale: 0.198   noise: 0.067\nIter 41/50 - Loss: 0.597   lengthscale: 0.206   noise: 0.048\nLes estimations sont accessibles ainsi (il est √† noter que dans le mod√®le de vraisemblance propos√© un bruit d‚Äôobservation est inclus comme un bruit blanc gaussien homosc√©dastique) :\nprint(model)\nprint(model.likelihood.noise.item()) # param√®tre de variance du bruit d'observation (ajout√© par d√©faut)\nprint(model.covar_module.base_kernel.lengthscale) # param√®tre de port√©e\nprint(model.covar_module.outputscale) # param√®tre de variance du GP\nprint(model.mean_module.constant) # param√®tre d'esp√©rance inf√©r√©\n\nExactGPModel(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n0.05167459696531296\ntensor([[0.1979]], grad_fn=&lt;SoftplusBackward0&gt;)\ntensor(0.5579, grad_fn=&lt;SoftplusBackward0&gt;)\nParameter containing:\ntensor(-0.1032, requires_grad=True)\nNous allons √† pr√©sent pr√©senter des pr√©dictions et des simulations conditionnelles :\n# Get into evaluation (predictive posterior) mode\nmodel.eval()\nlikelihood.eval()\n\n# Test points are regularly spaced along [0,1]\n# Make predictions by feeding model through likelihood\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    test_x = torch.linspace(0, 1, 51)\n    observed_pred = likelihood(model(test_x))\n\nf_preds = model(test_x)\ny_preds = likelihood(model(test_x))\n\nf_mean = f_preds.mean\nf_var = f_preds.variance\nf_covar = f_preds.covariance_matrix\n# do 10 conditional simulations\nf_samples = f_preds.sample(sample_shape=torch.Size([10,]))\n\n\nwith torch.no_grad():\n    # Initialize plot\n    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n\n    # Get upper and lower confidence bounds\n    lower, upper = observed_pred.confidence_region()\n    # Plot training data as black stars\n    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n    # Plot predictive means as blue line\n    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n    # Plot two conditional simulations in red\n    ax.plot(test_x.numpy(), f_samples[0,:].numpy(), 'r')\n    ax.plot(test_x.numpy(), f_samples[5,:].numpy(), 'r')\n    # Shade between the lower and upper confidence bounds\n    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n    ax.set_ylim([-3, 3])\n    ax.legend(['Observed Data', 'Mean','simu1','simu2', 'Confidence'])\n\n/usr/local/lib/python3.12/dist-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n  warnings.warn("
  },
  {
    "objectID": "tutoGPT.html#√†-faire",
    "href": "tutoGPT.html#√†-faire",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "√Ä faire",
    "text": "√Ä faire\nIl serait int√©ressant de tester les performances de ce module pour faire la pr√©diction dans le cas o√π il y a un grand nombre de points d‚Äôapprentissage et en grande dimension."
  },
  {
    "objectID": "GNNpytgeo.html",
    "href": "GNNpytgeo.html",
    "title": "Apprentissage de r√©seaux bipartites avec pytorch-geometric",
    "section": "",
    "text": "L‚Äôobjectif est de proposer une repr√©sentation latente des n≈ìuds d‚Äôun r√©seau bipartite (ou de plusieurs) dans un espace euclidien. Un sous-objectif peut √™tre de pr√©dire des dyades manquantes (non observ√©es) que ce soient des ar√™tes ou des non-ar√™tes. Nous allons utiliser des ‚Äúvariational graph auto-encoder‚Äù, il est possible d‚Äôaller consulter les tutoriels correspondant sur la page de pytorch-geometrics et celui pr√©sent√© lors d‚Äôun pr√©c√©dent atelier State of the R. Dans ce document, nous nous concentrons sur l‚Äôaspect sous-√©chantillonnage du r√©seau permettant de cr√©er les jeux de donn√©es d‚Äôapprentissage et d‚Äôentra√Ænement.\nL‚Äôapprentissage de la repr√©sentation latente se fait sur l‚Äôoptimisation d‚Äôune perte, √† savoir l‚Äôentropie crois√©e, de la pr√©diction d‚Äôar√™tes et de non ar√™tes (1 ou 0 dans la matrice d‚Äôincidence correspondant au r√©seau). Cet ensemble d‚Äôar√™tes et de non ar√™tes choisies en proportions √©gales s‚Äôappelle le jeu d‚Äôentra√Ænement et est consid√©r√© connu pour effectuer les convolutions au sein des couches de GNN (graph neural network). L‚Äô√©valuation de la performance se fait sur un autre sous-ensemble d‚Äôar√™tes et de non ar√™tes, appel√© le jeu de test et est inconnu lors de l‚Äôentra√Ænement et n‚Äôest pas utilis√© pour l‚Äôencodage (succession de couches de GNN projetant les n≈ìuds dans l‚Äôespace latent). Un autre jeu de donn√©es du m√™me type peut √™tre cr√©√© comme jeu de validation.\nPour ce faire la fonction RandomLinkSplit(...) de pytorch-geometric permet de faire cette s√©paration en trois jeux de donn√©es (ensemble d‚Äôar√™tes et non ar√™tes) d‚Äôun objet de type BipartiteData qui n‚Äôest pas une classe nativement d√©finie dans pytorch-geometric mais c‚Äôest la recommandation trouv√©e en ligne pour d√©finir ce genre de r√©seaux.\n\nimport numpy as np\nimport torch\nimport torch_geometric\nfrom torch import Tensor\nfrom torch_geometric.data import Data\n\nclass BipartiteData(Data):\n    def __inc__(self, key, value, *args, **kwargs):\n        if key == \"edge_index\":\n            return Tensor(\n                [[self.x_s.size(0)], [self.x_t.size(0)]]\n            )  # source and target (two classes of bipartite graph)\n        return super().__inc__(key, value, *args, **kwargs)\n\n    def is_bipartite(self):\n        return True\n\nOn d√©finit ensuite un r√©seau bipartite √† partir d‚Äôun r√©seau g√©n√©r√© al√©atoirement :\n\n# Generate a random bipartite graph\nnum_nodes = (10, 30)  # 100 nodes in each partition\nnum_edges = 50  # Total number of edges\nedge_index = generate_random_graph(num_nodes, num_edges)\n# Convert to PyTorch tensor\nedge_index = torch.tensor(edge_index, dtype=torch.long)\n# Create a BipartiteData object\nrandom_network = BipartiteData(\n    name=\"Random Graph\",\n    id_net=0,\n    x_s=torch.arange(start=0,end=num_nodes[0],dtype=torch.float).reshape(-1, 1),\n    x_t=torch.arange(start=0,end=num_nodes[1],dtype=torch.float).reshape(-1, 1),\n    edge_index=edge_index,\n    num_nodes=len(edge_index[0].unique())+len(edge_index[1].unique()),\n)\nprint(random_network)\n\nBipartiteData(edge_index=[2, 50], name='Random Graph', id_net=0, x_s=[10, 1], x_t=[30, 1], num_nodes=35)\n\n\no√π\n\nname et id_net permettent de renseigner un nom et un identifiant au r√©seau (utile particuli√®rement si on a plusieurs r√©seaux),\nx_s et x_t donnent les matrices de covariables sur les n≈ìuds en ligne et en colonne respectivement,\nedge_index contient une matrice √† 2 lignes et dont le nombre de colonnes correspond au nombre d‚Äôar√™tes,\nnum_nodes est le nombre de n≈ìuds dans le r√©seau mais cela posera des probl√®mes comme nous le verrons.\n\n√Ä pr√©sent, nous allons pouvoir proposer un sous-√©chantillonnage en jeux d‚Äôentra√Ænement, de validation et de test.\n\nimport torch_geometric.transforms as T\n\ntransform = T.RandomLinkSplit(\n            num_test=0.2,\n            num_val=0.1,\n            split_labels=True\n        )\n        \ntrain_data, val_data, test_data = transform(random_network)        \n\no√π\n\nnum_test permet de choisir la proportion de donn√©es dans le jeu de test,\nnum_val permet de choisir la proportion de donn√©es dans le jeu de validation,\nsplit_labels fix√© √† vrai permet de s√©parer en deux parties les ar√™tes et les non ar√™tes.\n\nSi on regarde ce que contiennent les objets train_data, val_data et test_data, on a :\n\nprint(train_data)\nprint(val_data)\nprint(test_data)\n\nBipartiteData(edge_index=[2, 35], name='Random Graph', id_net=0, x_s=[10, 1], x_t=[30, 1], num_nodes=35, pos_edge_label=[35], pos_edge_label_index=[2, 35], neg_edge_label=[35], neg_edge_label_index=[2, 35])\nBipartiteData(edge_index=[2, 35], name='Random Graph', id_net=0, x_s=[10, 1], x_t=[30, 1], num_nodes=35, pos_edge_label=[5], pos_edge_label_index=[2, 5], neg_edge_label=[5], neg_edge_label_index=[2, 5])\nBipartiteData(edge_index=[2, 40], name='Random Graph', id_net=0, x_s=[10, 1], x_t=[30, 1], num_nodes=35, pos_edge_label=[10], pos_edge_label_index=[2, 10], neg_edge_label=[10], neg_edge_label_index=[2, 10])\n\n\nChaque objet contient edge_index qui est le m√™me entre train_data et val_data et qui contient les ar√™tes consid√©r√©es comme connues et qui servent √† faire les convolutions dans les GNN pour l‚Äôentra√Ænement et la validation. Pour le test, on ajoute les ar√™tes du jeu validation au jeu d‚Äôentra√Ænement pour faire les convolutions.\nL‚Äôattribut pos_edge_label_index donne des indices d‚Äôar√™tes (sous-ensemble des ar√™tes existantes dans le r√©seau) et l‚Äôattribut neg_edge_label_index donne des indices de non ar√™tes.\nSi on regarde\n\nprint(test_data.pos_edge_label_index)\nprint(test_data.neg_edge_label_index)\n\ntensor([[ 2,  0,  5,  5,  2,  9,  3,  6,  0,  0],\n        [ 9, 17, 19,  9, 12,  4, 27,  2, 16,  4]])\ntensor([[14, 17,  3, 12, 31, 32, 26,  4,  5, 27],\n        [ 1, 34,  6,  6, 17, 21, 13, 26, 17,  3]])\n\n\non s‚Äôaper√ßoit d‚Äôune incoh√©rence sur les non-ar√™tes car nous obtenons des indices de n≈ìuds strictement sup√©rieurs aux nombres de n≈ìuds en ligne ou en colonne‚Ä¶ Cela vient du fait qu‚Äôil fait l‚Äô√©chantillonnage des non-ar√™tes sur la conversion en matrice d‚Äôadjacence du r√©seau bipartite, c‚Äôest-√†-dire sur une matrice de taille la somme des nombres de n≈ìuds en ligne et en colonne ayant des blocs diagonaux vides (car il n‚Äôy a pas d‚Äôinteraction au sein de chaque type de n≈ìuds). Il faut donc utiliser ‚Äúcorrectement‚Äù la fonction permettant d‚Äô√©chantillonner des non-ar√™tes au sein de la matrice d‚Äôincidence ce que ne fait pas la fonction RandomLinkSplit pour des graphes bipartites‚Ä¶\n\nfrom torch_geometric.utils import negative_sampling\nneg_edge_index = negative_sampling(\n                edge_index=train_data.edge_index,\n                num_nodes=(train_data.x_s.size(0), train_data.x_t.size(0)),\n                num_neg_samples=train_data.pos_edge_label_index.size(1),\n            )\nprint(neg_edge_index)            \n\ntensor([[ 0,  8,  2,  2,  8,  9,  4,  9,  8,  8,  2,  1,  9,  8,  9,  3,  4,  3,\n          0,  7,  1,  6,  3,  6,  5,  7,  1,  4,  1,  3,  9,  0,  3,  0,  2],\n        [ 3, 10, 11, 19, 25, 18, 24,  7,  2, 13, 27, 17, 17,  3, 15, 28,  9,  7,\n         12,  0, 13, 23, 12, 21,  1,  1, 20,  7, 26, 14, 26,  0, 25,  4, 18]])\n\n\nLors de l‚Äôoptimisation de la perte sur le jeu d‚Äôentra√Ænement, on utilise les m√™mes ar√™tes que celles servant √† la convolution et on ajoute des non ar√™tes (0 dans la matrice d‚Äôincidence) √©chantillonn√©es comme ci-dessus. Pour le calcul des m√©triques de validation, on fait de m√™me avec les ar√™tes dans val_data et en ajoutant des non-ar√™tes par rapport √† celles utilis√©es lors de la convolution gr√¢ce √† la fonction negative_sampling.\n√Ä la fin de l‚Äôentra√Ænement, on teste sur le jeu de test test_data mais en utilisant les convolutions sur la r√©unions des ar√™tes contenues dans train_data et val_data. Les non ar√™tes du jeu de test doivent √™tre choisies par rapport aux non ar√™tes du r√©seau complet."
  },
  {
    "objectID": "how_to_build_your_package.html",
    "href": "how_to_build_your_package.html",
    "title": "How To Build Your Package - Julia edition",
    "section": "",
    "text": "make a package available on GitHub - used in this work\nExample template for GitHub\nBest practices in Julia\nModules\nPkgTemplates.jl (is used in following )\nmake a package using PkgTemplates, then register it for the julia registry - way simpler !"
  },
  {
    "objectID": "how_to_build_your_package.html#defining-a-module",
    "href": "how_to_build_your_package.html#defining-a-module",
    "title": "How To Build Your Package - Julia edition",
    "section": "Defining a module",
    "text": "Defining a module\nFor large Julia packages, the code is usually organized in files, like\nmodule SomeModule\n# export, public, using, import statements are usually here; we discuss these below\ninclude(\"file1.jl\")\ninclude(\"file2.jl\")\nend"
  },
  {
    "objectID": "how_to_build_your_package.html#export-lists",
    "href": "how_to_build_your_package.html#export-lists",
    "title": "How To Build Your Package - Julia edition",
    "section": "Export lists",
    "text": "Export lists\nexport list = name of the different functions, types, global variables, constants‚Ä¶ that are visible from the outside of the module when using it.\nmodule NiceStuff\n       export nice, DOG\n       struct Dog end      # singleton type, not exported\n       const DOG = Dog()   # named instance, exported\n       nice(x) = \"nice $x\" # function, exported\nend;\nmeans that when calling\n using .NiceStuff\nfunctions nice and constant DOG will be available under their names nice and DOG , but that Dog will be available through NiceStuff.Dog.\nFor example, it means that if we define helper functions used inside a fit_mle then we can keep them hidden in the package, and only export the final function !\nThere can be as many export as wanted, located anywhere in the module block, but it is recommended to put it at the beggining of the code."
  },
  {
    "objectID": "how_to_build_your_package.html#using-and-import-a-module",
    "href": "how_to_build_your_package.html#using-and-import-a-module",
    "title": "How To Build Your Package - Julia edition",
    "section": "using and import a module",
    "text": "using and import a module\nLoading from a package : using Module. Loading locally : using .Module\nimporting import .Module imports the module name, all of the objects must be named as NiceStuff.nice for example using the module name as a prefix. Or we can use import .Module: something to get the name something directly.\nTo modify a function (for example proposing a new class and extending a function to this new class ), there are two ways :\nCall the function by its entire name\nusing .NiceStuff\n\nstruct Cat end\n\nNiceStuff.nice(::Cat) = \"nice üò∏\"\n@show(nice(Cat()))\nOr import it like this (this is where import is useful)\nimport .NiceStuff: nice\nnice(::Cat) = \"oo\"\n@show nice(Cat())\nWe can rename an import like this\nimport CSV as joli_csv"
  },
  {
    "objectID": "how_to_build_your_package.html#example-of-homemade-module",
    "href": "how_to_build_your_package.html#example-of-homemade-module",
    "title": "How To Build Your Package - Julia edition",
    "section": "Example of homemade module",
    "text": "Example of homemade module\nimport Pkg;\nPkg.add(\"Distributions\");\nPkg.add(\"ExtendedExtremes\");\nPkg.add(\"DocStringExtensions\");\n\n\n\n\n\n\nNote\n\n\n\nDocStringExtensions.jl exports a collection of macros, which can be used to add useful automatically generated information to docstrings.\n\n\n\nmodule MyExtendedExtremes\n\nexport MixedUniformTail, pdf, cdf, quantile, rand, fit_mix  #I export everything \n\nusing Distributions\nusing DocStringExtensions\nusing ExtendedExtremes\nusing Random \n\n\"\"\"\n$(TYPEDEF)\n\n$(TYPEDFIELDS)\n \nI wanted to put an uniform on the left part, an EGPD on the bulk and tail. \nEGPD only works when filtering very low value but I want all the values ! \n\"\"\"\nstruct MixedUniformTail{\n    T1&lt;:ContinuousUnivariateDistribution,\n    T2&lt;:ContinuousUnivariateDistribution,\n} &lt;: ContinuousUnivariateDistribution\n    \" probability of the left part \"\n    p::Float64\n    \" left part \"\n    uniform_part::T1\n    \" right part \"\n    tail_part::T2 \n    \" minimum value, for precip it is 0.1 \"\n    a::Float64 \n    \" threshold between both part, 0.5 for precips (included in left part) \"\n    b::Float64 \nend\n\nimport Distributions: pdf\n\n\"\"\"\n$(SIGNATURES)\n\nPDF\n\"\"\"\nfunction pdf(d::MixedUniformTail, y::Real)\n    if y &lt; d.a\n        return 0.0\n    elseif y &lt;= d.b\n        return d.p * pdf(d.uniform_part, y)\n    else\n        return (1 - d.p) * pdf(d.tail_part, y - d.b)\n    end\nend\n\nimport Distributions: cdf\n\n\"\"\"\n$(SIGNATURES)\n\nCDF\n\"\"\"\nfunction cdf(d::MixedUniformTail, y::Real)\n    if y &lt; d.a\n        return NaN\n    elseif y &lt;= d.b\n        return d.p * cdf(d.uniform_part, y)\n    else\n        return d.p + (1 - d.p) * cdf(d.tail_part, y - d.b)\n    end\nend\n\nimport Distributions: quantile\n\n\"\"\"\n$(SIGNATURES)\n\nQuantile function\n\"\"\"\nfunction quantile(d::MixedUniformTail, q::Real)\n    if q &lt; 0 || q &gt; 1\n        throw(DomainError(q, \"Quantile outside [0,1]\"))\n    end\n    if q &lt;= d.p\n        return quantile(d.uniform_part, q / d.p)\n    else\n        return d.b + quantile(d.tail_part, (q - d.p) / (1 - d.p))\n    end\nend\n\nimport Base: rand\n\nfunction rand(rng::AbstractRNG, d::MixedUniformTail)\n    if rand(rng) &lt;= d.p\n        return rand(rng, d.uniform_part)\n    else\n        return d.b + rand(rng, d.tail_part)\n    end\nend\n\nrand(d::MixedUniformTail) = rand(Random.GLOBAL_RNG, d)\n\n\"\"\"\n$(SIGNATURES)\n\nestimation\n\"\"\"\nfunction fit_mix(::Type{MixedUniformTail}, data; left = 0.1, middle = 0.5)\n    u = middle\n    prop_smallrain = sum(left .&lt;= data .&lt;= u) / sum(data .&gt; 0)\n    y = data[data .&gt; u] .- u\n\n    tail_part = fit_mle(ExtendedGeneralizedPareto{TBeta}, y)\n\n    return MixedUniformTail(prop_smallrain, Uniform(left, middle), tail_part, left, middle)\nend\n\nend\nNow, I have defined my module and I can use my new functions by using it:\nusing .MyModuleExtendedExtremes #it is a local module, so I have to use it like this\nusing Distributions\nusing ExtendedExtremes\nTry the new functions\n# Parameters\n\na, b = 0.1, 0.5\np = 0.3\n\n# Create the mixed distribution\n\nd = MixedUniformTail(p, Uniform(a, b), ExtendedGeneralizedPareto( TBeta(0.4), GeneralizedPareto(0.0,3, 0.1))   , a, b)\n\n# Example: use it like any Distributions.jl distribution\nprintln(pdf(d, 0.2))      # PDF in uniform part\nprintln(cdf(d, 0.2))      # CDF in uniform part\nprintln(quantile(d, 0.8)) # Quantile in tail part\n\n# Random draws\nsamples = rand(d, 10000)\n\nusing Plots\nhistogram(samples, bins=50, normalize=true, label=\"Sampled PDF\",alpha=0.3)\nplot!(x -&gt; pdf(d, x), 0, 50, label=\"Theoretical PDF\", lw=2)\n\n\ndd = fit_mix(MixedUniformTail,samples)\nsamples2=rand(dd,10000)\nhistogram!(samples2 , bins=50,normalize=true,label=\"samples from fitted on previous samples\",alpha=0.3)\nConclusion here : we have a module and some tests, now, how do we make that a proper package ?"
  },
  {
    "objectID": "how_to_build_your_package.html#creating-a-package",
    "href": "how_to_build_your_package.html#creating-a-package",
    "title": "How To Build Your Package - Julia edition",
    "section": "Creating a package",
    "text": "Creating a package\n\nCreate repository on GitHub. Custom : name it PackageName.jl\n\nFor me, MyExtendedExtremes.jl\n\ngenerate a basic package structure locally using generate\n\ncd to a folder where we want the package to be created. cd julia_package in my case\nstart Julia in this folder then in Julia REPL type ]¬†generate PackageName (in my case ]¬†generate MyExtendedExtremes)\n\n\n(@v1.11) pkg&gt; generate MyExtendedExtremes\n  Generating  project MyExtendedExtremes:\n    Project.toml\n    src/MyExtendedExtremes.jl\ngenerate has created the following files and folders :\nMyExtendedExtremes/\n‚îú‚îÄ‚îÄ Project.toml\n‚îú‚îÄ‚îÄ src/\n    ‚îî‚îÄ‚îÄ MyExtendedExtremes.jl\nThe Project.toml contains for now only this :\nname = \"MyExtendedExtremes\"\nuuid = \"8f8ad11f-fafd-4bca-9428-2b36ffe65f47\"\nauthors = [\"cognot &lt;caroline.cognot@agroparistech.fr&gt;\"]\nversion = \"0.1.0\"\n\n\n\n\n\n\nNote\n\n\n\nTo develop your package, it is advisable to use Revise.jl. It may help you keep your Julia sessions running longer, reducing the need to restart when you make changes to code.\n\n\n\nactivate the package folder. In the same julia REPL : type ] activate Full/path/PackageName (in my case ]activate MyExtendedExtremes)\n\nResult in the REPL :\n(@v1.11) pkg&gt; activate MyExtendedExtremes\n  Activating project at `~/StateOfTheR/finistr2025/julia_package/MyExtendedExtremes`\nNow, the REPL when typing ] looks like this : (MyExtendedExtremes) pkg&gt;\n\nAdding the required packages : (in my case, it will be Distributions,ExtendedExtremes,Plots,Random) Activating has created an environment for the folder. This is when we can add packages dependencies, which will modify the Project.toml. (do not add unecessary packages). After adding the packages, now sections of the Project.toml have been created named [deps]and [compat].\nSpecify compatible functions in the [compat] section :\n\nadd julia = \"1.10.0\" for example (means, the package will work for versions of julia after 1.10.0)\nmodify if necessary the versions of the packages needed for compatibility.\n\n\nNow, it is the time to put all this on GitHub."
  },
  {
    "objectID": "how_to_build_your_package.html#link-the-package-to-github",
    "href": "how_to_build_your_package.html#link-the-package-to-github",
    "title": "How To Build Your Package - Julia edition",
    "section": "Link the package to GitHub",
    "text": "Link the package to GitHub\nWe can exit() julia.\nThe tutorial says to put this in git. First, go in the folder\ncd MyExtendedExtremes #I added this.\ngit init     # Initialise the git repository\ngit add .    # Add all files, including in subfolders\ngit commit -a -m \"Initial package structure of MyAwasomePackage\" # Create a first commit\ngit branch -m main # Rename \"master\" to \"main\" as of the new GitHub policy\ngit remote add origin git@github.com:caroline-cognot/MyExtendedExtremes.jl.git # Link the remote github repository to the local one\ngit config pull.rebase false # Allow mering of remote vs local codebase for next step\ngit pull origin main --allow-unrelated-histories # Fetch the Readme and gitignore we created when we created the repository\ngit push --set-upstream origin main # Finally upload everything back to the GitHub repository\nNow, the package exists : let us start julia, then we can add it by\nusing Pkg\nPkg.add(url=\"git@github.com:caroline-cognot/MyExtendedExtremes.jl.git\")\nNow I can import my package without the ‚Äú.‚Äù\nusing MyExtendedExtremes \nBut there is no code inside yet. I have to add my module and make some tests.\nBy doing this, the package is moved in the julia dev directory :\n(@v1.11) pkg&gt; dev MyExtendedExtremes\nThe package and its code is now located inside this folder. We have to modify this folder instead of our previous folder (the old files are useless).\n(@v1.11) pkg&gt; dev /home/caroline/.julia/dev/MyExtendedExtremes"
  },
  {
    "objectID": "how_to_build_your_package.html#adding-functionalities-to-the-package",
    "href": "how_to_build_your_package.html#adding-functionalities-to-the-package",
    "title": "How To Build Your Package - Julia edition",
    "section": "Adding functionalities to the package",
    "text": "Adding functionalities to the package\nNow, I can edit my source by opening the file in VSCODE :\ncode /home/caroline/.julia/dev/MyExtendedExtremes/src/MyExtendedExtremes.jl\nAfter editing the file, restarting Julia then importing/using the package will make the functions available.\nusing MyExtendedExtremes\nusing Distributions,Random\nusing ExtendedExtremes\n######### try the new functions ###############################\"\n# Parameters\na, b = 0.1, 0.5\np = 0.3\n\n# Create the mixed distribution\nd = MixedUniformTail(p, Uniform(a, b), ExtendedGeneralizedPareto( TBeta(0.4), GeneralizedPareto(0.0,3, 0.1))   , a, b)"
  },
  {
    "objectID": "how_to_build_your_package.html#adding-tests",
    "href": "how_to_build_your_package.html#adding-tests",
    "title": "How To Build Your Package - Julia edition",
    "section": "Adding tests",
    "text": "Adding tests\ngo in the package directory, add a test/runtests.jl file and open it in vscode\ncd /home/caroline/.julia/dev/MyExtendedExtremes\nmkdir -p test \ncode test/runtests.jl\nNow, when I want to test the package, type\n] test MyExtendedExtremes"
  },
  {
    "objectID": "how_to_build_your_package.html#forgot-a-dependency",
    "href": "how_to_build_your_package.html#forgot-a-dependency",
    "title": "How To Build Your Package - Julia edition",
    "section": "Forgot a dependency ?",
    "text": "Forgot a dependency ?\n\ngo in the package folder\n\ncd /home/caroline/.julia/dev/MyExtendedExtremes\n\nstart julia\nactivate the package\n\nusing Pkg\nPkg.activate(\"/home/caroline/.julia/dev/MyExtendedExtremes\")\n\nadd the missing package\n\nPkg.add(\"Random\")\nIf it still does not work : in doubt kill all terminals and restart.\nor do it the way the tutorial says it in when adding the Test package :\n(@v1.x) pkg&gt; activate [USER_HOME_FOLDER]/.julia/dev/MyAwesomePackage/test/ #(remove test/ if the package has to be added in the package and not just in the test folder)\nadd Test\n(test) pkg&gt; activate # Without arguments, \"activate\" brings back to the default environment\n(@v1.x) pkg&gt; test MyAwesomePackage  # This perform the test"
  },
  {
    "objectID": "how_to_build_your_package.html#prepare-for-git-commit",
    "href": "how_to_build_your_package.html#prepare-for-git-commit",
    "title": "How To Build Your Package - Julia edition",
    "section": "Prepare for git commit",
    "text": "Prepare for git commit\nThe tutorial says to add a functionality that performs actions each time we git push by adding a file `[USER_HOME_FOLDER]/.julia/dev/MyExtendedExtremes/.github/workflows/ci.yml ÃÄ\nThen it says to add badges on the readme .\nFrom a terminal in the right folder (~/.julia/dev/MyExtendedExtremes) I can now do\ngit commit -a -m \"Adding functions and testing\"\ngit push"
  },
  {
    "objectID": "how_to_build_your_package.html#it-is-here",
    "href": "how_to_build_your_package.html#it-is-here",
    "title": "How To Build Your Package - Julia edition",
    "section": "It is here !",
    "text": "It is here !\nhttps://github.com/caroline-cognot/MyExtendedExtremes.jl/"
  },
  {
    "objectID": "positron.html",
    "href": "positron.html",
    "title": "Utilisation de l‚ÄôIDE positron pour R et python",
    "section": "",
    "text": "D√©j√† test√© lors de l‚Äô√©dition 2024, positron a √©t√© largement test√© cette ann√©e. Cependant, il est encore en version beta. L‚Äôint√©r√™t est d‚Äôavoir un environnement de travail similaire sous R et python avec un rendu proche de Rstudio (fen√™tres √©diteur, console, variables et graphiques). Il est d√©velopp√© √† partir du logiciel VS code.\nIl est √† noter que les packages la notion de projet famili√®re en R se traduit ici par un r√©pertoire de travail.\nAvantages :\nInconv√©nients :"
  },
  {
    "objectID": "positron.html#cas-pratique",
    "href": "positron.html#cas-pratique",
    "title": "Utilisation de l‚ÄôIDE positron pour R et python",
    "section": "Cas pratique",
    "text": "Cas pratique\nUn coll√®gue dispose d‚Äôun d√©p√¥t git contenant son code Python et incluant un fichier requirements.txt listant les biblioth√®ques Python n√©cessaires pour faire tourner son code. Voici en quelques √©tapes comment cr√©er l‚Äôenvironnement Python correspondant.\n\nCloner le projet soit en ligne de commande, soit en utilisant le menu File&gt;NewFolder from Git en choissant le r√©pertoire dans lequel cloner le projet et en donnant l‚Äôurl du d√©p√¥t √† cloner. Cette deuxi√®me solution permet d‚Äô√™tre directement dans le bon r√©pertoire, sinon ouvrir le r√©pertoire avec Open Folder.\nCr√©er l‚Äôenvironnement Python\n\n\nOuvrir la palette de commandes avec Ctrl/Cmd + Shift + P\nChoisir Python::Create Environment()\nChoisir par exemple Venv puis cocher utiliser le fichier requirements.txt"
  },
  {
    "objectID": "positron.html#quelques-r√©f√©rences-utiles",
    "href": "positron.html#quelques-r√©f√©rences-utiles",
    "title": "Utilisation de l‚ÄôIDE positron pour R et python",
    "section": "Quelques r√©f√©rences utiles",
    "text": "Quelques r√©f√©rences utiles\n\nMat√©riel issu d‚Äôun workshop pour les utilisateurs de RStudio : https://posit-dev.github.io/positron-workshop/"
  },
  {
    "objectID": "gp_exploration.html",
    "href": "gp_exploration.html",
    "title": "Manipulation efficace de Processus Gaussien",
    "section": "",
    "text": "‚Äì\n\n\nLe site qui explique la librairie - Objectif : mod√®les de processus gaussiens scalables et flexibles.\n- Points forts : - Int√©gration compl√®te dans l‚Äô√©cosyst√®me PyTorch (optimisation, r√©seaux de neurones, GPU). - M√©thodes d‚Äôinf√©rence variationnelle et approximations pour grands jeux de donn√©es (SVGP, SKI, etc.).\n\n\n\nLe site - Objectif : mod√®les de processus gaussiens fonctionnels et composables.\n- Points forts : - Diff√©rentiation automattrique compl√®te gr√¢ce √† JAX. - Compilation XLA pour rapidit√© et passage √† l‚Äô√©chelle. - Support natif d‚Äôune approche probabiliste fonctionnelle.\n\n\nIl s‚Äôagit du tutoriel disponible ici\n\n\n\nRasmussen 2006"
  },
  {
    "objectID": "gp_exploration.html#gpytorch",
    "href": "gp_exploration.html#gpytorch",
    "title": "Manipulation efficace de Processus Gaussien",
    "section": "",
    "text": "Le site qui explique la librairie - Objectif : mod√®les de processus gaussiens scalables et flexibles.\n- Points forts : - Int√©gration compl√®te dans l‚Äô√©cosyst√®me PyTorch (optimisation, r√©seaux de neurones, GPU). - M√©thodes d‚Äôinf√©rence variationnelle et approximations pour grands jeux de donn√©es (SVGP, SKI, etc.)."
  },
  {
    "objectID": "gp_exploration.html#gpjax",
    "href": "gp_exploration.html#gpjax",
    "title": "Manipulation efficace de Processus Gaussien",
    "section": "",
    "text": "Le site - Objectif : mod√®les de processus gaussiens fonctionnels et composables.\n- Points forts : - Diff√©rentiation automattrique compl√®te gr√¢ce √† JAX. - Compilation XLA pour rapidit√© et passage √† l‚Äô√©chelle. - Support natif d‚Äôune approche probabiliste fonctionnelle.\n\n\nIl s‚Äôagit du tutoriel disponible ici\n\n\n\nRasmussen 2006"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#o√π-quand",
    "href": "index.html#o√π-quand",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#programme",
    "href": "index.html#programme",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Programme",
    "text": "Programme"
  },
  {
    "objectID": "index.html#participants",
    "href": "index.html#participants",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Participants",
    "text": "Participants\nBaptiste Alglave, Julie Aubert, Pierre Barbillon, Gloria Buritica, Lucia Clarotto, Caroline Cognot, Marie-Pierre Etienne, Armand Favrot, Blanche Francheterre, Hugo Gangloff, Pascal Irz, Louis Lacoste, Arthur Leroy, Mahendra Mariadassou, Pierre Navaro, L√©o Micollet, Jeanne Tous."
  },
  {
    "objectID": "index.html#soutien",
    "href": "index.html#soutien",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Soutien",
    "text": "Soutien"
  },
  {
    "objectID": "package_python_tuto_annexe.html",
    "href": "package_python_tuto_annexe.html",
    "title": "Python packaging basics",
    "section": "",
    "text": "Tutorial on package structure, imports, the __init__.py file, and data usage."
  },
  {
    "objectID": "package_python_tuto_annexe.html#references",
    "href": "package_python_tuto_annexe.html#references",
    "title": "Python packaging basics",
    "section": "References",
    "text": "References\n\nhttps://realpython.com/python-modules-packages/\nhttps://importlib-resources.readthedocs.io/en/latest/using.html and https://stackoverflow.com/questions/779495/access-data-in-package-subdirectory\nhttps://realpython.com/python-init-py/"
  },
  {
    "objectID": "package_python_tuto_annexe.html#package-structure",
    "href": "package_python_tuto_annexe.html#package-structure",
    "title": "Python packaging basics",
    "section": "Package structure",
    "text": "Package structure\nWe consider the following architecture:\ntest: tree pkg\npkg\n‚îú‚îÄ‚îÄ mod1.py\n‚îú‚îÄ‚îÄ mod2.py\n‚îî‚îÄ‚îÄ sub_pkg\n    ‚îî‚îÄ‚îÄ sub_option.py\nwith\n\n# mod1.py\nfrom pkg.mod2 import add_plus_two\n\ndef predict (x):\n    return (add_plus_two (x) + 3)\n\n\n# mod2.py\ndef add_plus_two (x):\n    return (x + 2)\n\n\n# sub_option.py\ndef soption (x):\n    return (x * 2)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we want a function in one module to use a function from another module (for example, here the function predict from module 1 that uses the function add_plus_two from module 2) we need to import this function with from: from path import function. For path, we have two options: absolute imports and relative imports. For absolute imports, the starting point is the folder where the package is located (here pkg/). Relative imports are discussed further below.\n\n\nFrom there, we can start Python from the test folder and run:\ntest: python3\n\nPython 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n&gt;&gt;&gt; from pkg.mod1 import predict\n&gt;&gt;&gt; predict (4)\n9\nBut we cannot run:\ntest: python3\n\n&gt;&gt;&gt; from pkg import predict\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nImportError: cannot import name 'predict' from 'pkg' (/home/mmip/test/pkg/__init__.py)\nIf we want to be able to do that, we can use a file named __init__.py, which, among other things, allows us to expose functions.\nThe __init__.py files present in the structure of a package are executed when the package or one of its modules is imported. More information on this file here.\nWe add an __init__.py file placed in pkg/:\ntest: tree pkg\npkg\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ mod1.py\n‚îú‚îÄ‚îÄ mod2.py\n‚îî‚îÄ‚îÄ sub_pkg\n    ‚îî‚îÄ‚îÄ sub_option.py\nHere‚Äôs its content:\n# __init__.py\nfrom .mod1 import predict\nThis file is executed when we ‚Äúinvoke‚Äù pkg, and therefore it makes the predict function available at the level of the folder where __init__.py is located‚Äîhere pkg. It is as if the function were directly located in pkg/.\nIn Python, we can now do:\ntest: python3\n\n&gt;&gt;&gt; import pkg\n&gt;&gt;&gt; pkg.predict(4)\n9\nor\ntest: python3\n\n&gt;&gt;&gt; from pkg import predict\n&gt;&gt;&gt; predict (5)\n10\nWe can also place an __init__.py file in the sub_pkg folder to make the soption function available ‚Äúdirectly in this folder‚Äù.\ntest: tree pkg\npkg\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ mod1.py\n‚îú‚îÄ‚îÄ mod2.py\n‚îî‚îÄ‚îÄ sub_pkg\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îî‚îÄ‚îÄ sub_option.py\nwith:\n\nfrom pkg.sub_pkg.sub_option import soption\n\nWe can now run:\ntest: python3\n\n&gt;&gt;&gt; from pkg.sub_pkg import soption\n&gt;&gt;&gt; soption (4)\n4\nThe dir() function, when used without arguments, shows what is in the ‚Äúcurrent local symbol table‚Äù (check here for more details).\ntest: python3\n\n&gt;&gt;&gt; dir()\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__']\n\n&gt;&gt;&gt; import pkg\n\n&gt;&gt;&gt; dir()\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'pkg']\n\n&gt;&gt;&gt; pkg.predict(2)\n7\n\n&gt;&gt;&gt; from pkg import *\n\n&gt;&gt;&gt; dir()\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'mod1', 'mod2', 'pkg', 'predict', 'soption', 'sub_pkg']\n\n&gt;&gt;&gt; predict (3)\n8\n\n&gt;&gt;&gt; soption (2)\n2\nFor example, the code above shows that after doing from pkg import *, we can directly use predict or soption in Python.\n\n\n\n\n\n\nNote\n\n\n\nInstead of from pkg.mod2 import add_plus_two in mod1.py, we could have written from .mod2 import add_plus_two. The first way is an absolute import, the second is a relative import. The . in a relative import refers to the current folder. Using .. refers to the parent folder.\nAbsolute imports are more explicit.\n\n\nTo illustrate this, let‚Äôs create a new file zoption.py that contains a function one_more in the sub_pkg folder, and modify the function soption in the sub_option.py file so that it uses both the one_more function and the add_plus_two function, using relative imports.\ntest: tree pkg \npkg\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ mod1.py\n‚îú‚îÄ‚îÄ mod2.py\n‚îî‚îÄ‚îÄ sub_pkg\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ sub_option.py\n    ‚îî‚îÄ‚îÄ zoption.py\nwith:\n\n# zoption.py\ndef one_more (x):\n    return (x + 1)\n\nand the modified sub_option.py file:\n\n# sub_option.py\nfrom .zoption import one_more\nfrom ..mod2 import add_plus_two # √©quivalent : from pkg.mod2 import add_plus_two\n\ndef soption (x):\n    return (one_more (x) * 2 - add_plus_two (x))\n\nAnd let‚Äôs add one line in the ./pkg/__init__.py file so that the soption function can be imported directly from pkg:\n\n# ./pkg/__init__.py\nfrom .mod1 import predict\nfrom pkg.sub_pkg.sub_option import soption\n\nWe can now run:\ntest: python3\n\n&gt;&gt;&gt; from pkg import soption\n&gt;&gt;&gt; soption (4)\n4"
  },
  {
    "objectID": "package_python_tuto_annexe.html#adding-data",
    "href": "package_python_tuto_annexe.html#adding-data",
    "title": "Python packaging basics",
    "section": "Adding data",
    "text": "Adding data\nSuppose now that the predict function in mod1.py requires data, for example a DataFrame. We will therefore add a data folder to store this DataFrame.\ntest: tree \npkg\n‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ df.csv\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ mod1.py\n‚îú‚îÄ‚îÄ mod2.py\n‚îî‚îÄ‚îÄ sub_pkg\n    ‚îú‚îÄ‚îÄ __init__.py\n    ‚îú‚îÄ‚îÄ sub_option.py\n    ‚îî‚îÄ‚îÄ zoption.py\nTo use this DataFrame, we cannot simply do:\n\n# mod1.py\nfrom pkg.mod2 import add_plus_two\n\nimport pandas as pd\n\ny = pd.read_csv (\"pkg/data/df.csv\")\n\ndef predict (x):\n    return (add_plus_two (x) + 3 + y['x'][0])\n\nThis will work as long as we import pkg from the test folder, but once the package is installed, it will not work if we try to use the package from another folder. Suppose we installed the package in an environment called testpkg and we try to use it from my home directory:\n(testpkg) mmip: python3\n\n&gt;&gt;&gt; from pkg import predict\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/mmip/test/pkg/__init__.py\", line 2, in &lt;module&gt;\n    from .mod1 import predict\n  File \"/home/mmip/test/pkg/mod1.py\", line 6, in &lt;module&gt;\n    y = pd.read_csv (\"pkg/data/df.csv\")\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mmip/.pyenv/versions/testpkg/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mmip/.pyenv/versions/testpkg/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mmip/.pyenv/versions/testpkg/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mmip/.pyenv/versions/testpkg/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/mmip/.pyenv/versions/testpkg/lib/python3.12/site-packages/pandas/io/common.py\", line 873, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'pkg/data/df.csv'\nTo use data inside a package, we must use importlib.resources:\n\n# mod1.py\nfrom pkg.mod2 import add_plus_two\n\nimport pandas as pd\n\nfrom importlib.resources import files, as_file\n\nresource = files (\"pkg\").joinpath(\"data\").joinpath(\"df.csv\")\nwith as_file (resource) as path:\n    y = pd.read_csv (path)\n\ndef predict (x):\n    return (add_plus_two (x) + 3 + int(y['x'][0]))\n\n(testpkg) mmip: python3\n\n&gt;&gt;&gt; from pkg import predict\n&gt;&gt;&gt; predict (4)\n11"
  },
  {
    "objectID": "package_python_tuto_annexe.html#minimal-installation-of-the-package",
    "href": "package_python_tuto_annexe.html#minimal-installation-of-the-package",
    "title": "Python packaging basics",
    "section": "Minimal installation of the package",
    "text": "Minimal installation of the package\nTo be able to install the package locally, it is enough to add an empty pyproject.toml file next to the pkg folder, here in the test folder:\ntest: tree \n.\n‚îú‚îÄ‚îÄ pkg\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ df.csv\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mod1.py\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mod2.py\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sub_pkg\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ sub_option.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ zoption.py\n‚îî‚îÄ‚îÄ pyproject.toml\nBefore installing, we create a virtual environment:\ntest: pyenv virtualenv 3.12.3 testpkg\ntest: pyenv activate testpkg\n(testpkg) test: \nInstallation in development mode (so that modifications are taken into account immediately) with:\n(testpkg) test: pip install -e .\n\nObtaining file:///home/mmip/test\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nBuilding wheels for collected packages: pkg\n  Building editable for pkg (pyproject.toml) ... done\n  Created wheel for pkg: filename=pkg-0.1.0-0.editable-py3-none-any.whl size=1086 sha256=609fc84698d539b5585e9367508534d56cb7130364621a44b0976aa9dea60d6a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-p3s6hc5v/wheels/41/19/39/c8798bf013ec2255417f36ea21404d09ad946ffc60673d12db\nSuccessfully built pkg\nInstalling collected packages: pkg\nSuccessfully installed pkg-0.1.0\n\n[notice] A new release of pip is available: 24.0 -&gt; 25.2\n[notice] To update, run: python -m pip install --upgrade pip\nUsage from another folder:\n(testpkg) test: cd\n\n(testpkg) mmip: python3\n\n&gt;&gt;&gt; from pkg import predict\n&gt;&gt;&gt; predict (10)\n17"
  },
  {
    "objectID": "animint.html",
    "href": "animint.html",
    "title": "Animint 2",
    "section": "",
    "text": "We follow a simplified version of the tutorial given here: https://rcdata.nau.edu/genomic-ml/animint2-manual/Ch08-WorldBank-facets.html"
  },
  {
    "objectID": "animint.html#data-loading-and-formatting",
    "href": "animint.html#data-loading-and-formatting",
    "title": "Animint 2",
    "section": "Data loading and formatting",
    "text": "Data loading and formatting\n\ndata(WorldBank)\nWorldBank$Region &lt;- sub(\" (all income levels)\", \"\", WorldBank$region, fixed=TRUE)\n\n# Remove NAs\ndf &lt;- data.table(WorldBank)[!(is.na(life.expectancy) | is.na(fertility.rate))] \n\n# Display only the region south asia for ease of vizualization\ndf &lt;- df[df$region == \"South Asia\"]\nyears &lt;- unique(df[, .(year)])\n\nThe helper functions FACETS etc will be used to generate adequate facet grids.\n\nadd_facets &lt;- function(df, top, side){\n  data.frame(df,\n             top=factor(top, c(\"Fertility rate\", \"Years\")),\n             side=factor(side, c(\"Years\", \"Life expectancy\")))\n}\nfacet_right &lt;- function(df) add_facets(df, \"Years\", \"Life expectancy\")\nfacet_left  &lt;- function(df) add_facets(df, \"Fertility rate\", \"Life expectancy\")"
  },
  {
    "objectID": "animint.html#first-animint-plot-show-life-expectancy-for-each-country-over-time",
    "href": "animint.html#first-animint-plot-show-life-expectancy-for-each-country-over-time",
    "title": "Animint 2",
    "section": "First animint plot: show life expectancy for each country over time",
    "text": "First animint plot: show life expectancy for each country over time\n\nlife_expect_plot &lt;- ggplot()+\n                          geom_line(aes(year, life.expectancy,\n                                        group = country, colour = country),\n                                        clickSelects = \"country\", data = facet_right(df),\n                                        size = 4, alpha = 3/5)\nlife_expect_plot\n\n\n\n\n\n\n\n\nWith animint, the plot is shown in the ‚ÄúViewer‚Äù panel and one can select which countries should be left in the plot.\n\nanimint(life_expect_plot)"
  },
  {
    "objectID": "animint.html#adding-a-bar-to-highlights-a-year",
    "href": "animint.html#adding-a-bar-to-highlights-a-year",
    "title": "Animint 2",
    "section": "Adding a bar to highlights a year",
    "text": "Adding a bar to highlights a year\nThe important parameter here is ‚ÄòclickSelects‚Äô.\n\n# facet_right(years) contains the unique years of the df and 2 columns for facet used later in the tutorial\nplot_with_bar &lt;- life_expect_plot +\n                 geom_tallrect(aes(xmin = year-1/2, xmax = year+1/2),\n                               clickSelects=\"year\", data = facet_right(years),\n                               alpha=1/2, show.legend = T)\nanimint(plot_with_bar)"
  },
  {
    "objectID": "animint.html#modify-the-vertical-cursor-text-display-to-show-the-average-life-expectancy",
    "href": "animint.html#modify-the-vertical-cursor-text-display-to-show-the-average-life-expectancy",
    "title": "Animint 2",
    "section": "Modify the vertical cursor text display to show the average life expectancy",
    "text": "Modify the vertical cursor text display to show the average life expectancy\nBy default, the text displayed at the current state of the bar is the current value of the bar. Use the ‚Äòtoolkit‚Äô argument in aes() to change what is displayed.\n\nyears_df &lt;- df %&gt;% group_by(year) %&gt;% dplyr::summarise(mean_life_expectancy = mean(life.expectancy))\n\nplot_with_bar &lt;- life_expect_plot +\n                        geom_tallrect(aes(xmin=year-1/2, xmax=year+1/2,\n                                          tooltip = paste0(\"Mean life expectancy: \",\n                                                           round(mean_life_expectancy, 1))),\n                                          clickSelects=\"year\",\n                                      data = facet_right(years_df), alpha=1/2) \n\nanimint(plot_with_bar)"
  },
  {
    "objectID": "animint.html#add-points-to-get-exact-value-of-life-expectancy-for-each-country",
    "href": "animint.html#add-points-to-get-exact-value-of-life-expectancy-for-each-country",
    "title": "Animint 2",
    "section": "Add points to get exact value of life expectancy for each country",
    "text": "Add points to get exact value of life expectancy for each country\nLike the bar above, we use the ‚Äòtooltip‚Äô argument\n\nfull_plot &lt;- plot_with_bar +\n                    geom_point(aes(year, life.expectancy, color = country,\n                                   tooltip = paste0(country, \" - Life expectancy :\",\n                                                 round(life.expectancy, 1))),\n                                   showSelected = \"country\",\n                                   clickSelects = \"country\",\n                               size = 2,\n                               data = facet_right(df)) \nanimint(full_plot)"
  },
  {
    "objectID": "animint.html#let-us-add-a-second-facet",
    "href": "animint.html#let-us-add-a-second-facet",
    "title": "Animint 2",
    "section": "Let us add a second facet",
    "text": "Let us add a second facet\nIn this part, this is where the functions facet_right and facet_left are useful. These functions creates columns to order the axis. Like ggplot, the variable used for facet must be a factor.\n\n# First add the facet grid\nplot_right &lt;- full_plot + theme_bw() +\n                theme(panel.margin=grid::unit(0, \"lines\")) +\n                facet_grid(side ~ top, scales=\"free\") + xlab(\"\") + ylab(\"\") +\n                theme_animint(width=600)\n\nWhen adding the second plot, we use the ‚ÄòshowSelected‚Äô argument, the variable must correspond to the one used in the first plot with the argument ‚ÄòclickSelected‚Äô\n\n# Add the 2nd plot\nplot_both &lt;- plot_right +\n  geom_point(aes(fertility.rate, life.expectancy,\n                colour=country, size=population, key=country),\n            clickSelects=\"country\",\n            showSelected = \"year\",\n            data = facet_left(df)) +\n      scale_size_animint(pixel.range=c(2, 20), breaks=10^(9:5))\n\n\nanimint(plot_both)"
  },
  {
    "objectID": "animint.html#generating-a-us-map",
    "href": "animint.html#generating-a-us-map",
    "title": "Animint 2",
    "section": "Generating a US map",
    "text": "Generating a US map\nWe first generate an animint map of the US on which each state can be selected\n\nUSpolygons &lt;- map_data(\"state\") # Animint function to create a map, \"state\" is for US states\n\nmap = ggplot() + \n      theme_animint(width=750, height=500) +\n      geom_polygon(aes(x=long, y=lat, group=group, tooltip = region),\n                   clickSelects=\"region\",\n                   data=USpolygons, \n                   fill=\"#000000\",\n                   colour=\"white\", \n                   size=0.5, \n                  alpha=1)\nanimint(map)"
  },
  {
    "objectID": "animint.html#adding-the-tornadoes-segments",
    "href": "animint.html#adding-the-tornadoes-segments",
    "title": "Animint 2",
    "section": "Adding the tornadoes segments",
    "text": "Adding the tornadoes segments\nLet us add to the maps segments that give the tornadoes trajectory (segment from start point to end point). We also use the parameter ‚ÄòshowSelected‚Äô so the the map displays the tornado only for a specific year.\n\nseg.color &lt;- \"#55B1F7\"\ntornadoes_map &lt;- map + \n                 geom_segment(aes(x=startLong, y=startLat,\n                                 xend=endLong, yend=endLat),\n                              colour=seg.color, size = 1.2,\n                              showSelected=\"year\",\n                              data=UStornadoes) \nanimint(tornadoes_map)"
  },
  {
    "objectID": "animint.html#adding-the-tornadoes-points",
    "href": "animint.html#adding-the-tornadoes-points",
    "title": "Animint 2",
    "section": "Adding the tornadoes points",
    "text": "Adding the tornadoes points\nSuperposing tornadoes end point to tornadoes segments.\n\npt.color &lt;- \"#9999F9\"\ntornadoes_map &lt;- tornadoes_map +\n                  geom_point(aes(endLong, endLat),               \n                             colour=pt.color, \n                             showSelected=\"year\",\n                             data=UStornadoes,\n                             size=1)\nanimint(tornadoes_map)"
  },
  {
    "objectID": "animint.html#creating-histograms-with-the-number-of-tornadoes-per-state-year",
    "href": "animint.html#creating-histograms-with-the-number-of-tornadoes-per-state-year",
    "title": "Animint 2",
    "section": "Creating histograms with the number of tornadoes per (state, year)",
    "text": "Creating histograms with the number of tornadoes per (state, year)\n\nUStornadoCounts &lt;- ddply(UStornadoes, .(region, year), summarize, count=length(region))\n\nThe tornadoes histogram per (state, year). We use the parameter ‚ÄòshowSelected‚Äô = region so that it can be linked to the map later. Similarly for the argument ‚ÄòclickSelects‚Äô = year, it corresponds to ‚ÄòshowSelected‚Äô in the map.\n\ntornadoes_hist &lt;- ggplot() + \n                  xlab(\"year\") +\n                  ylab(\"Number of tornadoes\") +\n                  geom_bar(aes(year, count),\n                          clickSelects=\"year\", \n                          showSelected=\"region\",\n                          data=UStornadoCounts, \n                          stat=\"identity\", \n                          color = \"black\",\n                          fill = \"#22212100\",\n                          alpha = 1,\n                          position=\"identity\")\nanimint(tornadoes_hist)\n\n\n\n\n\n\nAdding value on top of the bar when it is clicked.\n\ntornadoes_hist &lt;- tornadoes_hist +\n                   geom_text(aes(year, count + 5, label=count),\n                             showSelected=c(\"region\", \"year\"),\n                             data=UStornadoCounts, size=20)\nanimint(tornadoes_hist)"
  },
  {
    "objectID": "animint.html#combining-the-map-and-histogram",
    "href": "animint.html#combining-the-map-and-histogram",
    "title": "Animint 2",
    "section": "Combining the map and histogram",
    "text": "Combining the map and histogram\n\nanimint(tornadoes_map, tornadoes_hist)"
  },
  {
    "objectID": "animint.html#get-the-data-to-plot",
    "href": "animint.html#get-the-data-to-plot",
    "title": "Animint 2",
    "section": "Get the data to plot",
    "text": "Get the data to plot\nFirst, let us get the data and usefull functions\n\nfrance_temperature=rast(\"data/EOBS_FR31.nc\")\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:data.table':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndates = seq.Date(ymd(\"1985-01-01\"),ymd(\"2015-12-31\"),by='day')\ndates = dates[\n  -which((month(dates)==2 & day(dates)==29))]\nfrance_temperature = france_temperature[[1:length(dates)]]\n\ndatessummers = dates[(month(dates)%in%6:8) & ( year(dates) %in% c(2003,2015,1990,1999)) ]\nsummers =subset(france_temperature, month(time(france_temperature)) %in% c(6,7,8))\nsummers =subset(summers, year(time(summers)) %in% c(2003,2015,1990,1999))\n\nnames(summers)=datessummers\n\n## function to get quantile map ----\n\nget_qmap=function(raster,li_q){\n  n=length(li_q)\n  s = c()\n  for(i in 1:n){\n    q=li_q[i]\n    s &lt;-c(s,app(raster, fun=function(i){quantile(i,q,na.rm=T)}))\n    \n  }\n  s=rast(s)\n  names(s) = li_q\n  return(s)\n}\n## get quantile maps from observation 1985-2015-------\nqmaps = (get_qmap(summers,c(0.9,0.95,0.98,0.99)))\nplot(qmaps)\n\n\n\n\n\n\n\n\nThe different maps are quantiles of temperature for each pixels.\n\n## function to get percentage of area over quantiles ----\nget_area_over_qmaps=function(raster,qmaps){\n  #returns a time dataframe of % of area over the maps in qmaps\n  # n+1 columns with n the number of layers in qmaps, named after quantiles.\n  \n  ngrid = global((not.na(raster)),sum)\n  \n  depsummer =data.frame(t=time(raster))\n  for(i in 1:nlyr(qmaps)){\n    qmap = qmaps[[i]]\n    summersq = raster&gt;qmap\n    depsummer[,i+1]= global(summersq,sum,na.rm=T)/ngrid\n  }\n  names(depsummer)=c('t',names(qmaps))\n  return(depsummer)\n}\n\ndata=get_area_over_qmaps(summers,qmaps)\ndepsummer=data\nlongdep=gather(depsummer,quantile,percentage,-t)\nlongdep$year =year(longdep$t)\nlongdep$day_of_year=yday(longdep$t)\nhead(depsummer)\n\n           t 0.9 0.95 0.98 0.99\n1 1990-06-01   0    0    0    0\n2 1990-06-02   0    0    0    0\n3 1990-06-03   0    0    0    0\n4 1990-06-04   0    0    0    0\n5 1990-06-05   0    0    0    0\n6 1990-06-06   0    0    0    0\n\nhead(longdep)\n\n           t quantile percentage year day_of_year\n1 1990-06-01      0.9          0 1990         152\n2 1990-06-02      0.9          0 1990         153\n3 1990-06-03      0.9          0 1990         154\n4 1990-06-04      0.9          0 1990         155\n5 1990-06-05      0.9          0 1990         156\n6 1990-06-06      0.9          0 1990         157\n\n\nlongdep contains for each date t, each quantile, each year and day_of_year the percentage of location over their quantile."
  },
  {
    "objectID": "animint.html#objective-number-1-getting-percentage-fyear-with-colorquantile-selecting-only-chosen-year-and-highlighting-only-chosen-quantile.",
    "href": "animint.html#objective-number-1-getting-percentage-fyear-with-colorquantile-selecting-only-chosen-year-and-highlighting-only-chosen-quantile.",
    "title": "Animint 2",
    "section": "Objective number 1 : getting percentage = f(year) with color=quantile , selecting only chosen year and highlighting only chosen quantile.",
    "text": "Objective number 1 : getting percentage = f(year) with color=quantile , selecting only chosen year and highlighting only chosen quantile.\n\nlibrary(animint2)\n\ngobs &lt;- animint2::ggplot() +\n  animint2::geom_line(\n   animint2::aes(x=day_of_year, y= percentage,group = interaction(quantile,year),color=quantile), data = longdep\n  ) +\n  animint2::labs(title = \"some summers - proportion of FR map exceeding given quantile - observations\")\n\ngobs\n\n\n\n\n\n\n\n\nThis is not very pretty ! There is too much info at once. We want to only plot a given year, selected using showSelected=c( ‚Äúyear‚Äù). And also, why not choosing the quantiles to plot ?\n\nplot_QER &lt;- animint2::ggplot()+\n\n animint2::geom_line(animint2::aes(\n    day_of_year,percentage,group=quantile,color=quantile),\n    clickSelects=c(\"year\",\"quantile\"), #use selection to choose the year.\n     showSelected=c( \"year\"), #only show the selected year\n    data=(longdep), size=4, alpha=3/5)+\n  theme_bw()+\n  xlab(\"Day of year\")+\n  ylab(\"Percentage\")\n\nanimint(plot_QER)\n\n\n\n\n\n\nWe can add a rectangle, higlighting a selected day (will be useful later)\n\nplot_QER &lt;- animint2::ggplot()+\n  \n  # highlight the chosen year again\n  geom_tallrect(animint2::aes(\n    xmin=day_of_year-1/2, xmax=day_of_year+1/2),\n    clickSelects=\"day_of_year\",\n    data=(longdep), alpha=1/2)+\n  \n animint2::geom_line(animint2::aes(\n    day_of_year,percentage,group=quantile,color=quantile),\n    clickSelects=c(\"year\",\"quantile\"), #use selection to choose the year.\n     showSelected=c( \"year\"), #only show the selected year\n    data=(longdep), size=4, alpha=3/5)+\n  theme_bw()+\n  xlab(\"Day of year\")+\n  ylab(\"Percentage\")\n\nanimint(plot_QER)"
  },
  {
    "objectID": "animint.html#objective-2-plot-the-temperature-for-chosen-day-t-of-a-given-year",
    "href": "animint.html#objective-2-plot-the-temperature-for-chosen-day-t-of-a-given-year",
    "title": "Animint 2",
    "section": "Objective 2 : plot the temperature for chosen day t of a given year",
    "text": "Objective 2 : plot the temperature for chosen day t of a given year\n\n# --- palette and limits ---\npal &lt;- c(\"black\",\"#67001f\",'#d73027','#f46d43','#fdae61','#fee090',\n         '#ffffbf','#e0f3f8','#abd9e9','#74add1','#4575b4','#313695')\nmm &lt;- c(0,36)\n# be careful, this is heavy data\nsummers_small &lt;- terra::aggregate(summers, fact = 1, fun = mean)\n\nWarning: [aggregate] all values in argument 'fact' are 1, nothing to do\n\n# --- 1. Raster data prepared for ggplot/animint2 ---\n# Extract one value per cell per date -&gt; convert to data.frame\n# summers is a SpatRaster with layers named as dates\nmap_df &lt;- as.data.frame(summers_small, xy = TRUE, na.rm = FALSE)\nmap_df &lt;- tidyr::pivot_longer(map_df, -c(x,y),\n                              names_to = \"t\",\n                              values_to = \"temperature\")\nmap_df$t &lt;- as.Date(map_df$t)\nmap_df$year &lt;- lubridate::year(map_df$t)\nmap_df$day_of_year &lt;- lubridate::yday(map_df$t)\n\n# --- 2. Map plot (left) ---\nmap_plot &lt;- animint2::ggplot() +\n  animint2::geom_tile(animint2::aes(x, y, fill = temperature),\n            data = map_df,\n            showSelected = c(\"year\",\"day_of_year\"),colour=NA) +\n  animint2::ggtitle(\"Date t (selected)\")+\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                       na.value = \"transparent\",\n                       limits = mm) +\n  animint2::coord_equal() +\n  animint2::labs(fill = \"temperature\")+theme_bw()\nanimint(map_plot)"
  },
  {
    "objectID": "animint.html#objective-3-plot-together-the-map-of-temperature-at-a-day-highlighting-that-day-on-my-first-plot",
    "href": "animint.html#objective-3-plot-together-the-map-of-temperature-at-a-day-highlighting-that-day-on-my-first-plot",
    "title": "Animint 2",
    "section": "Objective 3 : plot together the map of temperature at a day, highlighting that day on my first plot",
    "text": "Objective 3 : plot together the map of temperature at a day, highlighting that day on my first plot\nCan take a while, because the dataframe for plotting is large. We have used shared variables names, so selecting from (year,day_of_year) has the same meaning in both graphs.\n\nanimint2::animint(plot_QER,map_plot, first = list(year = c(2003), day_of_year = c(200)))"
  },
  {
    "objectID": "animint.html#objective-4-show-maps-at-different-time-steps-t-1-t-and-t1-to-show-temporal-evolution",
    "href": "animint.html#objective-4-show-maps-at-different-time-steps-t-1-t-and-t1-to-show-temporal-evolution",
    "title": "Animint 2",
    "section": "Objective 4 : Show maps at different time steps : t-1, t and t+1, to show temporal evolution",
    "text": "Objective 4 : Show maps at different time steps : t-1, t and t+1, to show temporal evolution\n\n# Map t-1\nmap_t_minus1 &lt;- animint2::ggplot() +\n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = map_df %&gt;% mutate(day_of_year = day_of_year + 1),\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 0.4,\n    colour = NA\n  ) +\n  ggtitle(\"t-1\") +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw()\n\n# Map t\nmap_t &lt;- animint2::ggplot() +\n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = map_df,\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 1,\n    colour = NA\n  ) +\n  ggtitle(\"t\") +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw()\n\n# Map t+1\nmap_t_plus1 &lt;- animint2::ggplot() +\n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = map_df %&gt;% mutate(day_of_year = day_of_year - 1),\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 0.4,\n    colour = NA\n  ) +\n  ggtitle(\"t+1\") +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw()\n\nanimint(map_t_minus1,map_t,map_t_plus1)"
  },
  {
    "objectID": "animint.html#final-objective",
    "href": "animint.html#final-objective",
    "title": "Animint 2",
    "section": "Final objective",
    "text": "Final objective\nPut everything togeteher I wanted a specific layout but nothing worked. It does not give an error, but does not work anyway‚Ä¶\n\nanimint2::animint(map_t_minus1,map_t,map_t_plus1,plot_QER, first = list(year = c(2003), day_of_year = c(200)),arrange = list(\n    top_row = c(\"map_t_minus1\", \"map_t\", \"map_t_plus1\"),  # top row\n    bottom_row   = \"plot_QER\"                               # bottom row\n  ))"
  },
  {
    "objectID": "animint.html#final-final-objective-add-facets-to-make-it-pretty",
    "href": "animint.html#final-final-objective-add-facets-to-make-it-pretty",
    "title": "Animint 2",
    "section": "Final Final Objective : add facets to make it pretty",
    "text": "Final Final Objective : add facets to make it pretty\nMany thanks to Blanche who did not let me leave the room with an ugly plot.\nHelper functions add_facets add a column to a dataframe, allowing us to make a facet name. We choose to only put a top name for the date.\n\nadd_facets &lt;- function(df, top){\n  data.frame(df,\n             top=factor(top, c(\"t-1\", \"t\", \"t+1\")))\n}\nfacet_left  &lt;- function(df) add_facets(df, \"t-1\")\nfacet_middle &lt;- function(df) add_facets(df, \"t\")\nfacet_right  &lt;- function(df) add_facets(df, \"t+1\")\n\n\n# Map t-1\nmap_t &lt;- animint2::ggplot() +\n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = facet_left(map_df %&gt;% mutate(day_of_year = day_of_year + 1)),\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 0.4,\n    colour = NA\n  ) +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw() +\n  \n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = facet_middle(map_df),\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 1,\n    colour = NA\n  ) +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw() +\n  animint2::geom_tile(\n    animint2::aes(x, y, fill = temperature),\n    data = facet_right(map_df %&gt;% mutate(day_of_year = day_of_year - 1)),\n    showSelected = c(\"year\", \"day_of_year\"),\n    alpha = 0.4,\n    colour = NA\n  ) +\n  animint2::scale_fill_gradientn(colors = rev(pal),\n                                na.value = \"transparent\",\n                                limits = mm) +\n  animint2::coord_equal() +\n  theme_bw()+\n  \n  facet_wrap(~top)\n\nScale for 'fill' is already present. Adding another scale for 'fill', which\nwill replace the existing scale.\nScale for 'fill' is already present. Adding another scale for 'fill', which\nwill replace the existing scale.\n\nmap_t\n\n\n\n\n\n\n\nanimint(map_t,plot_QER, first = list(year = c(2003), day_of_year = c(200)))"
  },
  {
    "objectID": "animint.html#option-1-in-rmd-and-qmd",
    "href": "animint.html#option-1-in-rmd-and-qmd",
    "title": "Animint 2",
    "section": "Option 1: in rmd and qmd",
    "text": "Option 1: in rmd and qmd\nOutput directory auto-generated, be careful use only one animint per named code chunk."
  },
  {
    "objectID": "animint.html#option-2-html",
    "href": "animint.html#option-2-html",
    "title": "Animint 2",
    "section": "Option 2: html",
    "text": "Option 2: html\n\nanimint2dir(animint(plot_both),\n            out.dir = \"myplot\", \n            open.browser = FALSE)\n\nThe plot can be viewed with index.html. If the web page is blank, configure your browser to allow execution of local JavaScript code, as explained on the FAQ."
  },
  {
    "objectID": "package_python.html",
    "href": "package_python.html",
    "title": "Python packaging",
    "section": "",
    "text": "The goal of this workshop is to learn how to build a Python package. It is divided into three parts. In the first part, we create a toy package and install it locally. In the second part, we publish this package on TestPyPI (which allows us to verify that the package is correct before publishing it on PyPI, Python‚Äôs equivalent of CRAN for R). In the third part, we integrate CI/CD (continuous integration / continuous deployment) via GitHub to ensure package maintenance and automatic deployment to PyPI when releasing a new version. The associated code is available here: https://github.com/armandfavrot/ammonia_predict_3.\nThe resources we used come mainly from Real Python (very pedagogical):\nTo a lesser extent, we also used the tutorials Python packaging - Jean-Benoist Leger (CI/CD with GitLab) and Distribuer son application Python - Lo√Øc Gouarin.\nA short note from RealPython about the Python packaging ecosystem, often criticized by R coders who are too proud of their CRAN:\n‚ÄúOver the last decade, many initiatives have improved the packaging landscape, bringing it from the Wild West and into a fairly modern and capable system. This is mainly done through Python Enhancement Proposals (PEPs) that are reviewed and implemented by the Python Packaging Authority (PyPA) working group.‚Äù\nThis highlights the fact that there are rules. For example, PEP 508 describes how dependencies should be specified."
  },
  {
    "objectID": "package_python.html#creating-a-virtual-environment",
    "href": "package_python.html#creating-a-virtual-environment",
    "title": "Python packaging",
    "section": "Creating a virtual environment",
    "text": "Creating a virtual environment\nThe first step is to create a virtual environment where the package will be developed. We used pyenv virtualenv to create the environment. Unlike venv, which only manages package versions, pyenv virtualenv also allows you to manage the Python version. For more information on environments, see this tutorial: Python Virtual Environments: A Primer.\nThe package was developed with python 3.12.3, torch 2.5.0, and pandas 2.2.3.\nInstall Python 3.12.3 with:\npyenv install 3.12.3\nCreate a virtual environment with:\npyenv virtualenv 3.12.3 ammonia_predict_3\nWhere ‚Äòammonia_predict_3‚Äô is the name of the environment.\nCreate a directory where the package will be developed, move inside it, and activate the environment.\nmmip: mkdir ammonia_predict_3\nmmip: cd ammonia_predict_3/\nammonia_predict_3: pyenv local ammonia_predict_3\n(ammonia_predict_3) ammonia_predict_3:\nInstall the packages:\npip install torch==2.5.0\npip install pandas==2.2.3\nCheck the versions:\n(ammonia_predict_3) ammonia_predict_3: python --version\nPython 3.12.3\n(ammonia_predict_3) ammonia_predict_3: pip list | egrep 'torch|pandas'\npandas                   2.2.3\ntorch                    2.5.0"
  },
  {
    "objectID": "package_python.html#package-creation",
    "href": "package_python.html#package-creation",
    "title": "Python packaging",
    "section": "Package creation",
    "text": "Package creation\nThe second step is to create the package. But first, what is a package in Python? Since we couldn‚Äôt find an official definition, we propose to see a package as a collection of modules, each of which is a file containing function definitions, classes, variables, and executable statements. Modules allow for modular programming, which ‚Äúrefers to the process of breaking a large, unwieldy programming task into separate, smaller, more manageable subtasks or modules‚Äù (Real Python). In other words, a package is essentially a collection of code divided into multiple files through a well-organized directory structure, which makes its use and maintenance easier.\nThe toy package we created predicts the dynamics of ammonia emissions under environmental conditions using a recurrent neural network.\nOur package consists of the following files:\n(ammonia_predict_3) ammonia_predict_3: tree\n.\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ammonia_predict_3\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ api.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ model_def.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ utils.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ data\n‚îÇ¬†¬†     ¬†¬† ‚îî‚îÄ‚îÄ final_model.pth\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ test_predict.py\nLet‚Äôs go through these files:\n\npyproject.toml: a configuration file that tells Python tools how to build, install, and manage your package. This file typically contains three sections: [build-system], [project], and [tool].\n\n[build-system] tells Python which tool should be used to build the package (e.g., setuptools, poetry, ‚Ä¶).\n\n[project] is used to specify the project‚Äôs basic metadata, such as dependencies, author information, etc.\n\n[tool] is used to configure the tools used in the project, such as setuptools, pytest, ruff, and so on.\n\nFor more information on how to write this file, check the documentation. Configuration of the setuptools tool is explained here.\n\n\n\npyproject.toml\n[build-system]\nrequires = [\"setuptools&gt;=61\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ammonia-predict-3\"\nversion = \"0.1.0\"\ndescription = \"Predict ammonia emissions with a simple RNN model\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12.3\"\nauthors = [\n  { name = \"Armand Favrot\", email = \"armand.favrot@inrae.fr\" }\n]\nclassifiers = [\n  \"License :: OSI Approved :: MIT License\",\n  \"Development Status :: 3 - Alpha\",\n  \"Programming Language :: Python\",\n]\ndependencies = [\n  \"pandas&gt;=2.2.3\",\n  \"torch&gt;=2.5.0\"\n]\n\n[tool.setuptools]\npackage-dir = { \"\" = \"src\" }\ninclude-package-data = true\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.setuptools.package-data]\nammonia_predict_3 = [\"data/*.pth\"]\n\n[project.optional-dependencies]\nbuild = [\"build\", \"twine\"]\ndev = [\"pytest\"]\n\n\n\nREADME.md: documentation for the package.\n\n\n\nREADME.md\n# ammonia-predict\n\nPredict ammonia emissions with a simple RNN model.\n\n## Install (dev)\n\npip install -e .\n\n## Usage\n\nYou can use the package in Python as follows:\n\nimport pandas as pd\nfrom ammonia_predict_3 import predict\n\ndf = pd.DataFrame({\n    \"pmid\": [1, 1],\n    \"ct\": [2, 4],\n    \"dt\": [2, 2],\n    \"air_temp\": [12, 15],\n    \"wind_2m\": [3, 3],\n    \"rain_rate\": [0, 0],\n    \"tan_app\": [36.7, 36.7],\n    \"app_rate\": [10, 10],\n    \"man_dm\": [0.1, 0.1],\n    \"man_ph\": [7, 7],\n    \"t_incorp\": [0, 0],\n    \"app_mthd\": [1, 1],\n    \"incorp\": [0, 0],\n    \"man_source\": [1, 1],\n})\n\npred = predict(df)\nprint(pred[[\"prediction_delta_ecum\", \"prediction_ecum\"]])\n\n\n## Notes\n\n- The trained weights are included in the package under `ammonia_predict/data/final_model.pth`.\n- The package requires **Python ‚â•3.9**, **PyTorch**, and **pandas**.\n\n\n\n__init__.py: a script that is executed when the package or one of its modules is imported. It is used, among other things, to control what the package exposes (for example, whether specific functions can be imported directly). More information on this file can be found here.\n\n\n\ninit.py\nfrom .api import predict\n\n__all__ = [\"predict\"]\n\n\nfrom importlib.metadata import version\n\n__version__ = version(\"ammonia-predict-3\")  \n\n\n\nThe other files are the package‚Äôs source code. api.py contains the function available to the package user, model_def.py defines the model class, and utils.py contains helper functions. final_model.pth is a data file that stores the model parameters.\n\n\n\napi.py\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom importlib.resources import files, as_file\n\nfrom .model_def import AmmoniaRNN\nfrom .utils import generate_tensors_predictors\n\nDEVICE = \"cpu\"\nnum_layers = 1\nnonlinearity = \"relu\"\nbidirectional = True\nhidden_size = 512 \n\ncat_dims = [5, 3, 2]  \nembedding_dims = [10, 9, 8]  \ninput_size = 13   \noutput_size = 1\n\nmodel = AmmoniaRNN(input_size = input_size, \n                   output_size = output_size, \n                   hidden_size = hidden_size, \n                   nonlinearity = nonlinearity,\n                   num_layers = num_layers,\n                   bidirectional = bidirectional,\n                   cat_dims = cat_dims, \n                   embedding_dims = embedding_dims).to(DEVICE)\n\n\nresource = files(__package__) / \"data\" / \"final_model.pth\"\nwith as_file(resource) as path:\n    model.load_state_dict(torch.load(str(path), weights_only = True, map_location=torch.device('cpu')))\n\n\ndef predict (df):\n\n    data_predictions = df.copy()\n\n    pmids = data_predictions['pmid'].unique()\n    \n    data_predictions['prediction_ecum'] = None\n    data_predictions['prediction_delta_ecum'] = None\n        \n    with torch.no_grad():\n    \n        all_predictions = torch.empty(0).to(DEVICE)\n    \n            \n        for i in pmids:\n    \n            x = generate_tensors_predictors (data_predictions, i, device = DEVICE)\n            y = model(x)\n            all_predictions = torch.cat ((all_predictions, y.squeeze()), 0)\n    \n        data_predictions['prediction_delta_ecum'] = all_predictions.to(\"cpu\").detach()\n    \n    data_predictions['prediction_ecum'] = data_predictions.groupby('pmid')['prediction_delta_ecum'].cumsum()\n\n    return data_predictions\n\n\n\n\nmodel_def.py\nimport torch\nimport torch.nn as nn\n\nclass AmmoniaRNN(nn.Module):\n    \n    def __init__(self, \n                 input_size, \n                 output_size, \n                 hidden_size, \n                 num_layers,\n                 nonlinearity, \n                 bidirectional,\n                 cat_dims = None, embedding_dims = None):\n        \n        super(AmmoniaRNN, self).__init__()\n        \n        D = 1 + 1 * bidirectional\n               \n        self.embeddings = nn.ModuleList([\n            nn.Embedding(num_embeddings = cat_dim, embedding_dim = embed_dim)\n            for cat_dim, embed_dim in zip(cat_dims, embedding_dims)\n        ])\n        \n        input_size = input_size - len(cat_dims) + sum(embedding_dims)           \n        \n        self.rnn = nn.RNN(input_size, \n                          hidden_size, \n                          num_layers = num_layers,\n                          nonlinearity = nonlinearity, \n                          bidirectional = bidirectional)    \n        \n        self.fc1 = nn.Linear(hidden_size * D, 6)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(6, output_size)\n\n    def forward(self, x):\n\n        x_continuous = x[0]\n        x_categoricals = x[1]\n        \n        x_embeds = [embed(x_cat) for embed, x_cat in zip(self.embeddings, x_categoricals)]\n        \n        x = torch.cat([x_continuous] + x_embeds, dim = -1)\n        \n        h, _ = self.rnn(x)\n\n        out = self.fc1(h)\n        out = self.relu(out)\n        out = self.fc2(out)\n       \n        return out\n\n\n\n\nutils.py\nimport torch\n\ndef generate_tensors_predictors(df, pmid, device):\n    \n    data_filtered = df[df['pmid'] == pmid]\n\n    x_cont = data_filtered[['ct', 'dt', 'air_temp', 'wind_2m', 'rain_rate', 'tan_app', 'app_rate', 'man_dm', 'man_ph', 't_incorp']]\n\n    x_cont_tensor = torch.tensor(x_cont.values, dtype=torch.float32).view(len(x_cont), len(x_cont.columns))\n    x_cont_tensor = x_cont_tensor.to(device)\n    \n    x_cat = data_filtered[['app_mthd', 'incorp', 'man_source']]\n    \n    x_cat_tensor = torch.tensor(x_cat.values, dtype=torch.long).view(len(x_cat), len(x_cat.columns))\n    x_cat_tensor = x_cat_tensor.to(device)\n    x_cat_tensor = torch.unbind (x_cat_tensor, dim = 1)\n\n    output = [x_cont_tensor, x_cat_tensor]\n    \n    return output\n\n\n\ntest_predict.py contains the tests to be run, which can be executed directly with the pytest tool.\n\n\n\ntest_predict.py\nimport pandas as pd\nfrom ammonia_predict_3 import predict\n\ndef test_predict_columns():\n    df = pd.DataFrame({\n        \"pmid\": [1, 1, 2, 2],\n        \"ct\": [2, 4, 2, 4],\n        \"dt\": [2, 2, 2, 2],\n        \"air_temp\": [12, 15, 11, 10],\n        \"wind_2m\": [3, 3, 4, 2],\n        \"rain_rate\": [0, 0, 1, 0],\n        \"tan_app\": [36.7, 36.7, 36.7, 36.7],\n        \"app_rate\": [10, 10, 12, 12],\n        \"man_dm\": [0.1, 0.1, 0.1, 0.1],\n        \"man_ph\": [7, 7, 7, 7],\n        \"t_incorp\": [0, 0, 0, 0],\n        \"app_mthd\": [1, 1, 1, 1],\n        \"incorp\": [0, 0, 0, 0],\n        \"man_source\": [1, 1, 1, 1],\n    })\n\n    out = predict(df)\n    assert \"prediction_delta_ecum\" in out.columns\n    assert \"prediction_ecum\" in out.columns\n    assert len(out) == len(df)\n\n\nFor more details on package structuring, how imports work, and the role of __init__.py, we have prepared an additional tutorial available here."
  },
  {
    "objectID": "package_python.html#local-installation-of-the-package",
    "href": "package_python.html#local-installation-of-the-package",
    "title": "Python packaging",
    "section": "Local installation of the package",
    "text": "Local installation of the package\nAt this point, we can now install the package:\n(ammonia_predict_3) ammonia_predict_3: pip install .\nTo check the installation, go into another directory, activate the environment where the package was installed, start Python, import the package, and run the example provided in the README.md file.\n(ammonia_predict_3) ammonia_predict_3: cd\nmmip: pyenv activate ammonia_predict_3\n(ammonia_predict_3) mmip: python\nPython 3.12.3 (main, Sep  1 2025, 16:05:27) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from ammonia_predict_3 import predict\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"pmid\": [1, 1],\n...     \"ct\": [2, 4],\n...     \"dt\": [2, 2],\n...     \"air_temp\": [12, 15],\n...     \"wind_2m\": [3, 3],\n...     \"rain_rate\": [0, 0],\n...     \"tan_app\": [36.7, 36.7],\n...     \"app_rate\": [10, 10],\n...     \"man_dm\": [0.1, 0.1],\n...     \"man_ph\": [7, 7],\n...     \"t_incorp\": [0, 0],\n...     \"app_mthd\": [1, 1],\n...     \"incorp\": [0, 0],\n...     \"man_source\": [1, 1],\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; pred = predict(df)\n&gt;&gt;&gt; print(pred[[\"prediction_delta_ecum\", \"prediction_ecum\"]])\n   prediction_delta_ecum  prediction_ecum\n0               7.981954         7.981954\n1               4.829587        12.811542\n&gt;&gt;&gt;\n\n\n\n\n\n\nDevelopment mode installation\n\n\n\nInstalling in development mode means that modifications to the source code are taken into account immediately, without needing to reinstall the package each time you make changes. It can be done with:\npip install -e .\nSee Install Your Package Locally - Real Python for more details."
  },
  {
    "objectID": "package_python.html#tests-with-pytest",
    "href": "package_python.html#tests-with-pytest",
    "title": "Python packaging",
    "section": "Tests with pytest",
    "text": "Tests with pytest\nTo run the test that have been placed in ./tests/, we just need to run pytest in the terminal:\n(ammonia_predict_3) ammonia_predict_3: pytest\n================================================================== test session starts ==================================================================\nplatform linux -- Python 3.12.3, pytest-7.4.4, pluggy-1.4.0\nrootdir: /home/mmip/FinistR/ammonia_predict_3\nconfigfile: pytest.ini\ntestpaths: tests\nplugins: anyio-4.6.2.post1\ncollected 1 item\n\ntests/test_predict.py .                                                                                                                           [100%]\n\n=================================================================== warnings summary ====================================================================\n../../.local/lib/python3.12/site-packages/pandas/core/arrays/masked.py:60\n  /home/mmip/.local/lib/python3.12/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n    from pandas.core import (\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n============================================================= 1 passed, 1 warning in 1.76s ==============================================================\nMore details here: Effective Python Testing With pytest - Real Python."
  },
  {
    "objectID": "package_python.html#first-workflow-linting",
    "href": "package_python.html#first-workflow-linting",
    "title": "Python packaging",
    "section": "First workflow: linting",
    "text": "First workflow: linting\nWe add the following file in .github/workflows:\n\nname: Lint Python Code\n\non:\n  pull_request:\n    branches:\n      - main\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.13\"\n          cache: \"pip\"\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install ruff\n\n      - name: Run Ruff\n        run: ruff check --output-format=github\n\nThis action will be triggered on GitHub when pushing, making a pull request, or manually using a button in github (workflow_dispatch).\nActions are visible in the Actions tab of the GitHub repository:\n\nAt this stage, excluding the ‚Äúparasitic‚Äù files and folders created during local installation and usage of the package (such as __pycache__, *.egg-info, ‚Ä¶), the project tree looks like this:\n(ammonia_predict_3) ammonia_predict_3: tree -a -I '.git|build|dist|__pycache__|*.egg-info|.pytest_cache|.python-version'\n.\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .github\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ .workflows\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ lint.yml\n‚îÇ¬†¬† \n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ src\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ammonia_predict_3\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ api.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ model_def.py\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ utils.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ data\n‚îÇ¬†¬†     ¬†¬† ‚îî‚îÄ‚îÄ final_model.pth\n‚îî‚îÄ‚îÄ tests\n    ‚îî‚îÄ‚îÄ test_predict.py"
  },
  {
    "objectID": "package_python.html#second-workflow-testing",
    "href": "package_python.html#second-workflow-testing",
    "title": "Python packaging",
    "section": "Second workflow: testing",
    "text": "Second workflow: testing\nWe specified in the .toml file that the package works with python&gt;=3.12, pandas&gt;=2.2.3, and torch&gt;=2.5.0.\nHowever, we had not actually verified that it truly works.\nTesting all possible combinations would be too costly.\nInstead, we adopt a min-max strategy: for each Python version, we run pytest with (i) the minimal versions of pandas and torch, and (ii) the latest available versions. [Refs?]\nWe add the following file in .github/workflows:\n\n\ntest.yml\nname: Run Tests\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  workflow_call:\n  workflow_dispatch:\n\njobs:\n  testing:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        include:\n          - python-version: \"3.12\"\n            pandas-version: \"2.2.3\"\n            torch-version: \"2.5.0\"\n          - python-version: \"3.13\"\n            pandas-version: \"2.2.3\"\n            torch-version: \"2.5.0\"\n          - python-version: \"3.12\"\n            pandas-version: \"latest\"\n            torch-version: \"latest\"\n          - python-version: \"3.13\"\n            pandas-version: \"latest\"\n            torch-version: \"latest\"\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: \"pip\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n\n          if [ \"${{ matrix.pandas-version }}\" = \"latest\" ]; then\n            python -m pip install \"pandas&gt;=2.2.3\"\n          else\n            python -m pip install pandas==${{ matrix.pandas-version }}\n          fi\n\n          if [ \"${{ matrix.torch-version }}\" = \"latest\" ]; then\n            python -m pip install \"torch&gt;=2.5.0\"\n          else\n            python -m pip install torch==${{ matrix.torch-version }}\n          fi\n\n          python -m pip install .[dev]\n\n      - name: Run Pytest\n        run: pytest"
  },
  {
    "objectID": "package_python.html#third-workflow-deployment",
    "href": "package_python.html#third-workflow-deployment",
    "title": "Python packaging",
    "section": "Third workflow: deployment",
    "text": "Third workflow: deployment\nIn order to automate deployment to PyPI when we change our package version, we need to add our token as a secret in the GitHub repository. To do that, follow these instructions: Using secrets in GitHub Actions\nNote: the secret is the token itself, which starts with pypi- (e.g., pypi-AgENdG...).\nWe then add the following file in .github/workflows:\n\nname: Publish to PyPI\non:\n  push:\n    tags:\n      - \"*.*.*\"\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.13\"\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install .[build]\n\n    - name: Build package\n      run: python -m build\n\n    - name: Test publish package\n      uses: pypa/gh-action-pypi-publish@release/v1\n      with:\n        user: __token__\n        password: ${{ secrets.PYPI_API_TOKEN }}\n        repository-url: https://test.pypi.org/legacy/\n\nRemark: PYPI_API_TOKEN is the name we gave to the token secret in GitHub.\nA useful tool for handling versioning is bumpver. It automates updating versions in all files where the version appears, and we will use it to automatically commit and tag new versions.\nWe install and initialize bumpver:\n(ammonia_predict_3) ammonia_predict_3: pip install bumpver\n(ammonia_predict_3) ammonia_predict_3: bumpver init\nAfter initialization, we need to ensure that the end of the pyproject.toml file looks like this:\n\n...\n\n[tool.bumpver]\ncurrent_version = \"0.1.0\"\nversion_pattern = \"MAJOR.MINOR.PACH\"\ncommit_message = \"bump version {old_version} -&gt; {new_version}\"\ntag_message = \"{new_version}\"\ncommit = true\ntag = true\npush = true\n\n[tool.bumpver.file_patterns]\n\"pyproject.toml\" = [\n    'current_version = \"{version}\"',\n    'version = \"{version}\"',\n]\n\nNow let‚Äôs update our package to a new version:\n(ammonia_predict_3) ammonia_predict_3: bumpver update --minor\nINFO    - fetching tags from remote (to turn off use: -n / --no-fetch)\nINFO    - Old Version: 0.1.0\nINFO    - New Version: 0.2.0\nINFO    - git commit --message 'bump version 0.1.0 -&gt; 0.2.0'\nINFO    - git tag --annotate 0.2.0 --message '0.2.0'\nINFO    - git push origin --follow-tags 0.2.0 HEAD\nWe can check on GitHub that the actions triggered by the push and the tag run successfully:\n\nAnd that the package has been updated on TestPyPI:\n\nRemark: with push = true in the [tool.bumpver] section of pyproject.toml, the deployment workflow is triggered at the same time as the other workflows (in particular, testing). However, we want the tests to pass before publishing to TestPyPI. Therefore, we set push = false, and manually perform the push and tag steps.\nbumpver update --major\ngit push\n[waiting for lint and test workflows to be completed]\ngit push --tags"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "href": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "href": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "Processus de mise en commun des ateliers",
    "text": "Processus de mise en commun des ateliers\n\nCr√©er une branche propre √† l‚Äôatelier nomm√©e explicitement mon_nom_parlant et basculer dessus\n\ngit checkout -b mon_nom_parlant\n\nCr√©er un fichier Rmarkdown de restitution de votre atelier fichier.Rmd dans votre branche\n\ngit add fichier.Rmd\ngit commit -m \"restitution atelier\"\n\nPousser vos modifications sur le serveur distant\n\ngit  push --set-upstream origin mon_nom_parlant ou\ngit  push\n\nFaire une pull request (PR) sur github\nindiquer dans le message de la PR la liste des packages ou autres besoins\nQuand la PR passe les tests, demander le merge.\ncorriger les erreurs √©ventuelles dans la compilation du Rmarkdown\nles admins peuvent avoir √† mettre √† jour l‚Äôimage docker"
  },
  {
    "objectID": "instructions.html#d√©tails-du-fonctionnement",
    "href": "instructions.html#d√©tails-du-fonctionnement",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "D√©tails du fonctionnement",
    "text": "D√©tails du fonctionnement\n\nLe docker\nLien vers la fiche pense-b√™te\nVers d‚Äôautres ressources utiles\nPour cr√©er des images Docker en local sur sa machine, voici une liste de commandes utiles\n\nPour construire une image docker, il faut cr√©er un fichier Dockerfile qui contient la recette du Docker. Pour ce site le ficher Dockerfile a la forme suivante\n\npuis demander la construction de l‚Äôimage √† l‚Äôaide de la commande\n\n docker build -t nom_depot_dockerhub/nom_du_repo:version  . ## avec un nom\n\net enfin pousser sur Dockerhub\n\n docker push nom_depot_dockerhub/nom_du_repo:version\n\n\n\nLes actions\nDans les action de Github, on peut sp√©cifier un container docker √† utiliser, c‚Äôest ce que fait la ligne container du fichier d‚Äôaction suivant, utiliser pour cr√©er ce site web"
  },
  {
    "objectID": "sparse_cholesky.html",
    "href": "sparse_cholesky.html",
    "title": "Tutoriel sur la factorisation de Cholesky avec des matrices creuses",
    "section": "",
    "text": "Note\n\n\n\nL‚Äôobjectif de ce tutoriel est d‚Äôappr√©hender (douloureusement) la factorisation de Cholesky pour des matrices sparses. Il s‚Äôinspire fortement de plusieurs ressources:\n\n2 posts de blog de Dan Simpson post 1 et post 2\n1 cours [C1] de Michael T Heath et Edgar Solomonik sur les syst√®mes lin√©aires sparses\n1 cours [C2] de Gr√©goire Pichon sur les arbres d‚Äô√©limination"
  },
  {
    "objectID": "sparse_cholesky.html#√©quations-de-r√©cursion",
    "href": "sparse_cholesky.html#√©quations-de-r√©cursion",
    "title": "Tutoriel sur la factorisation de Cholesky avec des matrices creuses",
    "section": "√âquations de r√©cursion",
    "text": "√âquations de r√©cursion\nLe calcul de L (une fois son support connu) se fait via les √©quations de r√©cursions suivantes donn√©es ici pour la colonne \\(j=5\\) et utilisant le fait que si \\(L_{ji} = 0\\), la colonne \\(i\\) n‚Äôest pas utilis√©e pour la mise √† jour de \\(j\\).\n\nInitialisation \\[\n\\begin{pmatrix}\nt_{1}\\\\\n0\\\\\nt_{2}\\\\\nt_{3}\\\\\nt_{4}\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\na_{55}\\\\\n0\\\\\n0\\\\\n0\\\\\na_{95}\\\\\n\\end{pmatrix}\n\\]\nMise √† jour \\[\n\\begin{pmatrix}\nt_{1}\\\\\n0\\\\\nt_{2}\\\\\nt_{3}\\\\\nt_{4}\\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nt_{1}\\\\\n0\\\\\nt_{2}\\\\\nt_{3}\\\\\nt_{4}\\\\\n\\end{pmatrix} - l_{51} . \\begin{pmatrix}\nl_{51}\\\\\n0\\\\\nl_{71}\\\\\n0\\\\\n0\\\\\n\\end{pmatrix} - l_{52} . \\begin{pmatrix}\nl_{52}\\\\\n0\\\\\n0\\\\\nl_{82}\\\\\n0\\\\\n\\end{pmatrix}\n\\]\nCalcul dans \\(L\\) \\[\n\\begin{pmatrix}\nl_{51}\\\\\n0\\\\\nl_{52}\\\\\nl_{53}\\\\\nl_{54}\\\\\n\\end{pmatrix} = \\frac{1}{\\sqrt{t_{1}}} .\n\\begin{pmatrix}\nt_{1}\\\\\n0\\\\\nt_{2}\\\\\nt_{3}\\\\\nt_{4}\\\\\n\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\nAvertissement\n\n\n\nLes repr√©sentations CSC utilis√©es jusqu‚Äô√† pr√©sent pour calculer le support de \\(L\\) (√† savoir L_indices et L_indptr) utilisait l‚Äôexpression de \\(A\\) sous forme symm√©trique. La repr√©sentation CSC de \\(L\\) est la m√™me suivant qu‚Äôon utilise \\(L\\) ou sa partie triangulaire inf√©rieure (puisque \\(L\\) est par construction triangulaire inf√©rieure). Pour initialiser \\(L_x\\), on a donc besoin de partir de la repr√©sentation CSC triangulaire inf√©rieure de \\(A\\) uniquement.\n\n\nComme \\(L\\) et \\(A\\) n‚Äôont pas le m√™me support (L_x est plus long que A_x), il faut attention au moment d‚Äôinitialiser \\(L\\) √† partir de \\(A\\) _deep_copy_csc permet d‚Äôinitialiser L_x avec la bonne longeur et les valeurs de A_x plac√©es au bon endroit.\n\ndef _deep_copy_csc(A_indices, A_indptr, A_x, L_indices, L_indptr):\n  n = len(A_indptr) - 1\n  L_x = np.zeros(len(L_indices))\n  \n  for j in range(0, n):\n    copy_idx = np.nonzero(np.isin(L_indices[L_indptr[j]:L_indptr[j + 1]],\n                                  A_indices[A_indptr[j]:A_indptr[j+1]]))[0]\n    L_x[L_indptr[j] + copy_idx] = A_x[A_indptr[j]:A_indptr[j+1]]\n  return L_x\n\n\nA_tril = np.tril(A)\nA_tril_csc = sparse.CSC.fromdense(A_tril)\nL_x = _deep_copy_csc(A_tril_csc.indices, A_tril_csc.indptr, A_tril_csc.data, L_indices, L_indptr) # il faut initialiser L_x avec la bonne longeur mais aussi les valeurs de A plac√©es au bon endroit\n# c'est le r√¥le de deep_copy_csc\nprint(L_x)\n\n[9. 1. 1. 9. 1. 1. 9. 1. 1. 9. 1. 1. 9. 0. 0. 1. 9. 0. 0. 1. 9. 0. 1. 9.\n 1. 9.]\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn constate bien que _deep_copy_csc a recopi√© les valeurs de A_x dans L_x mais en y ajoutant des \\(0\\).\n\n\n\n\n\n\n\n\nAu sujet de JIT\n\n\n\nOn souhaite ne pas construire la liste descendant √† chaque fois, ce qu‚Äôon veux jitter c‚Äôest cette derni√®re fonction: √† structure donn√©e on veut calculer les valeurs de L. L_indices et L_indptr ne changent pas ! On va essayer de modifier l‚Äôalgorithme d‚Äô√©limination tree pour quelle retourne la liste des descendants."
  },
  {
    "objectID": "sparse_cholesky.html#calcul-effectif-de-l_x",
    "href": "sparse_cholesky.html#calcul-effectif-de-l_x",
    "title": "Tutoriel sur la factorisation de Cholesky avec des matrices creuses",
    "section": "Calcul effectif de L_x",
    "text": "Calcul effectif de L_x\nLa fonction reprend celle d√©crite dans post 1 mais met √† profit la repr√©sentation CSR (row_useful) pr√©c√©demment calcul√©e pour ne pas avoir √† d√©terminer les colonnes utiles au moment du calcul de L[:j].\n\n\n\n\n\n\nImportant\n\n\n\n\nIl y a un peu de manipulation p√©nible √† faire sur les L[:i] pour en extraire la sous-partie utile au calcul de L[:j].\nOn ne peut jitter le code en l‚Äô√©tat actuel\n\n\n\n\ndef _sparse_cholesky_csc_impl_modif(L_indices, L_indptr, L_x, row_useful):\n    n = len(L_indptr) - 1\n    # parcours sur colonnes\n    for j in range(0, n):\n        tmp = np.copy(L_x[L_indptr[j]:L_indptr[j + 1]]) # j-eme colonne de L, ie il ne faut pas de valeur manquante (m√™me structure de sparsit√©\n        # dans 1ere col de L et de A (oui il suffit de voir l'algo e_tree pour j=0, on a un seul noeud dans cet e_tree)\n\n        # MAJ de la colonne courante avec le contenu de \n        for k in row_useful[j]: # voir eq slice 1/37\n            # if (k == row_useful[j][0]): \n            #     print(\"########## Looking at column\", j)\n            #     print(\"With non nul rows:\", L_indices[L_indptr[j]:L_indptr[j+1]])\n            #     print(\"Current value of column j:\", tmp)\n            # print(\"#### Looking at column\", k, \"to update column\", j)\n            # print(\"#### Value of column\", k, L_x[L_indptr[k]:L_indptr[k+1]])\n            ## row indices higher than j in column k which is required to update column j\n            col_k_ind = L_indices[L_indptr[k]:L_indptr[k+1]]\n            ## find element corresponding to row j in column k\n            pad = np.where(L_indices[L_indptr[k]:L_indptr[k+1]] == j)[0][0]\n            # print(L_indices[L_indptr[k]:L_indptr[k+1]], pad)\n\n            Ljk = L_x[L_indptr[k] + pad]\n            # print(\"Ljk =\", Ljk)\n            col_k_ind = L_indices[(L_indptr[k] + pad):L_indptr[k+1]]\n            update_idx = np.nonzero(np.isin(                                 \\\n              L_indices[L_indptr[j]:L_indptr[j+1]], col_k_ind                          \\\n              ))[0]\n            # print(\"row indices that should be updated using values from columun k:\", col_k_ind)\n            # print(\"row indices updated in column j:\", L_indices[L_indptr[j]:L_indptr[j+1]][update_idx])\n            ## Update only required elements\n            # print(col_k_ind)\n            # print(\"Update of column j:\", L_x[(L_indptr[k] + pad):L_indptr[k+1]]) \n            tmp[update_idx] = tmp[update_idx] - Ljk * L_x[(L_indptr[k] + pad):L_indptr[k+1]]\n            # print(\"Current value of column j:\", tmp)\n            \n            # pad = np.nonzero(                                                \\\n            #   L_indices[L_indptr[k]:L_indptr[k+1]] == L_indices[L_indptr[j]])[0][0]\n            # update_idx = np.nonzero(np.in1d(                                 \\\n            #   L_indices[L_indptr[j]:L_indptr[j+1]],                          \\\n            #   L_indices[(L_indptr[k] + pad):L_indptr[k+1]]))[0]\n            # tmp[update_idx] = tmp[update_idx] -                              \\\n            #   Ljk * L_x[(L_indptr[k] + pad):L_indptr[k + 1]]\n            \n        diag = np.sqrt(tmp[0])\n        L_x[L_indptr[j]] = diag\n        L_x[(L_indptr[j] + 1):L_indptr[j + 1]] = tmp[1:] / diag  # calcul de la j-eme col de L sauf diag.\n        # print(tmp[1:].shape, L_x[(L_indptr[j] + 1):L_indptr[j + 1]].shape)\n        # print(\"Final value of tmp:\", (tmp[1:] / diag))\n        # print(\"Final value of column j:\", L_x[(L_indptr[j] + 1):L_indptr[j + 1]])\n        # Pour le cas j=0 il nous faut bien\n        # que la struct de sparsit√© de A de la premi√®re col soit celle de L\n    return L_x\n\n\nL_x_recons = _sparse_cholesky_csc_impl_modif(L_indices, L_indptr, np.copy(L_x), row_useful)\n\n\nL_recons_csc = sparse.CSC((L_x_recons, L_indices, L_indptr), shape=(9,9))\nL_recons = L_recons_csc.todense()\nL_recons[:, 4]\n\nL_true = np.linalg.cholesky(A)\nL_true[:, 4]\n\nA_ = (L_recons @ L_recons.T)\nassert jnp.allclose(A_, A, atol=1e-7)\nprint(\"Victoire !!!!\")\n\nVictoire !!!!\n\n\n√Ä la pr√©cision machine pr√®s, notre cholesky sparse donne le m√™me r√©sultat que celle de numpy ü•≥."
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "",
    "text": "Note\n\n\n\nL‚Äôobectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d‚Äôune fonction qui n‚Äôest disponible dans les primitives fournies par JAX/Torch. Les deux cas d‚Äôusage envisag√©es sont:\n\nl‚Äôutilisation d‚Äôune fonction non diff√©rentiable pour lesquels on veut √©crire une d√©riv√©e ‚Äúnon-standard‚Äù afin de pouvoir l‚Äôutiliser dans JAX/Torch\nl‚Äôutilisation d‚Äôune fonction donc une approximation analytique de la d√©riv√©e est disponible mais qui n‚Äôest pas impl√©ment√©e dans JAX/Torch\nDans ce tutoriel, on consid√®re une fonction jouet \\(f\\) qui d√©pend d‚Äôune entr√©e \\(x\\) et de param√®tres \\(a, b\\).\n\\[\nf: (x, a, b) \\in \\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R} \\mapsto \\tanh(a^\\top x + b) \\in \\mathbb{R}\n\\]\nOn rappelle que \\(tanh'(x) = 1 - \\tanh^2(x)\\) et que \\[\n\\frac{\\partial f}{\\partial x} = a.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial f} = x.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial f}{\\partial b} = (1 - \\tanh^2(a^\\top x + b))\n\\] ou en mode matriciel \\[\n\\nabla f(x, a, b) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}\\right)^\\top\n\\]\nOn rappelle que la diff√©rentiation automatique fait appel √† la chain-rule. Pour une fonction \\(g\\) √† valeurs dans \\(\\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R}\\), et en notant \\(h = f \\circ g\\), on a \\[\n\\begin{align}\n\\nabla h(z) & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\nabla f(g(z))^\\top \\nabla g(z) \\\\\n            & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\frac{\\partial h(z)}{\\partial g(z)} \\frac{\\partial g(z)}{\\partial z}\n\\end{align}            \n\\]\nOn peut calculer \\(\\nabla h(z)\\) de deux fa√ßons:\nEn pratique il faut √©crire jvp et vjp pour chaque fonction utilis√©e dans la composition."
  },
  {
    "objectID": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "href": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant les primitives de torch",
    "text": "En utilisant les primitives de torch\n\nimport torch\nimport numpy as np\n\nOn definit notre fonction \\(f\\) en torch.\n\ndef f_torch(x, a, b):\n    return torch.tanh(torch.dot(x, a) + b)\n\nOn d√©finit des valeurs pour lesquelles on sait calculer facilement le gradient.\n\nx = torch.tensor([2., 3.], requires_grad = True)\na = torch.ones(2, requires_grad = True)\nb = torch.tensor(-2., requires_grad = True)\n\nEt on calcule les d√©riv√©es partielles (avec la convention \\(\\partial f / \\partial x =\\) x.grad).\n\n## D√©finit y par rapport √† x\ny = f_torch(x, a, b)\ny\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \ny.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn peut √™tre plus concis pour calculer notre gradient (ici par rapport √† \\(x\\)) en d√©finissant directement la fonction \\((x, a, b) \\mapsto \\frac{\\partial f}{\\partial x}(x, a, b)\\) dans df_torch_dx\n\n\n\n\n\n\nNote\n\n\n\nLe param√®tre argnums=0 pr√©cise qu‚Äôon calcule la d√©riv√©e par rapport au premier argument de \\(f\\), en l‚Äôoccurence \\(x\\).\n\n\n\ndf_torch_dx = torch.func.grad(f_torch, argnums=0)\n\nOn v√©rifie que les deux fa√ßons de faire donnent le m√™me r√©sultat.\n\nassert torch.allclose(df_torch_dx(x, a, b), x.grad)"
  },
  {
    "objectID": "autodiff.html#en-utilisant-notre-propre-fonction",
    "href": "autodiff.html#en-utilisant-notre-propre-fonction",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant notre propre fonction",
    "text": "En utilisant notre propre fonction\nLe code qui suit correspond √† l‚Äôapplication des informations disponibles dans la documentation de torch sur notre fonction example. Un autre tutoriel int√©ressant est le suivant.\nOn doit d√©finir 4 m√©thodes: - forward qui re√ßoit les entr√©es et calcule la sortie - setup_context qui stocke dans un objet ctx des tenseurs qui peuvent √™tre r√©utilis√©s au moment du calcul de la d√©riv√©e (dans notre exemple, on a juste besoin de \\(x\\), \\(a\\) et \\(1 - \\tanh^2(a^\\top x + b)\\). - backward (ou vjp) qui re√ßoit le gradient calcul√© en aval et renvoie le gradient, pour faire de la diff√©rentiation automatique en mode reverse. - jvp qui re√ßoit une diff√©rentielle calcul√©e en amont et la multiplie en amont avant de la renvoyer, pour faire de la diff√©rentiation automatique en mode forward.\n\nD√©finition de la fonction\n\nclass f_torch_manual(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(x, a, b):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output.\n        \"\"\"\n        output = torch.tanh(torch.dot(a, x) + b)\n        return output\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        \"\"\"\n        ctx is a context object that can be used\n        to stash information for backward computation. You can cache tensors for\n        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n        objects can be stored directly as attributes on the ctx object, such as\n        ``ctx.my_object = my_object``.\n        \"\"\"\n        x, a, b = inputs\n        ## save output to cut computation time\n        scaling = 1. - output.pow(2) # tanh' = 1 - tanh^2\n        ctx.save_for_backward(x, a, scaling)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"        \n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation\n        \"\"\"\n        x, a, scaling = ctx.saved_tensors\n        grad_x = grad_output * a * scaling\n        grad_a = grad_output * x * scaling\n        grad_b = grad_output * scaling\n        return grad_x, grad_a, grad_b # on doit calculer les grad par rapport √† tous les arguments rajouter grad par rapport √† a et b\n\n    @staticmethod\n    def jvp(x, a, b, tangents):\n        \"\"\"                \n        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation\n        \"\"\"\n        ## Vector v of small perturbations\n        tx, ta, tb = tangents\n        ## Matrix (in this case vector) of first order gradient\n        result = torch.tanh(torch.dot(a, x) + b)\n        scaling = (1. - result.pow(2))\n        Jx = a * scaling\n        Ja = x * scaling\n        Jb = scaling\n        ## Return J(x, a, b)v\n        return torch.dot(Jx, tx) + torch.dot(Ja, ta) + Jb * tb\n\n\n\nV√©rification des d√©riv√©es\n\nEn mode reverseEn mode forward\n\n\n\n## D√©finit f\nf = f_torch_manual.apply\nz = f(x, a, b)\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \n## R√©initialise les gradients √† z√©ro avant tout calcul \nx.grad.zero_(), a.grad.zero_(), b.grad.zero_()\nz.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn v√©rifie qu‚Äôon obtient bien le m√™me r√©sultat qu‚Äôen laissant torch faire le calcul :party:.\nOn aurait aussi pu utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode reverse)\n\nf_grad_rev = torch.func.jacrev(func=f, argnums=(0, 1, 2))\n\net v√©rifier que le r√©sultat coincide avec le calcul fait √† la main.\n\nassert all(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) for i in range(2))\n\n\n\nOn calcule la d√©riv√©e par rapport √† la premi√®re coordonn√©e de \\(x\\)\n\ntangents = (torch.tensor([1., 0.]), torch.tensor([0., 0.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0099, grad_fn=&lt;AddBackward0&gt;)\n\n\npuis par rapport √† la deuxi√®me coordonn√©e de \\(a\\)\n\ntangents = (torch.tensor([0., 0.]), torch.tensor([0., 1.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0296, grad_fn=&lt;AddBackward0&gt;)\n\n\nEt on valide que les r√©sultats obtenus co√Øncident avec ceux obtenus en mode reverse et directement en utilisant torch üòÅ\n\n\n\n\n\n\nAvertissement\n\n\n\n\n\nEn th√©orie, on pourrait utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode forward)\n\nf_grad_fwd = torch.func.jacfwd(func=f, argnums=(0, 1, 2))\n\nmais il faut d√©finir une m√©thode statique vmap et je n‚Äôai pas compris comment faire üò¢"
  },
  {
    "objectID": "autodiff.html#comparaison-des-temps-de-calculs",
    "href": "autodiff.html#comparaison-des-temps-de-calculs",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Comparaison des temps de calculs",
    "text": "Comparaison des temps de calculs\nOn compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.\n\ndim = 100\na = torch.tensor(np.arange(dim)/ (dim*10), requires_grad = True)\nb = torch.tensor(0.5, requires_grad = True)\nx = torch.tensor( (np.arange(dim) - 37) / (dim*10), requires_grad = True)\n\n\nf_torch_grad_rev = torch.func.jacrev(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_rev(x, a, b)\nf_torch_grad_fwd = torch.func.jacfwd(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_fwd(x, a, b)\n%timeit f_grad_rev(x, a, b)\n\n427 Œºs ¬± 1.27 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n494 Œºs ¬± 1 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n761 Œºs ¬± 1.62 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "autodiff.html#impact-de-jit",
    "href": "autodiff.html#impact-de-jit",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Impact de JIT",
    "text": "Impact de JIT\nOn essaie de jitter nos fonctions pour v√©rifier si cela acc√©l√®re le calcul des gradients.\nPlus d‚Äôinfo sur le JIT dans pytorch sont disponibles dans cette documentation.\n\nf_torch_grad_rev_jit = torch.jit.trace(f_torch_grad_rev, (x, a, b))\nf_torch_grad_fwd_jit = torch.jit.trace(f_torch_grad_fwd, (x, a, b))\n# f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))\n\n\n%timeit f_torch_grad_rev_jit(x, a, b)\n%timeit f_torch_grad_fwd_jit(x, a, b)\n# %timeit f_grad_rev_jit(x, a, b)\n\n55.7 Œºs ¬± 154 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n124 Œºs ¬± 195 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "href": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "D√©riv√©e par rapport √† \\(x\\)",
    "text": "D√©riv√©e par rapport √† \\(x\\)\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax import random\n\n## On d√©finit une dimension arbitraire pour nos inputs\ndim = 100\n\n## On initialise les param√®tres et le vecteur d'input de la fonction\na = jnp.arange(dim)/ (dim*10)\nb = 0.5\nx = (jnp.arange(dim) - 37) / (dim*10)\n\n## On d√©finit une fonction simple dont on conna√Æt les gradients analytiques\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n## On affiche la valeur de la fonction pour v√©rifier que tout est ok\nf(x, a, b) \n\nArray(0.56842977, dtype=float32)\n\n\n\nD√©riv√©e par rapport √† \\(x\\)D√©riv√©e par rapport √† \\(a\\) et \\(b\\)\n\n\nDans un premier temps, on peut d√©finir les gradients exacts de cette fonction √† partir d‚Äôune formule analytique.\n\ndef df_dx(x, a, b):\n    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n\nPuis on d√©finit les gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) \n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\nassert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\nOn d√©finit √©galement les gradients exacts par rapport aux param√®tres \\(a\\) et \\(b\\) pour v√©rifier que l‚Äôon pourrait les optimiser dans un algorithme d‚Äôapprentissage.\n\ndef df_dab(x, a, b):\n    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2\n\nPuis on d√©finit ces gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dab = jax.jacfwd(lambda a_b: f(x, *a_b), argnums=0)\n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\nassert all(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) for i in range(2))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\n\nNous avons donc bien v√©rifi√© que les gradients calcul√©s avec JAX sont identiques aux gradients analytiques."
  },
  {
    "objectID": "autodiff.html#avec-vjp-et-jvp",
    "href": "autodiff.html#avec-vjp-et-jvp",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Avec VJP et JVP",
    "text": "Avec VJP et JVP\nDans les sections pr√©c√©dentes, jax.jacrev et jax.jacfwd utilisent, respectivement, les VJP et les JVP des op√©rations √©l√©mentaires de la fonction f. Dans certains cas, nous pouvons avoir besoin de d√©finir nous-m√™mes les VJP et JVP comme vu en introduction.\nNous allons alors d√©finir les VJP et JVP pour f √† l‚Äôaide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions √©l√©mentaires sous-jacentes ne seront alors plus utilis√©s.\n\nJVPVJP\n\n\nLien vers la documentation de JAX\nUn JVP est capable de d√©voiler une colonne de la jacobienne √† la fois. Ce n‚Äôest pas adapt√© pour cette fonction dont la jacobienne est large. Une passe JVP ne peut d√©voiler qu‚Äôune seule d√©riv√©e partielle : si l‚Äôon veut la d√©riv√©e par rapport √† chaque dimension de \\(x\\), chaque dimension de \\(a\\) et \\(b\\), il nous faut faire \\(dim + dim + 1\\) fois des JVPs ce qui n‚Äôest pas du tout efficace. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacfwd doit en fait appeler tous ces JVPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nPour d√©finir un custom_jvp en JAX, il faut attacher √† f, une fonction f_jvp, qui prend deux entr√©es primals le point o√π l‚Äôon calcule le gradient et tangents le vecteur tangent (√† voir aussi comme les gradients en amont du graphe que l‚Äôon parcourt en descendant). f_jvp retourne un tuple de deux vecteurs, f(primals) et le JVP df_dx @ tangents, o√π, bien s√ªr, df_dx contient l‚Äôexpression analytique de la d√©riv√©e (c‚Äôest une matrice jacobienne mais elle n‚Äôest jamais stock√©e en m√©moire car tout de suite r√©duite par le produit matriciel).\nNous avons vu que si tangents est un vecteur one-hot encoded nous d√©voilons une colonne de la matrice jacobienne (celle o√π se situe le \\(1\\)). Dans l‚Äôexemple ci-dessous nous calculons de mani√®re forward \\(\\frac{\\partial f}{\\partial x_0}\\).\n\n@jax.custom_jvp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, a, b = primals\n    x_dot, a_dot, b_dot = tangents\n    primal_out = f(x, a, b)\n    return primal_out, (jnp.dot(df_dx(x, a, b), x_dot) +  jnp.dot((x * (1 - primal_out ** 2)), a_dot) + jnp.dot((1 - primal_out ** 2), b_dot))\n\nx0_tangents = jnp.zeros(dim)\nx0_tangents = x0_tangents.at[0].set(1)\n_, x_dot = jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), 0.))\n\nassert jnp.allclose(grad_df_dx(x)[0], x_dot)\n\nprint(\"All right!\")\n\nAll right!\n\n\nOn note que, s‚Äôil est d√©fini, le custom_jvp sera utilis√© par JAX, en mode forward et en mode backward. Notons aussi la syntaxe particuli√®re √©manant du fait que f prend trois arguments en entr√©e.\n\n\nLien vers la documentation de JAX\nUn VJP est capable de d√©voiler une ligne de la jacobienne √† la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de \\(f\\) en un seul appel √† JVP car c‚Äôest une matrice √† une seule ligne. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacrec doit en fait appeler tous ces VJPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nSi nous souhaitons explicitement d√©finir le VJP, nous devons d‚Äôabord √©crire une fonction qui d√©crit la passe forward. C‚Äôest ici f_fwd qui retourne f(primal) et des valeurs stock√©es pour le moment de la passe backward (√† la mani√®re de save_for_backward vu dans la section pytorch !). Il faut ici bien r√©fl√©chir √† ce qui est n√©cessaire de stocker et ce qui est superflu, afin d‚Äôoptimiser au mieux le code. Ici nous stockons f(x,a,b), x et a car ces valeurs sont r√©utilis√©es dans la passe backward o√π nous calculons \\(g. \\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial x}\\), \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial a}\\) et \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial b}\\). Avec \\(g\\) le gradient provenant de l‚Äôaval du graphe pour calculer les VJPs (rappelons que nous les calculons de mani√®re backward en remontant le graphe).\nNous comprenons √† nouveau que si g est un vecteur one-hot encoded nous d√©voilons une ligne de la matrice jacobienne (celle o√π se situe le \\(1\\)).\nNous devons √©galement √©crire une fonction f_bwd qui prend en argument les valeurs stock√©es dans la passe forward ainsi que g d√©fini dans le paragraphe pr√©c√©dent. Ici g est scalaire f a valeurs dans \\(\\mathbb{R}\\). f_bwd retourne autant de sorties que f compte d‚Äôentr√©es.\n\n@jax.custom_vjp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\ndef f_fwd(x, a, b):\n    primal_out = f(x, a, b)\n    return primal_out, (x, a, primal_out)\n\ndef f_bwd(res, g):\n    x, a, primal_out = res\n    return (a * (1 - primal_out **2) * g, x * (1 - primal_out **2) * g, (1 - primal_out **2) * g)\n\nf.defvjp(f_fwd, f_bwd)\n_, f_vjp = jax.vjp(f, x, a, b) # renvoie f(primal) et f_vjp qui est une fonction qui doit √™tre √©valu√©e en `g`\n\nassert jnp.allclose(grad_df_dx(x), f_vjp(1.)[0])\nassert all(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(1.)[i + 1]) for i in range(2))\nprint(\"All right!\")\n\nAll right!\n\n\nNotons que la d√©finition d‚Äôun custom_vjp red√©finit la fonction grad qui utilise donc aussi f_fwd. Ainsi, nous avons l‚Äô√©quivalence :\n\nassert jnp.allclose(f_vjp(1.)[0], jax.grad(f)(x, a, b))\nprint(\"All right!\")\n\nAll right!"
  },
  {
    "objectID": "autodiff.html#conclusion",
    "href": "autodiff.html#conclusion",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Conclusion",
    "text": "Conclusion"
  }
]