[
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions pour le dépot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec clés SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc complète."
  },
  {
    "objectID": "instructions.html#cloner-le-dépôt-git-du-bootcamp",
    "href": "instructions.html#cloner-le-dépôt-git-du-bootcamp",
    "title": "Instructions pour le dépot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec clés SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc complète."
  },
  {
    "objectID": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "href": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "title": "Instructions pour le dépot sur le site web",
    "section": "Processus de mise en commun des ateliers",
    "text": "Processus de mise en commun des ateliers\n\nCréer une branche propre à l’atelier nommée explicitement mon_nom_parlant et basculer dessus\n\ngit checkout -b mon_nom_parlant\n\nCréer un fichier Rmarkdown de restitution de votre atelier fichier.Rmd dans votre branche\n\ngit add fichier.Rmd\ngit commit -m \"restitution atelier\"\n\nPousser vos modifications sur le serveur distant\n\ngit  push --set-upstream origin mon_nom_parlant ou\ngit  push\n\nFaire une pull request (PR) sur github\nindiquer dans le message de la PR la liste des packages ou autres besoins\nQuand la PR passe les tests, demander le merge.\ncorriger les erreurs éventuelles dans la compilation du Rmarkdown\nles admins peuvent avoir à mettre à jour l’image docker"
  },
  {
    "objectID": "instructions.html#détails-du-fonctionnement",
    "href": "instructions.html#détails-du-fonctionnement",
    "title": "Instructions pour le dépot sur le site web",
    "section": "Détails du fonctionnement",
    "text": "Détails du fonctionnement\n\nLe docker\nLien vers la fiche pense-bête\nVers d’autres ressources utiles\nPour créer des images Docker en local sur sa machine, voici une liste de commandes utiles\n\nPour construire une image docker, il faut créer un fichier Dockerfile qui contient la recette du Docker. Pour ce site le ficher Dockerfile a la forme suivante\n\npuis demander la construction de l’image à l’aide de la commande\n\n docker build -t nom_depot_dockerhub/nom_du_repo:version  . ## avec un nom\n\net enfin pousser sur Dockerhub\n\n docker push nom_depot_dockerhub/nom_du_repo:version\n\n\n\nLes actions\nDans les action de Github, on peut spécifier un container docker à utiliser, c’est ce que fait la ligne container du fichier d’action suivant, utiliser pour créer ce site web"
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "Tutoriel de différentiation automatique",
    "section": "",
    "text": "Note\n\n\n\nL’obectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d’une fonction qui n’est disponible dans les primitives fournies par JAX/Torch. Les deux cas d’usage envisagées sont:\n\nl’utilisation d’une fonction non différentiable pour lesquels on veut écrire une dérivée “non-standard” afin de pouvoir l’utiliser dans JAX/Torch\nl’utilisation d’une fonction donc une approximation analytique de la dérivée est disponible mais qui n’est pas implémentée dans JAX/Torch\nDans ce tutoriel, on considère une fonction jouet \\(f\\) qui dépend d’une entrée \\(x\\) et de paramètres \\(a, b\\).\n\\[\nf: (x, a, b) \\in \\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R} \\mapsto \\tanh(a^\\top x + b) \\in \\mathbb{R}\n\\]\nOn rappelle que \\(tanh'(x) = 1 - \\tanh^2(x)\\) et que \\[\n\\frac{\\partial f}{\\partial x} = a.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial f} = x.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial f}{\\partial b} = (1 - \\tanh^2(a^\\top x + b))\n\\] ou en mode matriciel \\[\n\\nabla f(x, a, b) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}\\right)^\\top\n\\]\nOn rappelle que la différentiation automatique fait appel à la chain-rule. Pour une fonction \\(g\\) à valeurs dans \\(\\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R}\\), et en notant \\(h = f \\circ g\\), on a \\[\n\\begin{align}\n\\nabla h(z) & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\nabla f(g(z))^\\top \\nabla g(z) \\\\\n            & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\frac{\\partial h(z)}{\\partial g(z)} \\frac{\\partial g(z)}{\\partial z}\n\\end{align}            \n\\]\nOn peut calculer \\(\\nabla h(z)\\) de deux façons:\nEn pratique il faut écrire jvp et vjp pour chaque fonction utilisée dans la composition."
  },
  {
    "objectID": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "href": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "title": "Tutoriel de différentiation automatique",
    "section": "En utilisant les primitives de torch",
    "text": "En utilisant les primitives de torch\n\nimport torch\nimport numpy as np\n\nOn definit notre fonction \\(f\\) en torch.\n\ndef f_torch(x, a, b):\n    return torch.tanh(torch.dot(x, a) + b)\n\nOn définit des valeurs pour lesquelles on sait calculer facilement le gradient.\n\nx = torch.tensor([2., 3.], requires_grad = True)\na = torch.ones(2, requires_grad = True)\nb = torch.tensor(-2., requires_grad = True)\n\nEt on calcule les dérivées partielles (avec la convention \\(\\partial f / \\partial x =\\) x.grad).\n\n## Définit y par rapport à x\ny = f_torch(x, a, b)\ny\n## Calcule et évalue le graphe de différentiation automatique de y par rapport à x \ny.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn peut être plus concis pour calculer notre gradient (ici par rapport à \\(x\\)) en définissant directement la fonction \\((x, a, b) \\mapsto \\frac{\\partial f}{\\partial x}(x, a, b)\\) dans df_torch_dx\n\n\n\n\n\n\nNote\n\n\n\nLe paramètre argnums=0 précise qu’on calcule la dérivée par rapport au premier argument de \\(f\\), en l’occurence \\(x\\).\n\n\n\ndf_torch_dx = torch.func.grad(f_torch, argnums=0)\n\nOn vérifie que les deux façons de faire donnent le même résultat.\n\nassert torch.allclose(df_torch_dx(x, a, b), x.grad)"
  },
  {
    "objectID": "autodiff.html#en-utilisant-notre-propre-fonction",
    "href": "autodiff.html#en-utilisant-notre-propre-fonction",
    "title": "Tutoriel de différentiation automatique",
    "section": "En utilisant notre propre fonction",
    "text": "En utilisant notre propre fonction\nLe code qui suit correspond à l’application des informations disponibles dans la documentation de torch sur notre fonction example. Un autre tutoriel intéressant est le suivant.\nOn doit définir 4 méthodes: - forward qui reçoit les entrées et calcule la sortie - setup_context qui stocke dans un objet ctx des tenseurs qui peuvent être réutilisés au moment du calcul de la dérivée (dans notre exemple, on a juste besoin de \\(x\\), \\(a\\) et \\(1 - \\tanh^2(a^\\top x + b)\\). - backward (ou vjp) qui reçoit le gradient calculé en aval et renvoie le gradient, pour faire de la différentiation automatique en mode reverse. - jvp qui reçoit une différentielle calculée en amont et la multiplie en amont avant de la renvoyer, pour faire de la différentiation automatique en mode forward.\n\nDéfinition de la fonction\n\nclass f_torch_manual(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(x, a, b):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output.\n        \"\"\"\n        output = torch.tanh(torch.dot(a, x) + b)\n        return output\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        \"\"\"\n        ctx is a context object that can be used\n        to stash information for backward computation. You can cache tensors for\n        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n        objects can be stored directly as attributes on the ctx object, such as\n        ``ctx.my_object = my_object``.\n        \"\"\"\n        x, a, b = inputs\n        ## save output to cut computation time\n        scaling = 1. - output.pow(2) # tanh' = 1 - tanh^2\n        ctx.save_for_backward(x, a, scaling)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"        \n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation\n        \"\"\"\n        x, a, scaling = ctx.saved_tensors\n        grad_x = grad_output * a * scaling\n        grad_a = grad_output * x * scaling\n        grad_b = grad_output * scaling\n        return grad_x, grad_a, grad_b # on doit calculer les grad par rapport à tous les arguments rajouter grad par rapport à a et b\n\n    @staticmethod\n    def jvp(x, a, b, tangents):\n        \"\"\"                \n        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation\n        \"\"\"\n        ## Vector v of small perturbations\n        tx, ta, tb = tangents\n        ## Matrix (in this case vector) of first order gradient\n        result = torch.tanh(torch.dot(a, x) + b)\n        scaling = (1. - result.pow(2))\n        Jx = a * scaling\n        Ja = x * scaling\n        Jb = scaling\n        ## Return J(x, a, b)v\n        return torch.dot(Jx, tx) + torch.dot(Ja, ta) + Jb * tb\n\n\n\nVérification des dérivées\n\nEn mode reverseEn mode forward\n\n\n\n## Définit f\nf = f_torch_manual.apply\nz = f(x, a, b)\n## Calcule et évalue le graphe de différentiation automatique de y par rapport à x \n## Réinitialise les gradients à zéro avant tout calcul \nx.grad.zero_(), a.grad.zero_(), b.grad.zero_()\nz.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn vérifie qu’on obtient bien le même résultat qu’en laissant torch faire le calcul :party:.\nOn aurait aussi pu utiliser les opérateurs fonctionnels pour calculer la fonction dérivée (en utilisant le mode reverse)\n\nf_grad_rev = torch.func.jacrev(func=f, argnums=(0, 1, 2))\n\net vérifier que le résultat coincide avec le calcul fait à la main.\n\nassert all(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) for i in range(2))\n\n\n\nOn calcule la dérivée par rapport à la première coordonnée de \\(x\\)\n\ntangents = (torch.tensor([1., 0.]), torch.tensor([0., 0.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0099, grad_fn=&lt;AddBackward0&gt;)\n\n\npuis par rapport à la deuxième coordonnée de \\(a\\)\n\ntangents = (torch.tensor([0., 0.]), torch.tensor([0., 1.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0296, grad_fn=&lt;AddBackward0&gt;)\n\n\nEt on valide que les résultats obtenus coïncident avec ceux obtenus en mode reverse et directement en utilisant torch 😁\n\n\n\n\n\n\nAvertissement\n\n\n\n\n\nEn théorie, on pourrait utiliser les opérateurs fonctionnels pour calculer la fonction dérivée (en utilisant le mode forward)\n\nf_grad_fwd = torch.func.jacfwd(func=f, argnums=(0, 1, 2))\n\nmais il faut définir une méthode statique vmap et je n’ai pas compris comment faire 😢"
  },
  {
    "objectID": "autodiff.html#comparaison-des-temps-de-calculs",
    "href": "autodiff.html#comparaison-des-temps-de-calculs",
    "title": "Tutoriel de différentiation automatique",
    "section": "Comparaison des temps de calculs",
    "text": "Comparaison des temps de calculs\nOn compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.\n\ndim = 100\na = torch.tensor(np.arange(dim)/ (dim*10), requires_grad = True)\nb = torch.tensor(0.5, requires_grad = True)\nx = torch.tensor( (np.arange(dim) - 37) / (dim*10), requires_grad = True)\n\n\nf_torch_grad_rev = torch.func.jacrev(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_rev(x, a, b)\nf_torch_grad_fwd = torch.func.jacfwd(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_fwd(x, a, b)\n%timeit f_grad_rev(x, a, b)\n\n428 μs ± 1.31 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n508 μs ± 2.29 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n776 μs ± 2.68 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "autodiff.html#impact-de-jit",
    "href": "autodiff.html#impact-de-jit",
    "title": "Tutoriel de différentiation automatique",
    "section": "Impact de JIT",
    "text": "Impact de JIT\nOn essaie de jitter nos fonctions pour vérifier si cela accélère le calcul des gradients.\nPlus d’info sur le JIT dans pytorch sont disponibles dans cette documentation.\n\nf_torch_grad_rev_jit = torch.jit.trace(f_torch_grad_rev, (x, a, b))\nf_torch_grad_fwd_jit = torch.jit.trace(f_torch_grad_fwd, (x, a, b))\n# f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))\n\n\n%timeit f_torch_grad_rev_jit(x, a, b)\n%timeit f_torch_grad_fwd_jit(x, a, b)\n# %timeit f_grad_rev_jit(x, a, b)\n\n55.8 μs ± 130 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n125 μs ± 340 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "autodiff.html#dérivée-par-rapport-à-x",
    "href": "autodiff.html#dérivée-par-rapport-à-x",
    "title": "Tutoriel de différentiation automatique",
    "section": "Dérivée par rapport à \\(x\\)",
    "text": "Dérivée par rapport à \\(x\\)\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax import random\n\n## On définit une dimension arbitraire pour nos inputs\ndim = 100\n\n## On initialise les paramètres et le vecteur d'input de la fonction\na = jnp.arange(dim)/ (dim*10)\nb = 0.5\nx = (jnp.arange(dim) - 37) / (dim*10)\n\n## On définit une fonction simple dont on connaît les gradients analytiques\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n## On affiche la valeur de la fonction pour vérifier que tout est ok\nf(x, a, b) \n\nArray(0.56842977, dtype=float32)\n\n\n\nDérivée par rapport à \\(x\\)Dérivée par rapport à \\(a\\) et \\(b\\)\n\n\nDans un premier temps, on peut définir les gradients exacts de cette fonction à partir d’une formule analytique.\n\ndef df_dx(x, a, b):\n    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n\nPuis on définit les gradients via autograd et on vérifie que les résultats sont identiques.\n\n## jax.grad calcule la formule backward par défaut\ngrad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) \n\n## On vérifie que les gradients retournent des valeurs identiques\nassert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\nassert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\nOn définit également les gradients exacts par rapport aux paramètres \\(a\\) et \\(b\\) pour vérifier que l’on pourrait les optimiser dans un algorithme d’apprentissage.\n\ndef df_dab(x, a, b):\n    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2\n\nPuis on définit ces gradients via autograd et on vérifie que les résultats sont identiques.\n\n## jax.grad calcule la formule backward par défaut\ngrad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dab = jax.jacfwd(lambda a_b: f(x, *a_b), argnums=0)\n\n## On vérifie que les gradients retournent des valeurs identiques\nassert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\nassert all(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) for i in range(2))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\n\nNous avons donc bien vérifié que les gradients calculés avec JAX sont identiques aux gradients analytiques."
  },
  {
    "objectID": "autodiff.html#avec-vjp-et-jvp",
    "href": "autodiff.html#avec-vjp-et-jvp",
    "title": "Tutoriel de différentiation automatique",
    "section": "Avec VJP et JVP",
    "text": "Avec VJP et JVP\nDans les sections précédentes, jax.jacrev et jax.jacfwd utilisent, respectivement, les VJP et les JVP des opérations élémentaires de la fonction f. Dans certains cas, nous pouvons avoir besoin de définir nous-mêmes les VJP et JVP comme vu en introduction.\nNous allons alors définir les VJP et JVP pour f à l’aide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions élémentaires sous-jacentes ne seront alors plus utilisés.\n\nJVPVJP\n\n\nLien vers la documentation de JAX\nUn JVP est capable de dévoiler une colonne de la jacobienne à la fois. Ce n’est pas adapté pour cette fonction dont la jacobienne est large. Une passe JVP ne peut dévoiler qu’une seule dérivée partielle : si l’on veut la dérivée par rapport à chaque dimension de \\(x\\), chaque dimension de \\(a\\) et \\(b\\), il nous faut faire \\(dim + dim + 1\\) fois des JVPs ce qui n’est pas du tout efficace. Nous l’avons vu dans la section précédente où en fait, jax.jacfwd doit en fait appeler tous ces JVPs (ce qui est fait de manière cachée à l’utilisateur).\nPour définir un custom_jvp en JAX, il faut attacher à f, une fonction f_jvp, qui prend deux entrées primals le point où l’on calcule le gradient et tangents le vecteur tangent (à voir aussi comme les gradients en amont du graphe que l’on parcourt en descendant). f_jvp retourne un tuple de deux vecteurs, f(primals) et le JVP df_dx @ tangents, où, bien sûr, df_dx contient l’expression analytique de la dérivée (c’est une matrice jacobienne mais elle n’est jamais stockée en mémoire car tout de suite réduite par le produit matriciel).\nNous avons vu que si tangents est un vecteur one-hot encoded nous dévoilons une colonne de la matrice jacobienne (celle où se situe le \\(1\\)). Dans l’exemple ci-dessous nous calculons de manière forward \\(\\frac{\\partial f}{\\partial x_0}\\).\n\n@jax.custom_jvp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, a, b = primals\n    x_dot, a_dot, b_dot = tangents\n    primal_out = f(x, a, b)\n    return primal_out, (jnp.dot(df_dx(x, a, b), x_dot) +  jnp.dot((x * (1 - primal_out ** 2)), a_dot) + jnp.dot((1 - primal_out ** 2), b_dot))\n\nx0_tangents = jnp.zeros(dim)\nx0_tangents = x0_tangents.at[0].set(1)\n_, x_dot = jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), 0.))\n\nassert jnp.allclose(grad_df_dx(x)[0], x_dot)\n\nprint(\"All right!\")\n\nAll right!\n\n\nOn note que, s’il est défini, le custom_jvp sera utilisé par JAX, en mode forward et en mode backward. Notons aussi la syntaxe particulière émanant du fait que f prend trois arguments en entrée.\n\n\nLien vers la documentation de JAX\nUn VJP est capable de dévoiler une ligne de la jacobienne à la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de \\(f\\) en un seul appel à JVP car c’est une matrice à une seule ligne. Nous l’avons vu dans la section précédente où en fait, jax.jacrec doit en fait appeler tous ces VJPs (ce qui est fait de manière cachée à l’utilisateur).\nSi nous souhaitons explicitement définir le VJP, nous devons d’abord écrire une fonction qui décrit la passe forward. C’est ici f_fwd qui retourne f(primal) et des valeurs stockées pour le moment de la passe backward (à la manière de save_for_backward vu dans la section pytorch !). Il faut ici bien réfléchir à ce qui est nécessaire de stocker et ce qui est superflu, afin d’optimiser au mieux le code. Ici nous stockons f(x,a,b), x et a car ces valeurs sont réutilisées dans la passe backward où nous calculons \\(g. \\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial x}\\), \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial a}\\) et \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial b}\\). Avec \\(g\\) le gradient provenant de l’aval du graphe pour calculer les VJPs (rappelons que nous les calculons de manière backward en remontant le graphe).\nNous comprenons à nouveau que si g est un vecteur one-hot encoded nous dévoilons une ligne de la matrice jacobienne (celle où se situe le \\(1\\)).\nNous devons également écrire une fonction f_bwd qui prend en argument les valeurs stockées dans la passe forward ainsi que g défini dans le paragraphe précédent. Ici g est scalaire f a valeurs dans \\(\\mathbb{R}\\). f_bwd retourne autant de sorties que f compte d’entrées.\n\n@jax.custom_vjp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\ndef f_fwd(x, a, b):\n    primal_out = f(x, a, b)\n    return primal_out, (x, a, primal_out)\n\ndef f_bwd(res, g):\n    x, a, primal_out = res\n    return (a * (1 - primal_out **2) * g, x * (1 - primal_out **2) * g, (1 - primal_out **2) * g)\n\nf.defvjp(f_fwd, f_bwd)\n_, f_vjp = jax.vjp(f, x, a, b) # renvoie f(primal) et f_vjp qui est une fonction qui doit être évaluée en `g`\n\nassert jnp.allclose(grad_df_dx(x), f_vjp(1.)[0])\nassert all(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(1.)[i + 1]) for i in range(2))\nprint(\"All right!\")\n\nAll right!\n\n\nNotons que la définition d’un custom_vjp redéfinit la fonction grad qui utilise donc aussi f_fwd. Ainsi, nous avons l’équivalence :\n\nassert jnp.allclose(f_vjp(1.)[0], jax.grad(f)(x, a, b))\nprint(\"All right!\")\n\nAll right!"
  },
  {
    "objectID": "autodiff.html#conclusion",
    "href": "autodiff.html#conclusion",
    "title": "Tutoriel de différentiation automatique",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "",
    "text": "L’atelier Finist’R 2025 – ou bootcamp R du groupe State Of The R se déroulera à la station biologique de Roscoff du 18 au 22 août 2025.\nStateoftheR est un réseau du département MathNum INRAE.\n\n\nIl s’agit de la neuvième édition de l’atelier Finist’R. Cet atelier réunit annuellement un groupe de chercheurs, ingénieurs, doctorants, tous utilisateurs avancés de R et développeurs de paquets pour explorer les dernières fonctionnalités du logiciel et les nouvelles pratiques de développement. A l’issue de l’atelier le collectif produit une synthèse de cette veille logiciel de manière à progresser collectivement dans l’utilisation du logiciel mais surtout dans la production d’outils statistiques à destination de la communauté.\nLe résultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#où-quand",
    "href": "index.html#où-quand",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "",
    "text": "L’atelier Finist’R 2025 – ou bootcamp R du groupe State Of The R se déroulera à la station biologique de Roscoff du 18 au 22 août 2025.\nStateoftheR est un réseau du département MathNum INRAE.\n\n\nIl s’agit de la neuvième édition de l’atelier Finist’R. Cet atelier réunit annuellement un groupe de chercheurs, ingénieurs, doctorants, tous utilisateurs avancés de R et développeurs de paquets pour explorer les dernières fonctionnalités du logiciel et les nouvelles pratiques de développement. A l’issue de l’atelier le collectif produit une synthèse de cette veille logiciel de manière à progresser collectivement dans l’utilisation du logiciel mais surtout dans la production d’outils statistiques à destination de la communauté.\nLe résultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#programme",
    "href": "index.html#programme",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "Programme",
    "text": "Programme"
  },
  {
    "objectID": "index.html#participants",
    "href": "index.html#participants",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "Participants",
    "text": "Participants\nBaptiste Alglave, Julie Aubert, Pierre Barbillon, Gloria Buritica, Lucia Clarotto, Caroline Cognot, Marie-Pierre Etienne, Armand Favrot, Blanche Francheterre, Hugo Gangloff, Pascal Irz, Louis Lacoste, Arthur Leroy, Mahendra Mariadassou, Pierre Navaro, Léo Micollet, Jeanne Tous."
  },
  {
    "objectID": "index.html#soutien",
    "href": "index.html#soutien",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "Soutien",
    "text": "Soutien"
  }
]