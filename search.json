[
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "href": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "href": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "Processus de mise en commun des ateliers",
    "text": "Processus de mise en commun des ateliers\n\nCr√©er une branche propre √† l‚Äôatelier nomm√©e explicitement mon_nom_parlant et basculer dessus\n\ngit checkout -b mon_nom_parlant\n\nCr√©er un fichier Rmarkdown de restitution de votre atelier fichier.Rmd dans votre branche\n\ngit add fichier.Rmd\ngit commit -m \"restitution atelier\"\n\nPousser vos modifications sur le serveur distant\n\ngit  push --set-upstream origin mon_nom_parlant ou\ngit  push\n\nFaire une pull request (PR) sur github\nindiquer dans le message de la PR la liste des packages ou autres besoins\nQuand la PR passe les tests, demander le merge.\ncorriger les erreurs √©ventuelles dans la compilation du Rmarkdown\nles admins peuvent avoir √† mettre √† jour l‚Äôimage docker"
  },
  {
    "objectID": "instructions.html#d√©tails-du-fonctionnement",
    "href": "instructions.html#d√©tails-du-fonctionnement",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "D√©tails du fonctionnement",
    "text": "D√©tails du fonctionnement\n\nLe docker\nLien vers la fiche pense-b√™te\nVers d‚Äôautres ressources utiles\nPour cr√©er des images Docker en local sur sa machine, voici une liste de commandes utiles\n\nPour construire une image docker, il faut cr√©er un fichier Dockerfile qui contient la recette du Docker. Pour ce site le ficher Dockerfile a la forme suivante\n\npuis demander la construction de l‚Äôimage √† l‚Äôaide de la commande\n\n docker build -t nom_depot_dockerhub/nom_du_repo:version  . ## avec un nom\n\net enfin pousser sur Dockerhub\n\n docker push nom_depot_dockerhub/nom_du_repo:version\n\n\n\nLes actions\nDans les action de Github, on peut sp√©cifier un container docker √† utiliser, c‚Äôest ce que fait la ligne container du fichier d‚Äôaction suivant, utiliser pour cr√©er ce site web"
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "",
    "text": "Note\n\n\n\nL‚Äôobectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d‚Äôune fonction qui n‚Äôest disponible dans les primitives fournies par JAX/Torch. Les deux cas d‚Äôusage envisag√©es sont:\n\nl‚Äôutilisation d‚Äôune fonction non diff√©rentiable pour lesquels on veut √©crire une d√©riv√©e ‚Äúnon-standard‚Äù afin de pouvoir l‚Äôutiliser dans JAX/Torch\nl‚Äôutilisation d‚Äôune fonction donc une approximation analytique de la d√©riv√©e est disponible mais qui n‚Äôest pas impl√©ment√©e dans JAX/Torch\nDans ce tutoriel, on consid√®re une fonction jouet \\(f\\) qui d√©pend d‚Äôune entr√©e \\(x\\) et de param√®tres \\(a, b\\).\n\\[\nf: (x, a, b) \\in \\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R} \\mapsto \\tanh(a^\\top x + b) \\in \\mathbb{R}\n\\]\nOn rappelle que \\(tanh'(x) = 1 - \\tanh^2(x)\\) et que \\[\n\\frac{\\partial f}{\\partial x} = a.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial f} = x.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial f}{\\partial b} = (1 - \\tanh^2(a^\\top x + b))\n\\] ou en mode matriciel \\[\n\\nabla f(x, a, b) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}\\right)^\\top\n\\]\nOn rappelle que la diff√©rentiation automatique fait appel √† la chain-rule. Pour une fonction \\(g\\) √† valeurs dans \\(\\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R}\\), et en notant \\(h = f \\circ g\\), on a \\[\n\\begin{align}\n\\nabla h(z) & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\nabla f(g(z))^\\top \\nabla g(z) \\\\\n            & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\frac{\\partial h(z)}{\\partial g(z)} \\frac{\\partial g(z)}{\\partial z}\n\\end{align}            \n\\]\nOn peut calculer \\(\\nabla h(z)\\) de deux fa√ßons:\nEn pratique il faut √©crire jvp et vjp pour chaque fonction utilis√©e dans la composition."
  },
  {
    "objectID": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "href": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant les primitives de torch",
    "text": "En utilisant les primitives de torch\n\nimport torch\nimport numpy as np\n\nOn definit notre fonction \\(f\\) en torch.\n\ndef f_torch(x, a, b):\n    return torch.tanh(torch.dot(x, a) + b)\n\nOn d√©finit des valeurs pour lesquelles on sait calculer facilement le gradient.\n\nx = torch.tensor([2., 3.], requires_grad = True)\na = torch.ones(2, requires_grad = True)\nb = torch.tensor(-2., requires_grad = True)\n\nEt on calcule les d√©riv√©es partielles (avec la convention \\(\\partial f / \\partial x =\\) x.grad).\n\n## D√©finit y par rapport √† x\ny = f_torch(x, a, b)\ny\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \ny.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn peut √™tre plus concis pour calculer notre gradient (ici par rapport √† \\(x\\)) en d√©finissant directement la fonction \\((x, a, b) \\mapsto \\frac{\\partial f}{\\partial x}(x, a, b)\\) dans df_torch_dx\n\n\n\n\n\n\nNote\n\n\n\nLe param√®tre argnums=0 pr√©cise qu‚Äôon calcule la d√©riv√©e par rapport au premier argument de \\(f\\), en l‚Äôoccurence \\(x\\).\n\n\n\ndf_torch_dx = torch.func.grad(f_torch, argnums=0)\n\nOn v√©rifie que les deux fa√ßons de faire donnent le m√™me r√©sultat.\n\nassert torch.allclose(df_torch_dx(x, a, b), x.grad)"
  },
  {
    "objectID": "autodiff.html#en-utilisant-notre-propre-fonction",
    "href": "autodiff.html#en-utilisant-notre-propre-fonction",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant notre propre fonction",
    "text": "En utilisant notre propre fonction\nLe code qui suit correspond √† l‚Äôapplication des informations disponibles dans la documentation de torch sur notre fonction example. Un autre tutoriel int√©ressant est le suivant.\nOn doit d√©finir 4 m√©thodes: - forward qui re√ßoit les entr√©es et calcule la sortie - setup_context qui stocke dans un objet ctx des tenseurs qui peuvent √™tre r√©utilis√©s au moment du calcul de la d√©riv√©e (dans notre exemple, on a juste besoin de \\(x\\), \\(a\\) et \\(1 - \\tanh^2(a^\\top x + b)\\). - backward (ou vjp) qui re√ßoit le gradient calcul√© en aval et renvoie le gradient, pour faire de la diff√©rentiation automatique en mode reverse. - jvp qui re√ßoit une diff√©rentielle calcul√©e en amont et la multiplie en amont avant de la renvoyer, pour faire de la diff√©rentiation automatique en mode forward.\n\nD√©finition de la fonction\n\nclass f_torch_manual(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(x, a, b):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output.\n        \"\"\"\n        output = torch.tanh(torch.dot(a, x) + b)\n        return output\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        \"\"\"\n        ctx is a context object that can be used\n        to stash information for backward computation. You can cache tensors for\n        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n        objects can be stored directly as attributes on the ctx object, such as\n        ``ctx.my_object = my_object``.\n        \"\"\"\n        x, a, b = inputs\n        ## save output to cut computation time\n        scaling = 1. - output.pow(2) # tanh' = 1 - tanh^2\n        ctx.save_for_backward(x, a, scaling)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"        \n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation\n        \"\"\"\n        x, a, scaling = ctx.saved_tensors\n        grad_x = grad_output * a * scaling\n        grad_a = grad_output * x * scaling\n        grad_b = grad_output * scaling\n        return grad_x, grad_a, grad_b # on doit calculer les grad par rapport √† tous les arguments rajouter grad par rapport √† a et b\n\n    @staticmethod\n    def jvp(x, a, b, tangents):\n        \"\"\"                \n        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation\n        \"\"\"\n        ## Vector v of small perturbations\n        tx, ta, tb = tangents\n        ## Matrix (in this case vector) of first order gradient\n        result = torch.tanh(torch.dot(a, x) + b)\n        scaling = (1. - result.pow(2))\n        Jx = a * scaling\n        Ja = x * scaling\n        Jb = scaling\n        ## Return J(x, a, b)v\n        return torch.dot(Jx, tx) + torch.dot(Ja, ta) + Jb * tb\n\n\n\nV√©rification des d√©riv√©es\n\nEn mode reverseEn mode forward\n\n\n\n## D√©finit f\nf = f_torch_manual.apply\nz = f(x, a, b)\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \n## R√©initialise les gradients √† z√©ro avant tout calcul \nx.grad.zero_(), a.grad.zero_(), b.grad.zero_()\nz.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn v√©rifie qu‚Äôon obtient bien le m√™me r√©sultat qu‚Äôen laissant torch faire le calcul :party:.\nOn aurait aussi pu utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode reverse)\n\nf_grad_rev = torch.func.jacrev(func=f, argnums=(0, 1, 2))\n\net v√©rifier que le r√©sultat coincide avec le calcul fait √† la main.\n\nassert all(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) for i in range(2))\n\n\n\nOn calcule la d√©riv√©e par rapport √† la premi√®re coordonn√©e de \\(x\\)\n\ntangents = (torch.tensor([1., 0.]), torch.tensor([0., 0.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0099, grad_fn=&lt;AddBackward0&gt;)\n\n\npuis par rapport √† la deuxi√®me coordonn√©e de \\(a\\)\n\ntangents = (torch.tensor([0., 0.]), torch.tensor([0., 1.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0296, grad_fn=&lt;AddBackward0&gt;)\n\n\nEt on valide que les r√©sultats obtenus co√Øncident avec ceux obtenus en mode reverse et directement en utilisant torch üòÅ\n\n\n\n\n\n\nAvertissement\n\n\n\n\n\nEn th√©orie, on pourrait utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode forward)\n\nf_grad_fwd = torch.func.jacfwd(func=f, argnums=(0, 1, 2))\n\nmais il faut d√©finir une m√©thode statique vmap et je n‚Äôai pas compris comment faire üò¢"
  },
  {
    "objectID": "autodiff.html#comparaison-des-temps-de-calculs",
    "href": "autodiff.html#comparaison-des-temps-de-calculs",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Comparaison des temps de calculs",
    "text": "Comparaison des temps de calculs\nOn compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.\n\ndim = 100\na = torch.tensor(np.arange(dim)/ (dim*10), requires_grad = True)\nb = torch.tensor(0.5, requires_grad = True)\nx = torch.tensor( (np.arange(dim) - 37) / (dim*10), requires_grad = True)\n\n\nf_torch_grad_rev = torch.func.jacrev(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_rev(x, a, b)\nf_torch_grad_fwd = torch.func.jacfwd(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_fwd(x, a, b)\n%timeit f_grad_rev(x, a, b)\n\n428 Œºs ¬± 1.31 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n508 Œºs ¬± 2.29 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n776 Œºs ¬± 2.68 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "autodiff.html#impact-de-jit",
    "href": "autodiff.html#impact-de-jit",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Impact de JIT",
    "text": "Impact de JIT\nOn essaie de jitter nos fonctions pour v√©rifier si cela acc√©l√®re le calcul des gradients.\nPlus d‚Äôinfo sur le JIT dans pytorch sont disponibles dans cette documentation.\n\nf_torch_grad_rev_jit = torch.jit.trace(f_torch_grad_rev, (x, a, b))\nf_torch_grad_fwd_jit = torch.jit.trace(f_torch_grad_fwd, (x, a, b))\n# f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))\n\n\n%timeit f_torch_grad_rev_jit(x, a, b)\n%timeit f_torch_grad_fwd_jit(x, a, b)\n# %timeit f_grad_rev_jit(x, a, b)\n\n55.8 Œºs ¬± 130 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n125 Œºs ¬± 340 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "href": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "D√©riv√©e par rapport √† \\(x\\)",
    "text": "D√©riv√©e par rapport √† \\(x\\)\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax import random\n\n## On d√©finit une dimension arbitraire pour nos inputs\ndim = 100\n\n## On initialise les param√®tres et le vecteur d'input de la fonction\na = jnp.arange(dim)/ (dim*10)\nb = 0.5\nx = (jnp.arange(dim) - 37) / (dim*10)\n\n## On d√©finit une fonction simple dont on conna√Æt les gradients analytiques\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n## On affiche la valeur de la fonction pour v√©rifier que tout est ok\nf(x, a, b) \n\nArray(0.56842977, dtype=float32)\n\n\n\nD√©riv√©e par rapport √† \\(x\\)D√©riv√©e par rapport √† \\(a\\) et \\(b\\)\n\n\nDans un premier temps, on peut d√©finir les gradients exacts de cette fonction √† partir d‚Äôune formule analytique.\n\ndef df_dx(x, a, b):\n    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n\nPuis on d√©finit les gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) \n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\nassert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\nOn d√©finit √©galement les gradients exacts par rapport aux param√®tres \\(a\\) et \\(b\\) pour v√©rifier que l‚Äôon pourrait les optimiser dans un algorithme d‚Äôapprentissage.\n\ndef df_dab(x, a, b):\n    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2\n\nPuis on d√©finit ces gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dab = jax.jacfwd(lambda a_b: f(x, *a_b), argnums=0)\n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\nassert all(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) for i in range(2))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\n\nNous avons donc bien v√©rifi√© que les gradients calcul√©s avec JAX sont identiques aux gradients analytiques."
  },
  {
    "objectID": "autodiff.html#avec-vjp-et-jvp",
    "href": "autodiff.html#avec-vjp-et-jvp",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Avec VJP et JVP",
    "text": "Avec VJP et JVP\nDans les sections pr√©c√©dentes, jax.jacrev et jax.jacfwd utilisent, respectivement, les VJP et les JVP des op√©rations √©l√©mentaires de la fonction f. Dans certains cas, nous pouvons avoir besoin de d√©finir nous-m√™mes les VJP et JVP comme vu en introduction.\nNous allons alors d√©finir les VJP et JVP pour f √† l‚Äôaide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions √©l√©mentaires sous-jacentes ne seront alors plus utilis√©s.\n\nJVPVJP\n\n\nLien vers la documentation de JAX\nUn JVP est capable de d√©voiler une colonne de la jacobienne √† la fois. Ce n‚Äôest pas adapt√© pour cette fonction dont la jacobienne est large. Une passe JVP ne peut d√©voiler qu‚Äôune seule d√©riv√©e partielle : si l‚Äôon veut la d√©riv√©e par rapport √† chaque dimension de \\(x\\), chaque dimension de \\(a\\) et \\(b\\), il nous faut faire \\(dim + dim + 1\\) fois des JVPs ce qui n‚Äôest pas du tout efficace. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacfwd doit en fait appeler tous ces JVPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nPour d√©finir un custom_jvp en JAX, il faut attacher √† f, une fonction f_jvp, qui prend deux entr√©es primals le point o√π l‚Äôon calcule le gradient et tangents le vecteur tangent (√† voir aussi comme les gradients en amont du graphe que l‚Äôon parcourt en descendant). f_jvp retourne un tuple de deux vecteurs, f(primals) et le JVP df_dx @ tangents, o√π, bien s√ªr, df_dx contient l‚Äôexpression analytique de la d√©riv√©e (c‚Äôest une matrice jacobienne mais elle n‚Äôest jamais stock√©e en m√©moire car tout de suite r√©duite par le produit matriciel).\nNous avons vu que si tangents est un vecteur one-hot encoded nous d√©voilons une colonne de la matrice jacobienne (celle o√π se situe le \\(1\\)). Dans l‚Äôexemple ci-dessous nous calculons de mani√®re forward \\(\\frac{\\partial f}{\\partial x_0}\\).\n\n@jax.custom_jvp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, a, b = primals\n    x_dot, a_dot, b_dot = tangents\n    primal_out = f(x, a, b)\n    return primal_out, (jnp.dot(df_dx(x, a, b), x_dot) +  jnp.dot((x * (1 - primal_out ** 2)), a_dot) + jnp.dot((1 - primal_out ** 2), b_dot))\n\nx0_tangents = jnp.zeros(dim)\nx0_tangents = x0_tangents.at[0].set(1)\n_, x_dot = jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), 0.))\n\nassert jnp.allclose(grad_df_dx(x)[0], x_dot)\n\nprint(\"All right!\")\n\nAll right!\n\n\nOn note que, s‚Äôil est d√©fini, le custom_jvp sera utilis√© par JAX, en mode forward et en mode backward. Notons aussi la syntaxe particuli√®re √©manant du fait que f prend trois arguments en entr√©e.\n\n\nLien vers la documentation de JAX\nUn VJP est capable de d√©voiler une ligne de la jacobienne √† la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de \\(f\\) en un seul appel √† JVP car c‚Äôest une matrice √† une seule ligne. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacrec doit en fait appeler tous ces VJPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nSi nous souhaitons explicitement d√©finir le VJP, nous devons d‚Äôabord √©crire une fonction qui d√©crit la passe forward. C‚Äôest ici f_fwd qui retourne f(primal) et des valeurs stock√©es pour le moment de la passe backward (√† la mani√®re de save_for_backward vu dans la section pytorch !). Il faut ici bien r√©fl√©chir √† ce qui est n√©cessaire de stocker et ce qui est superflu, afin d‚Äôoptimiser au mieux le code. Ici nous stockons f(x,a,b), x et a car ces valeurs sont r√©utilis√©es dans la passe backward o√π nous calculons \\(g. \\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial x}\\), \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial a}\\) et \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial b}\\). Avec \\(g\\) le gradient provenant de l‚Äôaval du graphe pour calculer les VJPs (rappelons que nous les calculons de mani√®re backward en remontant le graphe).\nNous comprenons √† nouveau que si g est un vecteur one-hot encoded nous d√©voilons une ligne de la matrice jacobienne (celle o√π se situe le \\(1\\)).\nNous devons √©galement √©crire une fonction f_bwd qui prend en argument les valeurs stock√©es dans la passe forward ainsi que g d√©fini dans le paragraphe pr√©c√©dent. Ici g est scalaire f a valeurs dans \\(\\mathbb{R}\\). f_bwd retourne autant de sorties que f compte d‚Äôentr√©es.\n\n@jax.custom_vjp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\ndef f_fwd(x, a, b):\n    primal_out = f(x, a, b)\n    return primal_out, (x, a, primal_out)\n\ndef f_bwd(res, g):\n    x, a, primal_out = res\n    return (a * (1 - primal_out **2) * g, x * (1 - primal_out **2) * g, (1 - primal_out **2) * g)\n\nf.defvjp(f_fwd, f_bwd)\n_, f_vjp = jax.vjp(f, x, a, b) # renvoie f(primal) et f_vjp qui est une fonction qui doit √™tre √©valu√©e en `g`\n\nassert jnp.allclose(grad_df_dx(x), f_vjp(1.)[0])\nassert all(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(1.)[i + 1]) for i in range(2))\nprint(\"All right!\")\n\nAll right!\n\n\nNotons que la d√©finition d‚Äôun custom_vjp red√©finit la fonction grad qui utilise donc aussi f_fwd. Ainsi, nous avons l‚Äô√©quivalence :\n\nassert jnp.allclose(f_vjp(1.)[0], jax.grad(f)(x, a, b))\nprint(\"All right!\")\n\nAll right!"
  },
  {
    "objectID": "autodiff.html#conclusion",
    "href": "autodiff.html#conclusion",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#o√π-quand",
    "href": "index.html#o√π-quand",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#programme",
    "href": "index.html#programme",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Programme",
    "text": "Programme"
  },
  {
    "objectID": "index.html#participants",
    "href": "index.html#participants",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Participants",
    "text": "Participants\nBaptiste Alglave, Julie Aubert, Pierre Barbillon, Gloria Buritica, Lucia Clarotto, Caroline Cognot, Marie-Pierre Etienne, Armand Favrot, Blanche Francheterre, Hugo Gangloff, Pascal Irz, Louis Lacoste, Arthur Leroy, Mahendra Mariadassou, Pierre Navaro, L√©o Micollet, Jeanne Tous."
  },
  {
    "objectID": "index.html#soutien",
    "href": "index.html#soutien",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Soutien",
    "text": "Soutien"
  }
]