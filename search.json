[
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "href": "instructions.html#cloner-le-d√©p√¥t-git-du-bootcamp",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2025.git\nAvec cl√©s SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2025.git\n\n\nLien vers une doc compl√®te."
  },
  {
    "objectID": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "href": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "Processus de mise en commun des ateliers",
    "text": "Processus de mise en commun des ateliers\n\nCr√©er une branche propre √† l‚Äôatelier nomm√©e explicitement mon_nom_parlant et basculer dessus\n\ngit checkout -b mon_nom_parlant\n\nCr√©er un fichier Rmarkdown de restitution de votre atelier fichier.Rmd dans votre branche\n\ngit add fichier.Rmd\ngit commit -m \"restitution atelier\"\n\nPousser vos modifications sur le serveur distant\n\ngit  push --set-upstream origin mon_nom_parlant ou\ngit  push\n\nFaire une pull request (PR) sur github\nindiquer dans le message de la PR la liste des packages ou autres besoins\nQuand la PR passe les tests, demander le merge.\ncorriger les erreurs √©ventuelles dans la compilation du Rmarkdown\nles admins peuvent avoir √† mettre √† jour l‚Äôimage docker"
  },
  {
    "objectID": "instructions.html#d√©tails-du-fonctionnement",
    "href": "instructions.html#d√©tails-du-fonctionnement",
    "title": "Instructions pour le d√©pot sur le site web",
    "section": "D√©tails du fonctionnement",
    "text": "D√©tails du fonctionnement\n\nLe docker\nLien vers la fiche pense-b√™te\nVers d‚Äôautres ressources utiles\nPour cr√©er des images Docker en local sur sa machine, voici une liste de commandes utiles\n\nPour construire une image docker, il faut cr√©er un fichier Dockerfile qui contient la recette du Docker. Pour ce site le ficher Dockerfile a la forme suivante\n\npuis demander la construction de l‚Äôimage √† l‚Äôaide de la commande\n\n docker build -t nom_depot_dockerhub/nom_du_repo:version  . ## avec un nom\n\net enfin pousser sur Dockerhub\n\n docker push nom_depot_dockerhub/nom_du_repo:version\n\n\n\nLes actions\nDans les action de Github, on peut sp√©cifier un container docker √† utiliser, c‚Äôest ce que fait la ligne container du fichier d‚Äôaction suivant, utiliser pour cr√©er ce site web"
  },
  {
    "objectID": "traccar.html",
    "href": "traccar.html",
    "title": "Traccar",
    "section": "",
    "text": "Traccar is an open-source GPS tracking platform. It allows you to track vehicles, people, or any GPS-enabled device in real time. It comes with a web User Interface to view devices on a map and can integrate a large number of GPS tracking devices.\nIt works with:\n\na GPS device (can be a smartphone) that sends location data to the Traccar server.\na server that stores data in a database (e.g., an OVH server).\na web app and APIs that allow you to visualize, analyze, or forward that data.\n\nWe first provide a tutorial to test the Traccar system (a demo provided by the developers). Then, we describe the main steps to build your own tracking system (setting up the server, connecting devices, and extracting the data from the database).\n\n\nFirst, download the Traccar client application and follow the installation steps.\n\n\n\n\n\nBy default, it should be connected to a demo server (http://demo.traccar.org) and have a specific device identifier.\nYou can connect to the server via this same link and add a system device (click the + symbol on the top of the left bar).\n\n\n\n\n\nTo connect your system device, you must enter your identifier in the Traccar client app in the identifier box.\n\n\n\n\n\nFrom the client app, send your location. This should now be visible on the Traccar Manager.\n‚ö†Ô∏è Note: There are several demo servers. If one server is not working properly (e.g., you cannot connect your device to it), try another one. Here is the list of demo servers.\nYou can follow your own track through time by clicking the replay button."
  },
  {
    "objectID": "traccar.html#demo-of-the-traccar-system",
    "href": "traccar.html#demo-of-the-traccar-system",
    "title": "Traccar",
    "section": "",
    "text": "First, download the Traccar client application and follow the installation steps.\n\n\n\n\n\nBy default, it should be connected to a demo server (http://demo.traccar.org) and have a specific device identifier.\nYou can connect to the server via this same link and add a system device (click the + symbol on the top of the left bar).\n\n\n\n\n\nTo connect your system device, you must enter your identifier in the Traccar client app in the identifier box.\n\n\n\n\n\nFrom the client app, send your location. This should now be visible on the Traccar Manager.\n‚ö†Ô∏è Note: There are several demo servers. If one server is not working properly (e.g., you cannot connect your device to it), try another one. Here is the list of demo servers.\nYou can follow your own track through time by clicking the replay button."
  },
  {
    "objectID": "traccar.html#setting-up-the-ovh-server",
    "href": "traccar.html#setting-up-the-ovh-server",
    "title": "Traccar",
    "section": "Setting up the OVH server",
    "text": "Setting up the OVH server\nFirst, subscribe to an OVH account for a Virtual Private Server (VPS). Choose an Ubuntu 24.04 distribution.\nIt is required to create an ssh authentification key. For the first login, you will need a temporary password, created for the first connection an send by email by OVH.\nAs Traccar relies on MySQL and we want to be able to administrate the MySQL Database, we first need to install the so-called LAMP stack:\n\nLinux\nApache\nMySQL\nphpMyAdmin\n\n\nStep 1: Install and configure Apache and MySQL\nsudo su -\napt update && apt upgrade -y && apt install apache2 -y\nsystemctl start apache2\nsystemctl enable apache2\n\napt install mysql-server -y\nsystemctl start mysql\nsystemctl enable mysql\n\nmysql_secure_installation\nChoose security level 1 (medium) and configure the root password.\nTypical configuration:\nPlease enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 1\nRemove anonymous users? (y|Y for Yes, any other key for No): y\nDisallow root login remotely? (y|Y for Yes, any other key for No): No\nRemove test database and access to it? (y|Y for Yes, any other key for No): n\nReload privilege tables now? (y|Y for Yes, any other key for No): y\nBy default, there is no root password. Let‚Äôs create one manually:\nmysql -u root\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_your_root_password';\n\n\nStep 2: Install PHP and phpMyAdmin\nThis not mandatory for traccar but is helpful to explore the database.\napt install php libapache2-mod-php php-mysql php-mbstring php-zip php-gd php-json php-curl -y\napt install phpmyadmin -y\nDuring installation, choose: - Apache - Do not configure the phpmyadmin database automatically\n\n\nStep 3: Create the phpMyAdmin database and user\nmysql -u root -p\nCREATE DATABASE phpmyadmin;\nCREATE USER 'phpmyadmin'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_phpmyadmin_password'; \nGRANT ALL PRIVILEGES ON phpmyadmin.* TO 'phpmyadmin'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n\n\nImport phpMyAdmin tables\nmysql -u root -p phpmyadmin &lt; /usr/share/phpmyadmin/sql/create_tables.sql\n\n\nEdit the configuration file\nnano /etc/phpmyadmin/config-db.php\n&lt;?php\n$dbuser='phpmyadmin';\n$dbpass='enter_phpmyadmin_password';\n$basepath='';\n$dbname='phpmyadmin';\n$dbserver='localhost';\n$dbport='3306';\n$dbtype='mysql';\n?&gt;\nYou can now access phpMyAdmin at:\nüëâ http://your-server-ip/phpmyadmin with your root or phpmyadmin credentials."
  },
  {
    "objectID": "traccar.html#installing-traccar",
    "href": "traccar.html#installing-traccar",
    "title": "Traccar",
    "section": "Installing Traccar",
    "text": "Installing Traccar\n\nStep 1: Prepare MySQL for Traccar\nThe Traccar installer temporarily uses root/root. We need to relax password restrictions:\nSET GLOBAL validate_password.LENGTH = 4;\nSET GLOBAL validate_password.policy = 0;\nSET GLOBAL validate_password.mixed_case_count = 0;\nSET GLOBAL validate_password.number_count = 0;\nSET GLOBAL validate_password.special_char_count = 0;\nSET GLOBAL validate_password.check_user_name = 0;\n\nALTER USER 'root'@'localhost' IDENTIFIED BY 'root';\nGRANT ALL ON *.* TO 'root'@'localhost' WITH GRANT OPTION;\n\nCREATE USER 'traccar'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_traccar_password';\nCREATE DATABASE traccar;\nGRANT ALL PRIVILEGES ON traccar.* TO 'traccar'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n\n\nStep 2: Install Traccar\nwget https://www.traccar.org/download/traccar-linux-64-latest.zip\napt install unzip\nunzip traccar-linux-*.zip\n./traccar.run\n\n\nStep 3: Configure Traccar\nnano /opt/traccar/conf/traccar.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE properties SYSTEM 'http://java.sun.com/dtd/properties.dtd'&gt;\n&lt;properties&gt;\n    &lt;entry key='database.driver'&gt;com.mysql.cj.jdbc.Driver&lt;/entry&gt;\n    &lt;entry key='database.url'&gt;jdbc:mysql://localhost/traccar?zeroDateTimeBehavior=round&amp;serverTimezone=UTC&amp;allowPublicKeyRetrieval=true&amp;useSSL=false&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useUnicode=yes&amp;characterEncoding=UTF-8&amp;sessionVariables=sql_mode=''&lt;/entry&gt;\n    &lt;entry key='database.user'&gt;traccar&lt;/entry&gt;\n    &lt;entry key='database.password'&gt;enter_traccar_password&lt;/entry&gt;\n&lt;/properties&gt;\n\n\nStep 4: Secure MySQL and start Traccar\nALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'enter_your_root_password';\nGRANT ALL PRIVILEGES ON traccar.* TO 'traccar'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\nVerify connection:\nmysql -u root -p\nThen start Traccar:\nservice traccar start\nThis last step will fill the database (and creates the diferent tables for the first call)"
  },
  {
    "objectID": "traccar.html#securing-with-apache-and-ssl",
    "href": "traccar.html#securing-with-apache-and-ssl",
    "title": "Traccar",
    "section": "Securing with Apache and SSL",
    "text": "Securing with Apache and SSL\nCreate the configuration file:\nnano /etc/apache2/sites-available/traccar.conf\n&lt;VirtualHost *:80&gt;\n  ServerName your-domain-or-ip\n  Redirect / https://your-domain-or-ip\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost _default_:443&gt;\n        ServerName your-domain-or-ip\n        ServerAdmin your_email@example.com\n\n        DocumentRoot /var/www/html\n\n        ProxyPass /api/socket ws://localhost:8082/api/socket\n        ProxyPassReverse /api/socket ws://localhost:8082/api/socket\n\n        ProxyPass / http://localhost:8082/\n        ProxyPassReverse / http://localhost:8082/\n\n        SSLEngine on\n        SSLCertificateFile /etc/ssl/certs/ssl-cert-snakeoil.pem\n        SSLCertificateKeyFile /etc/ssl/private/ssl-cert-snakeoil.key\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nInstall SSL and required modules:\nsudo apt-get install ssl-cert\nsudo a2enmod ssl proxy_http proxy_wstunnel rewrite\nsudo service apache2 restart\nEnable the site and generate a Let‚Äôs Encrypt certificate:\nsudo a2dissite 000-default\nsudo a2ensite traccar\nsudo service apache2 restart\nsudo apt install certbot python3-certbot-apache\nsudo certbot --apache\nThe server mysql is now ready and the database which will be used to record the position data is named traccar."
  },
  {
    "objectID": "traccar.html#setting-up-the-individual-device",
    "href": "traccar.html#setting-up-the-individual-device",
    "title": "Traccar",
    "section": "Setting up the individual device",
    "text": "Setting up the individual device\nThe traccar client is available on the Apple store and the Google store. Once downladed, there is a few step to set up the client\nEnter the server address http://51.91.58.42:5055,\nChoose Pr√©cision de la localisation : la plus √©llev√©e\nIntervalle (secondes) 30\nThe user has to send the device id to the traccar administrator\nOn the traccar server, click on the + to add a device and enter the device id.\nThe traccar app records the position when the traacar app is on. The user click on Envoyer la position to send the recoded positions to the traccar database."
  },
  {
    "objectID": "traccar.html#acc√®s-√†-la-base-de-donn√©es-tracca",
    "href": "traccar.html#acc√®s-√†-la-base-de-donn√©es-tracca",
    "title": "Traccar",
    "section": "Acc√®s √† la base de donn√©es tracca",
    "text": "Acc√®s √† la base de donn√©es tracca\nTO access the traccar database, one ption which works is to first create a ssh tunnel which links local port 3307 to remote port 3306 on the server via\nssh -L 3307:localhost:3306 root@51.91.58.42\nThen you can access to the database via mysql or through the following R script\n\nlibrary(DBI)\nlibrary(RMySQL)\n\n\n# Connexion\ncon &lt;- dbConnect(RMySQL::MySQL(),\n                 host = \"127.0.0.1\",\n                 port = 3307,\n                 dbname = \"traccar\",\n                 username = \"user_db_name\",\n                 password = \"user_db_passwd\")\n\n# Exemples de requ√™tes\ndevices &lt;- dbGetQuery(con, \"SELECT * FROM tc_devices\")\npositions &lt;- dbGetQuery(con, \"SELECT * FROM tc_positions\")\n\n# Fermer connexion\ndbDisconnect(con)\n\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\npositions |&gt; select(id, deviceid, devicetime, latitude,longitude) |&gt; group_by(deviceid) |&gt; count()\n\n# A tibble: 3 √ó 2\n# Groups:   deviceid [3]\n  deviceid     n\n     &lt;int&gt; &lt;int&gt;\n1        1    62\n2        2  1216\n3        3     9\n\npositions |&gt; select(id, deviceid, devicetime, latitude,longitude) |&gt;  ggplot() +aes(x=longitude, y = latitude, col = as.factor(deviceid)) + geom_path() \n\n\n\n\n\n\n\npositions |&gt; \n  mutate(temps = ymd_hms(devicetime)) |&gt;  \n  group_by(deviceid) |&gt;\n  mutate(elapse_time = temps - lag(temps)) |&gt; \n  ggplot() + aes(x=as.factor(deviceid), y = elapse_time) + geom_point() \n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "",
    "text": "Note\n\n\n\nL‚Äôobectif de ce tutoriel est de montrer comment utiliser JAX et PyTorch pour calculer le JVP (Jacobian Vector Product) et le VJP (Vector Jacobian Product) d‚Äôune fonction qui n‚Äôest disponible dans les primitives fournies par JAX/Torch. Les deux cas d‚Äôusage envisag√©es sont:\n\nl‚Äôutilisation d‚Äôune fonction non diff√©rentiable pour lesquels on veut √©crire une d√©riv√©e ‚Äúnon-standard‚Äù afin de pouvoir l‚Äôutiliser dans JAX/Torch\nl‚Äôutilisation d‚Äôune fonction donc une approximation analytique de la d√©riv√©e est disponible mais qui n‚Äôest pas impl√©ment√©e dans JAX/Torch\nDans ce tutoriel, on consid√®re une fonction jouet \\(f\\) qui d√©pend d‚Äôune entr√©e \\(x\\) et de param√®tres \\(a, b\\).\n\\[\nf: (x, a, b) \\in \\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R} \\mapsto \\tanh(a^\\top x + b) \\in \\mathbb{R}\n\\]\nOn rappelle que \\(tanh'(x) = 1 - \\tanh^2(x)\\) et que \\[\n\\frac{\\partial f}{\\partial x} = a.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial d}{\\partial f} = x.(1 - \\tanh^2(a^\\top x + b)) \\qquad \\frac{\\partial f}{\\partial b} = (1 - \\tanh^2(a^\\top x + b))\n\\] ou en mode matriciel \\[\n\\nabla f(x, a, b) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}\\right)^\\top\n\\]\nOn rappelle que la diff√©rentiation automatique fait appel √† la chain-rule. Pour une fonction \\(g\\) √† valeurs dans \\(\\mathbb{R}^p \\times \\mathbb{R}^p \\times \\mathbb{R}\\), et en notant \\(h = f \\circ g\\), on a \\[\n\\begin{align}\n\\nabla h(z) & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\nabla f(g(z))^\\top \\nabla g(z) \\\\\n            & = \\frac{\\partial (f \\circ g)(z)}{\\partial z} = \\frac{\\partial h(z)}{\\partial g(z)} \\frac{\\partial g(z)}{\\partial z}\n\\end{align}            \n\\]\nOn peut calculer \\(\\nabla h(z)\\) de deux fa√ßons:\nEn pratique il faut √©crire jvp et vjp pour chaque fonction utilis√©e dans la composition."
  },
  {
    "objectID": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "href": "autodiff.html#en-utilisant-les-primitives-de-torch",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant les primitives de torch",
    "text": "En utilisant les primitives de torch\n\nimport torch\nimport numpy as np\n\nOn definit notre fonction \\(f\\) en torch.\n\ndef f_torch(x, a, b):\n    return torch.tanh(torch.dot(x, a) + b)\n\nOn d√©finit des valeurs pour lesquelles on sait calculer facilement le gradient.\n\nx = torch.tensor([2., 3.], requires_grad = True)\na = torch.ones(2, requires_grad = True)\nb = torch.tensor(-2., requires_grad = True)\n\nEt on calcule les d√©riv√©es partielles (avec la convention \\(\\partial f / \\partial x =\\) x.grad).\n\n## D√©finit y par rapport √† x\ny = f_torch(x, a, b)\ny\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \ny.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn peut √™tre plus concis pour calculer notre gradient (ici par rapport √† \\(x\\)) en d√©finissant directement la fonction \\((x, a, b) \\mapsto \\frac{\\partial f}{\\partial x}(x, a, b)\\) dans df_torch_dx\n\n\n\n\n\n\nNote\n\n\n\nLe param√®tre argnums=0 pr√©cise qu‚Äôon calcule la d√©riv√©e par rapport au premier argument de \\(f\\), en l‚Äôoccurence \\(x\\).\n\n\n\ndf_torch_dx = torch.func.grad(f_torch, argnums=0)\n\nOn v√©rifie que les deux fa√ßons de faire donnent le m√™me r√©sultat.\n\nassert torch.allclose(df_torch_dx(x, a, b), x.grad)"
  },
  {
    "objectID": "autodiff.html#en-utilisant-notre-propre-fonction",
    "href": "autodiff.html#en-utilisant-notre-propre-fonction",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "En utilisant notre propre fonction",
    "text": "En utilisant notre propre fonction\nLe code qui suit correspond √† l‚Äôapplication des informations disponibles dans la documentation de torch sur notre fonction example. Un autre tutoriel int√©ressant est le suivant.\nOn doit d√©finir 4 m√©thodes: - forward qui re√ßoit les entr√©es et calcule la sortie - setup_context qui stocke dans un objet ctx des tenseurs qui peuvent √™tre r√©utilis√©s au moment du calcul de la d√©riv√©e (dans notre exemple, on a juste besoin de \\(x\\), \\(a\\) et \\(1 - \\tanh^2(a^\\top x + b)\\). - backward (ou vjp) qui re√ßoit le gradient calcul√© en aval et renvoie le gradient, pour faire de la diff√©rentiation automatique en mode reverse. - jvp qui re√ßoit une diff√©rentielle calcul√©e en amont et la multiplie en amont avant de la renvoyer, pour faire de la diff√©rentiation automatique en mode forward.\n\nD√©finition de la fonction\n\nclass f_torch_manual(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(x, a, b):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output.\n        \"\"\"\n        output = torch.tanh(torch.dot(a, x) + b)\n        return output\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        \"\"\"\n        ctx is a context object that can be used\n        to stash information for backward computation. You can cache tensors for\n        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n        objects can be stored directly as attributes on the ctx object, such as\n        ``ctx.my_object = my_object``.\n        \"\"\"\n        x, a, b = inputs\n        ## save output to cut computation time\n        scaling = 1. - output.pow(2) # tanh' = 1 - tanh^2\n        ctx.save_for_backward(x, a, scaling)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"        \n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        It corresponds to a Vector Jacobian Product (vjp), used for reverse auto-differentiation\n        \"\"\"\n        x, a, scaling = ctx.saved_tensors\n        grad_x = grad_output * a * scaling\n        grad_a = grad_output * x * scaling\n        grad_b = grad_output * scaling\n        return grad_x, grad_a, grad_b # on doit calculer les grad par rapport √† tous les arguments rajouter grad par rapport √† a et b\n\n    @staticmethod\n    def jvp(x, a, b, tangents):\n        \"\"\"                \n        It corresponds to a Jacobian Vector Product (jvp), used for forward auto-differentiation\n        \"\"\"\n        ## Vector v of small perturbations\n        tx, ta, tb = tangents\n        ## Matrix (in this case vector) of first order gradient\n        result = torch.tanh(torch.dot(a, x) + b)\n        scaling = (1. - result.pow(2))\n        Jx = a * scaling\n        Ja = x * scaling\n        Jb = scaling\n        ## Return J(x, a, b)v\n        return torch.dot(Jx, tx) + torch.dot(Ja, ta) + Jb * tb\n\n\n\nV√©rification des d√©riv√©es\n\nEn mode reverseEn mode forward\n\n\n\n## D√©finit f\nf = f_torch_manual.apply\nz = f(x, a, b)\n## Calcule et √©value le graphe de diff√©rentiation automatique de y par rapport √† x \n## R√©initialise les gradients √† z√©ro avant tout calcul \nx.grad.zero_(), a.grad.zero_(), b.grad.zero_()\nz.backward()\n## Renvoie dy/dx\nx.grad, a.grad, b.grad\n\n(tensor([0.0099, 0.0099]), tensor([0.0197, 0.0296]), tensor(0.0099))\n\n\nOn v√©rifie qu‚Äôon obtient bien le m√™me r√©sultat qu‚Äôen laissant torch faire le calcul :party:.\nOn aurait aussi pu utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode reverse)\n\nf_grad_rev = torch.func.jacrev(func=f, argnums=(0, 1, 2))\n\net v√©rifier que le r√©sultat coincide avec le calcul fait √† la main.\n\nassert all(torch.allclose(f_grad_rev(x, a, b)[i], (x.grad, a.grad, b.grad)[i]) for i in range(2))\n\n\n\nOn calcule la d√©riv√©e par rapport √† la premi√®re coordonn√©e de \\(x\\)\n\ntangents = (torch.tensor([1., 0.]), torch.tensor([0., 0.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0099, grad_fn=&lt;AddBackward0&gt;)\n\n\npuis par rapport √† la deuxi√®me coordonn√©e de \\(a\\)\n\ntangents = (torch.tensor([0., 0.]), torch.tensor([0., 1.]), torch.tensor(0.))\nf_torch_manual.jvp(x = x, a = a, b = b, tangents = tangents)\n\ntensor(0.0296, grad_fn=&lt;AddBackward0&gt;)\n\n\nEt on valide que les r√©sultats obtenus co√Øncident avec ceux obtenus en mode reverse et directement en utilisant torch üòÅ\n\n\n\n\n\n\nAvertissement\n\n\n\n\n\nEn th√©orie, on pourrait utiliser les op√©rateurs fonctionnels pour calculer la fonction d√©riv√©e (en utilisant le mode forward)\n\nf_grad_fwd = torch.func.jacfwd(func=f, argnums=(0, 1, 2))\n\nmais il faut d√©finir une m√©thode statique vmap et je n‚Äôai pas compris comment faire üò¢"
  },
  {
    "objectID": "autodiff.html#comparaison-des-temps-de-calculs",
    "href": "autodiff.html#comparaison-des-temps-de-calculs",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Comparaison des temps de calculs",
    "text": "Comparaison des temps de calculs\nOn compare ici les temps de calculs du calcul du gradient en mode forward, reverse pour la version qui utilise les primitives de torch et en mode reverse pour notre version.\n\ndim = 100\na = torch.tensor(np.arange(dim)/ (dim*10), requires_grad = True)\nb = torch.tensor(0.5, requires_grad = True)\nx = torch.tensor( (np.arange(dim) - 37) / (dim*10), requires_grad = True)\n\n\nf_torch_grad_rev = torch.func.jacrev(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_rev(x, a, b)\nf_torch_grad_fwd = torch.func.jacfwd(func=f_torch, argnums=(0, 1, 2))\n%timeit f_torch_grad_fwd(x, a, b)\n%timeit f_grad_rev(x, a, b)\n\n433 Œºs ¬± 1.88 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n501 Œºs ¬± 1.68 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n778 Œºs ¬± 1.44 Œºs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "autodiff.html#impact-de-jit",
    "href": "autodiff.html#impact-de-jit",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Impact de JIT",
    "text": "Impact de JIT\nOn essaie de jitter nos fonctions pour v√©rifier si cela acc√©l√®re le calcul des gradients.\nPlus d‚Äôinfo sur le JIT dans pytorch sont disponibles dans cette documentation.\n\nf_torch_grad_rev_jit = torch.jit.trace(f_torch_grad_rev, (x, a, b))\nf_torch_grad_fwd_jit = torch.jit.trace(f_torch_grad_fwd, (x, a, b))\n# f_grad_rev_jit = torch.jit.trace(f_grad_rev, (x, a, b))\n\n\n%timeit f_torch_grad_rev_jit(x, a, b)\n%timeit f_torch_grad_fwd_jit(x, a, b)\n# %timeit f_grad_rev_jit(x, a, b)\n\n56.4 Œºs ¬± 419 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\n123 Œºs ¬± 528 ns per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)"
  },
  {
    "objectID": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "href": "autodiff.html#d√©riv√©e-par-rapport-√†-x",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "D√©riv√©e par rapport √† \\(x\\)",
    "text": "D√©riv√©e par rapport √† \\(x\\)\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import grad, jit\nfrom jax import random\n\n## On d√©finit une dimension arbitraire pour nos inputs\ndim = 100\n\n## On initialise les param√®tres et le vecteur d'input de la fonction\na = jnp.arange(dim)/ (dim*10)\nb = 0.5\nx = (jnp.arange(dim) - 37) / (dim*10)\n\n## On d√©finit une fonction simple dont on conna√Æt les gradients analytiques\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n## On affiche la valeur de la fonction pour v√©rifier que tout est ok\nf(x, a, b) \n\nArray(0.56842977, dtype=float32)\n\n\n\nD√©riv√©e par rapport √† \\(x\\)D√©riv√©e par rapport √† \\(a\\) et \\(b\\)\n\n\nDans un premier temps, on peut d√©finir les gradients exacts de cette fonction √† partir d‚Äôune formule analytique.\n\ndef df_dx(x, a, b):\n    return a * (1 - jnp.tanh(jnp.dot(a, x) + b) **2)\n\nPuis on d√©finit les gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dx = jax.grad(lambda x: f(x, a, b), argnums=0) \n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dx = jax.jacfwd(lambda x: f(x, a, b), argnums=0) \n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert jnp.allclose(grad_df_dx(x), df_dx(x, a, b))\nassert jnp.allclose(grad_df_dx(x), fwdgrad_df_dx(x))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\nOn d√©finit √©galement les gradients exacts par rapport aux param√®tres \\(a\\) et \\(b\\) pour v√©rifier que l‚Äôon pourrait les optimiser dans un algorithme d‚Äôapprentissage.\n\ndef df_dab(x, a, b):\n    return x * (1 - jnp.tanh(jnp.dot(a, x) + b) **2), 1 - jnp.tanh(jnp.dot(a, x) + b) **2\n\nPuis on d√©finit ces gradients via autograd et on v√©rifie que les r√©sultats sont identiques.\n\n## jax.grad calcule la formule backward par d√©faut\ngrad_df_dab = jax.grad(lambda a_b: f(x, *a_b), argnums=0)\n## On peut aussi calculer la formule forward via jax.jacfwd\nfwdgrad_df_dab = jax.jacfwd(lambda a_b: f(x, *a_b), argnums=0)\n\n## On v√©rifie que les gradients retournent des valeurs identiques\nassert all(jnp.allclose(grad_df_dab((a,b))[i], df_dab(x, a, b)[i]) for i in range(2))\nassert all(jnp.allclose(grad_df_dab((a,b))[i], fwdgrad_df_dab((a,b))[i]) for i in range(2))\n\n## Si les assertions ne retournent pas d'erreur, les gradients sont corrects\nprint('All good, we are ready to go!')\n\nAll good, we are ready to go!\n\n\n\n\n\nNous avons donc bien v√©rifi√© que les gradients calcul√©s avec JAX sont identiques aux gradients analytiques."
  },
  {
    "objectID": "autodiff.html#avec-vjp-et-jvp",
    "href": "autodiff.html#avec-vjp-et-jvp",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Avec VJP et JVP",
    "text": "Avec VJP et JVP\nDans les sections pr√©c√©dentes, jax.jacrev et jax.jacfwd utilisent, respectivement, les VJP et les JVP des op√©rations √©l√©mentaires de la fonction f. Dans certains cas, nous pouvons avoir besoin de d√©finir nous-m√™mes les VJP et JVP comme vu en introduction.\nNous allons alors d√©finir les VJP et JVP pour f √† l‚Äôaide de gradients que nous connaissons analytiquement. Les VJP et JVP des fonctions √©l√©mentaires sous-jacentes ne seront alors plus utilis√©s.\n\nJVPVJP\n\n\nLien vers la documentation de JAX\nUn JVP est capable de d√©voiler une colonne de la jacobienne √† la fois. Ce n‚Äôest pas adapt√© pour cette fonction dont la jacobienne est large. Une passe JVP ne peut d√©voiler qu‚Äôune seule d√©riv√©e partielle : si l‚Äôon veut la d√©riv√©e par rapport √† chaque dimension de \\(x\\), chaque dimension de \\(a\\) et \\(b\\), il nous faut faire \\(dim + dim + 1\\) fois des JVPs ce qui n‚Äôest pas du tout efficace. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacfwd doit en fait appeler tous ces JVPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nPour d√©finir un custom_jvp en JAX, il faut attacher √† f, une fonction f_jvp, qui prend deux entr√©es primals le point o√π l‚Äôon calcule le gradient et tangents le vecteur tangent (√† voir aussi comme les gradients en amont du graphe que l‚Äôon parcourt en descendant). f_jvp retourne un tuple de deux vecteurs, f(primals) et le JVP df_dx @ tangents, o√π, bien s√ªr, df_dx contient l‚Äôexpression analytique de la d√©riv√©e (c‚Äôest une matrice jacobienne mais elle n‚Äôest jamais stock√©e en m√©moire car tout de suite r√©duite par le produit matriciel).\nNous avons vu que si tangents est un vecteur one-hot encoded nous d√©voilons une colonne de la matrice jacobienne (celle o√π se situe le \\(1\\)). Dans l‚Äôexemple ci-dessous nous calculons de mani√®re forward \\(\\frac{\\partial f}{\\partial x_0}\\).\n\n@jax.custom_jvp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\n@f.defjvp\ndef f_jvp(primals, tangents):\n    x, a, b = primals\n    x_dot, a_dot, b_dot = tangents\n    primal_out = f(x, a, b)\n    return primal_out, (jnp.dot(df_dx(x, a, b), x_dot) +  jnp.dot((x * (1 - primal_out ** 2)), a_dot) + jnp.dot((1 - primal_out ** 2), b_dot))\n\nx0_tangents = jnp.zeros(dim)\nx0_tangents = x0_tangents.at[0].set(1)\n_, x_dot = jax.jvp(f, (x, a, b), (x0_tangents, jnp.zeros(dim), 0.))\n\nassert jnp.allclose(grad_df_dx(x)[0], x_dot)\n\nprint(\"All right!\")\n\nAll right!\n\n\nOn note que, s‚Äôil est d√©fini, le custom_jvp sera utilis√© par JAX, en mode forward et en mode backward. Notons aussi la syntaxe particuli√®re √©manant du fait que f prend trois arguments en entr√©e.\n\n\nLien vers la documentation de JAX\nUn VJP est capable de d√©voiler une ligne de la jacobienne √† la fois. Cela va donc nous permettre de calculer toute la matrice jacobienne de \\(f\\) en un seul appel √† JVP car c‚Äôest une matrice √† une seule ligne. Nous l‚Äôavons vu dans la section pr√©c√©dente o√π en fait, jax.jacrec doit en fait appeler tous ces VJPs (ce qui est fait de mani√®re cach√©e √† l‚Äôutilisateur).\nSi nous souhaitons explicitement d√©finir le VJP, nous devons d‚Äôabord √©crire une fonction qui d√©crit la passe forward. C‚Äôest ici f_fwd qui retourne f(primal) et des valeurs stock√©es pour le moment de la passe backward (√† la mani√®re de save_for_backward vu dans la section pytorch !). Il faut ici bien r√©fl√©chir √† ce qui est n√©cessaire de stocker et ce qui est superflu, afin d‚Äôoptimiser au mieux le code. Ici nous stockons f(x,a,b), x et a car ces valeurs sont r√©utilis√©es dans la passe backward o√π nous calculons \\(g. \\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial x}\\), \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial a}\\) et \\(g.\\frac{\\partial \\mathrm{tanh}(f(x,a,b))}{\\partial b}\\). Avec \\(g\\) le gradient provenant de l‚Äôaval du graphe pour calculer les VJPs (rappelons que nous les calculons de mani√®re backward en remontant le graphe).\nNous comprenons √† nouveau que si g est un vecteur one-hot encoded nous d√©voilons une ligne de la matrice jacobienne (celle o√π se situe le \\(1\\)).\nNous devons √©galement √©crire une fonction f_bwd qui prend en argument les valeurs stock√©es dans la passe forward ainsi que g d√©fini dans le paragraphe pr√©c√©dent. Ici g est scalaire f a valeurs dans \\(\\mathbb{R}\\). f_bwd retourne autant de sorties que f compte d‚Äôentr√©es.\n\n@jax.custom_vjp\ndef f(x, a, b):\n    return jnp.tanh(jnp.dot(a, x) + b)\n\ndef f_fwd(x, a, b):\n    primal_out = f(x, a, b)\n    return primal_out, (x, a, primal_out)\n\ndef f_bwd(res, g):\n    x, a, primal_out = res\n    return (a * (1 - primal_out **2) * g, x * (1 - primal_out **2) * g, (1 - primal_out **2) * g)\n\nf.defvjp(f_fwd, f_bwd)\n_, f_vjp = jax.vjp(f, x, a, b) # renvoie f(primal) et f_vjp qui est une fonction qui doit √™tre √©valu√©e en `g`\n\nassert jnp.allclose(grad_df_dx(x), f_vjp(1.)[0])\nassert all(jnp.allclose(grad_df_dab((a,b))[i], f_vjp(1.)[i + 1]) for i in range(2))\nprint(\"All right!\")\n\nAll right!\n\n\nNotons que la d√©finition d‚Äôun custom_vjp red√©finit la fonction grad qui utilise donc aussi f_fwd. Ainsi, nous avons l‚Äô√©quivalence :\n\nassert jnp.allclose(f_vjp(1.)[0], jax.grad(f)(x, a, b))\nprint(\"All right!\")\n\nAll right!"
  },
  {
    "objectID": "autodiff.html#conclusion",
    "href": "autodiff.html#conclusion",
    "title": "Tutoriel de diff√©rentiation automatique",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#o√π-quand",
    "href": "index.html#o√π-quand",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "",
    "text": "L‚Äôatelier Finist‚ÄôR 2025 ‚Äì ou bootcamp R du groupe State Of The R se d√©roulera √† la station biologique de Roscoff du 18 au 22 ao√ªt 2025.\nStateoftheR est un r√©seau du d√©partement MathNum INRAE.\n\n\nIl s‚Äôagit de la neuvi√®me √©dition de l‚Äôatelier Finist‚ÄôR. Cet atelier r√©unit annuellement un groupe de chercheurs, ing√©nieurs, doctorants, tous utilisateurs avanc√©s de R et d√©veloppeurs de paquets pour explorer les derni√®res fonctionnalit√©s du logiciel et les nouvelles pratiques de d√©veloppement. A l‚Äôissue de l‚Äôatelier le collectif produit une synth√®se de cette veille logiciel de mani√®re √† progresser collectivement dans l‚Äôutilisation du logiciel mais surtout dans la production d‚Äôoutils statistiques √† destination de la communaut√©.\nLe r√©sultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#programme",
    "href": "index.html#programme",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Programme",
    "text": "Programme"
  },
  {
    "objectID": "index.html#participants",
    "href": "index.html#participants",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Participants",
    "text": "Participants\nBaptiste Alglave, Julie Aubert, Pierre Barbillon, Gloria Buritica, Lucia Clarotto, Caroline Cognot, Marie-Pierre Etienne, Armand Favrot, Blanche Francheterre, Hugo Gangloff, Pascal Irz, Louis Lacoste, Arthur Leroy, Mahendra Mariadassou, Pierre Navaro, L√©o Micollet, Jeanne Tous."
  },
  {
    "objectID": "index.html#soutien",
    "href": "index.html#soutien",
    "title": "FinistR : bootcamp R √† Roscoff",
    "section": "Soutien",
    "text": "Soutien"
  }
]